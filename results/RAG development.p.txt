M A N N I N G
 Abhinav Kimothi
Retrieval Augmented
Generation
A SIMPLE GUIDE TO
The indexing and generation pipelines together make a RAG system. The indexing pipeline is an offline 
process, while the generation pipeline facilitates real-time interaction with the knowledge base.
User asks a
question.
The system searches
for relevant
information.
The information relevant
to the input question is
fetched, or retrieved.
The prompt with the user
question is augmented
with the retrieved
information.
The LLM responds
with a contextual
answer.
Connect to
external sources.
Extract documents and
parse text from
documents.
Break down long 
pieces of text into
smaller manageable
pieces.
Convert these small
pieces into a suitable
format.
Generation pipeline :
Uses the knowledge
base to generate context
aware responses
LLM
Response
Search
Retriever
User
Question
{Question + Information}
Parametric memory
Storage
Non-parametric memory
Source
Connector
Extracter &
parser
Splitter
Converter
Knowledge base
Indexing pipeline :
Facilitates the creation
of the knowledge base
Fetch
i
M A N N I N G
Shelter Island
Abhinav Kimothi
A Simple Guide to Retrieval 
Augmented Generation
For online information and ordering of this and other Manning books, please visit www.manning.com. 
The publisher offers discounts on this book when ordered in quantity.
For more information, please contact
Special Sales Department
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
Email: orders@manning.com
© 2025 Manning Publications Co. All rights reserved.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form 
or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the 
publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed 
as trademarks. Where those designations appear in the book, and Manning Publications was aware of a 
trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books 

To Pallavi and Zara—my greatest sources of love, strength, and inspiration
iv
brief contents
Part 1		 Foundations..................................................................1
	
1	
■ 	 LLMs and the need for RAG   3
	
2	
■ 	 RAG systems and their design  17
Part 2		 Creating RAG systems...............................................31
	
3	
■ 	 Indexing pipeline: Creating a knowledge base for RAG  33
	
4	
■ 	 Generation pipeline: Generating contextual LLM responses  58
	
5	
■ 	 RAG evaluation: Accuracy, relevance, and faithfulness  87
Part 3		 RAG in production...................................................119
	
6	
■ 	 Progression of RAG systems: Naïve, advanced, and  
modular RAG  121
	
7	
■ 	 Evolving RAGOps stack  145
Part 4		 Additional considerations.....................................165
	
8	
■ 	 Graph, multimodal, agentic, and other RAG variants  167
	
9	
■ 	 RAG development framework and further exploration  200
v
contents
preface    ix
acknowledgments    xi
about this book    xiii
about the author    xvii
about the cover illustration    xviii
Part 1	 Foundations...................................................1
	
1	
LLMs and the need for RAG   3
	 1.1	
Curse of the LLMs and the idea of RAG  4
LLMs are not trained for facts  6  ■  What is RAG?  8
	 1.2	
The novelty of RAG  11
The RAG discovery  11  ■  How does RAG help?  12
	 1.3	
Popular RAG use cases  13
Search Engine Experience  13  ■  Personalized marketing content 
generation  13  ■  Real-time event commentary  14 
Conversational agents  14  ■  Document question answering 
systems  14  ■  Virtual assistants  15  ■  AI-powered research  15 
Social media monitoring and sentiment analysis  15 
News generation and content curation  15
vi
contents
vi
	
2	
RAG systems and their design  17
	 2.1	
What does a RAG system look like?  18
	 2.2	
Design of RAG systems  22
	 2.3	
Indexing pipeline  23
	 2.4	
Generation pipeline  25
	 2.5	
Evaluation and monitoring  26
	 2.6	
The RAGOps Stack  27
	 2.7	
Caching, guardrails, security, and other layers  28
Part 2	 Creating RAG systems.................................31
	
3	
Indexing pipeline: Creating a knowledge base for RAG  33
	 3.1	
Data loading  34
	 3.2	
Data splitting (chunking)  38
Advantages of chunking  38  ■  Chunking process  39 
Chunking methods  39  ■  Choosing a chunking strategy  44
	 3.3	
Data conversion (embeddings)  46
What are embeddings?  46  ■  Common pretrained  
embeddings models  48  ■  Embeddings use cases  49 
How to choose embeddings?  52
	 3.4	
Storage (vector databases)  53
What are vector databases?   53  ■  Types of vector databases  53 
Choosing a vector database  55
	
4	
Generation pipeline: Generating contextual LLM responses  58
	 4.1	
Generat
vii
contents
	
vii
	
5	
RAG evaluation: Accuracy, relevance, and faithfulness  87
	 5.1	
Key aspects of RAG evaluation  88
Quality scores  89  ■  Required abilities  89
	 5.2	
Evaluation metrics  91
Retrieval metrics  91  ■  RAG-specific metrics  98
	 5.3	
Frameworks  104
RAGAs  104  ■  Automated RAG evaluation system  110
	 5.4	
Benchmarks  111
RGB  111
	 5.5	
Limitations and best practices  115
Part 3	 RAG in production.....................................119
	
6	
Progression of RAG systems: Naïve, advanced, and  
	
	
modular RAG  121
	 6.1	
Limitations of naïve RAG  122
	 6.2	
Advanced RAG techniques  123
	 6.3	
Pre-retrieval techniques  125
Index optimization  125  ■  Query optimization  130
	 6.4	
Retrieval strategies  133
Hybrid retrieval  133  ■  Iterative retrieval  134 
Recursive retrieval  134  ■  Adaptive retrieval  134
	 6.5	
Post-retrieval techniques  136
Compression  136
	 6.6	
Modular RAG  139
Core modules  139  ■  New modules  140
	
7	
Evolving RAGOps stack  145
	 7.1	

viii
contents
viii
Part 4	 Additional considerations......................165
	
8	
Graph, multimodal, agentic, and other RAG variants  167
	 8.1	
What are RAG variants, and why do we need them?  168
	 8.2	
Multimodal RAG  169
Data modality  169  ■  Multimodal RAG use cases  170 
Multimodal RAG pipelines  170  ■  Challenges and best 
practices  176
	 8.3	
Knowledge graph RAG  177
Knowledge graphs  177  ■  Knowledge graph RAG use cases  179 
Graph RAG approaches  179  ■  Graph RAG pipelines  181 
Challenges and best practices  186
	 8.4	
Agentic RAG  187
LLM agents  187  ■  Agentic RAG capabilities  190  ■  Agentic 
RAG pipelines  190  ■  Challenges and pest practices  193
	 8.5	
Other RAG variants  194
Corrective RAG  194  ■  Speculative RAG  195  ■  Self-reflective 
(self RAG)  196  ■  RAPTOR  197
	
9	
RAG development framework and further exploration  200
	 9.1	
RAG development framework  201
Initiation stage: Defining and scoping the RAG system  203
	 9.2	
Design stage: Layering the 
ix
preface
How machines understand human intent has always been a subject of deep interest 
for me. Although I embarked on my journey into AI and machine learning in 2007, 
it was in early 2016 that I became fascinated by natural language processing (NLP), 
while building a virtual data analyst. When Google released BERT in 2018, I became 
convinced that NLP was on the brink of a revolution.
In 2022, following the release of text-davinci-002, a model in OpenAI’s GPT-3 series, 
I decided to join Yarnit, a generative-AI-based content marketing platform, to build the 
AI backbone of the application. The mission was to create a platform where enterprise 
content marketing teams could generate marketing assets—social media posts, blogs, 
emails, and more—at high speed, large scale, and lower cost, with greater accuracy. It 
quickly became apparent that no generative model could achieve this effectively with-
out incorporating brand-specific knowledge and access to proprietary data. This rea
x
preface
x
on social platforms and in blog posts. Eventually, the idea of consolidating all these 
learnings into a comprehensive book took shape.
With the goal of creating a simple, practical resource for technology professionals 
building LLM-based applications, I started working on this book in mid-2024. Over 
time, it has evolved into a foundational guide to RAG, covering both breadth and 
depth, while ensuring practical implementation through clear explanations and simple 
Python code.
I firmly believe that RAG is an essential skill for anyone working with AI applications 
and that mastering it requires a solid conceptual foundation. This book is designed to 
provide just that. Writing it has been an incredibly enriching experience, and I have 
learned a great deal along the way. I hope you find it both enlightening and enjoyable.
xi
acknowledgments
Authoring a book requires countless hours of research and dedicated writing, espe-
cially on a rapidly evolving topic such as RAG, where new research emerges almost 
every week. This book would not have been possible without the unwavering love and 
support of my wife, Pallavi. Her encouragement and patience sustained me through-
out this journey, and for that, I am eternally grateful.
I am deeply thankful to my co-founders, Jyotirmoy and Akash, and the entire team at 
Yarnit, who have significantly contributed to my understanding of RAG. The hands-on 
experiences of building real-world AI applications have undoubtedly enriched this 
book, making it a more valuable resource for readers.
I would also like to express my heartfelt gratitude to colleagues and mentors—Ashish 
Rishi, Satyakam Mohanty, Pradeepta Mishra, Megha John, Sandeep Acharya, Akshit 
Sharma, Vishal Sinha, and many others—for their insightful discussions and guidance 
over the years. Their perspectives
xii
acknowledgments
xii
My deep gratitude goes to the AI research community as well, whose relentless pur-
suit of knowledge and innovation continues to push the boundaries of what’s possible. 
In many ways, this book is a reflection of the collective knowledge shared by research-
ers, open source contributors, and practitioners who have generously published their 
insights in papers, blogs, and forums.
To all the reviewers—Abhishek Gupta, Alejandro Cuevas Rivero, Alex McLintock, 
Alireza Aghamohammadi, Amit Dixit, Anindita Nath, Anindyadeep Sannigrahi, Aryan 
Jadon, Ashish Sarkar, Aushim Nagarkatti, Avinash Tiwari, Babloo Kumar, Balaji Dha-
modharan, Balakrishnan Balasubramanian, Bert Gollnick, Bhargob Deka, Brian Daley, 
Charan Akiri, Christopher G. Fry, Harcharan S. Kabbay, Harshwardhan S. Fartale, 
Igor Svilenkov Božić, Iván Moreno, Lalit Chourey, Louis Luangkesorn, Louis-François 
Bouchard, Manas Talukdar, Márcio F. Nogueira, Marine Serré, Naga Santhosh Reddy 
Vootukuri, Neelesh P
xiii
about this book
Retrieval-augmented generation (RAG) is transforming the landscape of applied 
generative AI. First introduced by Lewis and colleagues in their seminal paper 
“Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks” (https://
arxiv.org/abs/2005.11401), RAG has quickly become a cornerstone of modern AI, 
enhancing the reliability and trustworthiness of large language models (LLMs).
A Simple Guide to Retrieval Augmented Generation is a foundational guide for individuals 
looking to explore RAG. It offers a gentle, yet comprehensive introduction to the con-
cept, along with practical insights helpful in using RAG to their advantage.
Who should read this book?
This book is for technology professionals who want to be introduced to the concept 
of RAG and build LLM-based apps. It is a handy book for both beginners and experi-
enced professionals alike. If you’re a data scientist, data engineer, ML engineer, soft-
ware developer, technology leader, or student in
xiv
about this book
xiv
¡ Acquire knowledge of available tools, technologies, and frameworks for building 
and deploying production-grade RAG systems.
¡ Learn about state-of-the-art RAG variants, such as multimodal and agentic RAG.
¡ Get an understanding of the current limitations of RAG and learn more about 
popular emerging techniques for further exploration.
While prior exposure to the world of ML, generative AI, and LLMs is always helpful, 
this book is a foundational guide and does not assume that you have a deep under-
standing of the concepts. You’ll develop a deeper understanding of LLMs as you go 
through the first chapter. 
This book is also interspersed with code snippets in Python, using the LangChain 
framework. It is important to note that the code snippets act only as supplementary 
illustrations to the concepts and are aimed at readers who want to get a hands-on expe-
rience. Only a beginner-level understanding of Python and APIs is expected from those 
who want to try 
xv
about this book
	
xv
chapter will also contain details about popular frameworks and benchmarks used 
in RAG evaluation.
Part 3 will guide you in improving your RAG pipeline and lay out a blueprint for the 
layers required to build a production-ready RAG system:
¡ Chapter 6 looks into the advanced concepts in RAG from the perspective of 
naïve, advanced, and modular RAG implementation. We discuss important com-
ponents and pre-/post-retrieval strategies. This chapter also provides optimiza-
tion techniques to improve RAG system performance.
¡ Chapter 7 reviews different tools and technologies that enable the RAGOps 
stack. You will learn about the critical layers without which any RAG system will 
fail, the essential layers that improve system performance, and the enhancement 
layers that focus on system usability, scalability, and efficiency. 
In Part 4, you will learn about the popular state-of-the-art variants of RAG and a RAG 
development framework:
¡ Chapter 8 discusses the stat
xvi
about this book
xvi
features, you can attach comments to the book globally or to specific sections or 
paragraphs. It’s a snap to make notes for yourself, ask and answer technical questions, 
and receive help from the author and other users. To access the forum, go to https://
livebook.manning.com/book/a-simple-guide-to-retrieval-augmented-generation/
discussion. 
Manning’s commitment to our readers is to provide a venue where a meaningful dia-
logue between individual readers and between readers and the author can take place. It 
is not a commitment to any specific amount of participation on the part of the author, 
whose contribution to the forum remains voluntary (and unpaid). We suggest you try 
asking the author some challenging questions lest their interest stray! The forum and 
the archives of previous discussions will be accessible from the publisher’s website for as 
long as the book is in print.
xvii
about the author
Abhinav Kimothi is a seasoned AI practitioner with 
over 15 years of experience developing cutting-edge 
AI and machine learning solutions. Throughout his 
career, Abhinav has led AI projects across analytics, pre-
dictive ML, NLP, and generative AI—some were success-
ful, while others provided valuable lessons. Driven by 
curiosity and a passion for innovation, he continues to 
push the boundaries of AI to create effective solutions. 
You can learn more about Abhinav at https://www 
.abhinavkimothi.com/.
xviii
about the cover illustration
The figure on the cover of A Simple Guide to Retrieval Augmented Generation, titled “Le 
Marchand D’Habits,” or “The Clothes Merchant,” is taken from a book by Louis Cur-
mer published in 1841. Each illustration is finely drawn and colored by hand.  
In those days, it was easy to identify where people lived and what their trade or station 
in life was just by their dress. Manning celebrates the inventiveness and initiative of the 
computer business with book covers based on the rich diversity of regional culture cen-
turies ago, brought back to life by pictures from collections such as this one.
Part 1
Foundations
This first part of the book introduces the core idea behind retrieval-
augmented generation (RAG) and the high-level design of a RAG system. 
Chapter 1 deals with various challenges that AI systems based on large lan-
guage models (LLMs) face. Furthermore, it illustrates the ways RAG addresses 
these challenges to improve the reliability of such systems. The chapter also pro-
vides a brief overview of the workings of LLMs and some popular RAG use cases. 
Chapter 2 discusses the steps involved in building a RAG system. This chapter 
details the basics of two core RAG pipelines and other essential components of a 
RAG system. 
By the end of the first part of the book, you should have a foundational under-
standing of a RAG system and be ready to dive deep into the intricacies of RAG.

3
1
LLMs and the 
need for RAG 
This chapter covers
¡ The limits of LLMs and the need for RAG
¡ The RAG basics
¡ Popular use cases of RAG
In a short time, large language models (LLMs) have found widespread application 
in modern language processing tasks and autonomous AI agents. OpenAI’s GPT, 
Anthropic’s Claude, Google’s Gemini, and Meta’s Llama series are notable LLMs 
integrated into various platforms and techniques. Retrieval-augmented generation, 
or RAG, plays a pivotal role in the LLM application by enhancing the accuracy and 
relevance of responses. According to Grand View Research (https://mng.bz/BzKg), 
in 2023, the global RAG market was estimated at some $1 billion USD, and it has 
been projected to grow by 44.7% annually, which makes it one of the fastest-growing 
AI methodologies. 
This book aims to demystify the idea of RAG and its application. Chapter by chap-
ter, the book will present the RAG definition, design, implementation, evaluation, 
and evolution. To kick thin
4
Chapter 1  LLMs and the need for RAG 
of RAG and builds toward a definition. The chapter ends by listing the popular use 
cases enabled by RAG.
By the end of this chapter, you will gain foundational knowledge to be ready for a 
deeper exploration of the RAG system components. In addition, you should
¡ Have a strong hold on the RAG definition.
¡ Understand the limitations of LLMs and the need for RAG.
¡ Be ready to dive into the components of a RAG system.
November 30, 2022, will be remembered as a watershed moment in the field of artifi-
cial intelligence. This was the day OpenAI released ChatGPT, and the world became 
mesmerized by it. ChatGPT turned out to be the fastest app ever to reach a million 
users. Interest in previously obscure terms such as generative AI and LLMs skyrocketed 
over the following 12 months (see figure 1.1). 
Large Language Models
Generative AI
November 2022
November 2023
November 2024
Figure 1.1  Google trends of “Generative AI” and “Large Language Models” 
5
Curse of the LLMs and the idea of RAG
imagine you are interacting with ChatGPT, and you ask, “Who won the 2023 Cricket 
World Cup?” You are, in truth, interacting with GPT-4o, or o1, LLMs developed and 
maintained by OpenAI that power ChatGPT. In the first few sections of this chapter, we 
will use the terms ChatGPT and LLMs interchangeably for simplicity. So, you ask the 
question and, most likely, you will get a response as the one in figure 1.2.
Figure 1.2  ChatGPT (GPT 3.5) response to the question, “Who won the 2023 Cricket World Cup?” 
Source: Screenshot of the author’s account on https://chat.openai.com.
ChatGPT does not have any memory of the 2023 Cricket World Cup, and it tells you 
to check the information from other sources. This is not ideal, but at least ChatGPT is 
honest in its response. The same question asked again might also provide a factually 
inaccurate result. Look at the response in figure 1.3. ChatGPT falsely responds that 
India was the winner of the tourname
6
Chapter 1  LLMs and the need for RAG 
This is problematic. Despite not having any memory of the 2023 Cricket World Cup, 
ChatGPT still generates the answer in a seemingly confident tone, but it does so inac-
curately. This is what is called a “hallucination,” and it has become a major point of 
criticism for LLMs.
NOTE  In September 2023, ChatGPT’s “Browse with Bing” feature was intro-
duced, which allows ChatGPT Plus users to fetch live information from the web 
for more accurate and up-to-date responses. This is a feature of the applica-
tion, which is enabled via agentic search and retrieval mechanisms. The under-
lying LLM doesn’t inherently have the latest information. 
Many users treat LLMs as a source of information as an alternative to Google Search. 
In our example, we also expected ChatGPT (GPT 3.5 model) to know the answer to 
the simple question. Why does an LLM fail to meet this expectation?
1.1.1	
LLMs are not trained for facts
Generally, LLMs can be thought of as a nex
7
Curse of the LLMs and the idea of RAG
The teacher
teaches the
student.
GB/TB/PB
of text data
Sample
sentence.
Teaches
Causal Language Modeling
Objective: Predict next token
Unidirectional context
Masked Language Modeling
Encoder- only
model
Objective: Reconstruct text (“denoising”)
The
Teacher
<MASK>
Student
Bidirectional context
The
student
the
The
teacher
teaches
student
the
The
teacher
teaches
Decoder- only
model
The
teacher
?
student
the
The
teacher
teaches
The
Teacher
Teaches
Figure 1.4  Two token prediction techniques: CLM and MLM. In the CLM approach, the model predicts 
the next token based on the preceding tokens. In MLM, the model predicts the masked token based on 
both the preceding and the succeeding tokens.
walks
applauds
teaches
.......
0.1
0.02
0.4
0.3
greets
The
teacher
?
Selected
word
Figure 1.5  Illustrative probability distribution of words after “The teacher” 
Knowledge cut-off date
Training an LLM is an expensive and time-consuming process. It takes massive volu
8
Chapter 1  LLMs and the need for RAG 
Hallucinations
It is observed that LLMs sometimes provide factually incorrect responses. (We saw 
this in the 2023 Cricket World Cup example at the beginning of this chapter.) Despite 
being factually incorrect, the LLM responses sound extremely confident and legiti-
mate. This characteristic of “lying with confidence,” called hallucinations, has proved 
to be one of the biggest criticisms of LLMs. The reason for hallucinations can be traced 
back to LLMs being a next-token prediction model that selects the most probable word 
from a distribution. 
Knowledge limitation
As you have already seen, LLMs have been trained on large volumes of data obtained 
from a variety of sources, including the open internet. However, they do not have any 
knowledge of information that is not public. The LLMs have not been trained on infor-
mation such as internal company documents, customer information, product docu-
ments, confidential personnel information, and s
9
Curse of the LLMs and the idea of RAG
Context from External Source
Figure 1.6  Wikipedia article on 2023 Cricket World Cup. Source: https://mng.bz/yN4J.
External Context
Provided
Figure 1.7  ChatGPT (GPT 3.5) response to the question, augmented with external context. Source: 
Screenshot of the author’s account on https://chat.openai.com.
10
Chapter 1  LLMs and the need for RAG 
RAG does the same thing programmatically. It overcomes the limitations of LLMs by 
providing them with previously unknown information and, consequently, enhances 
the overall memory of the system. 
As the name implies, “retrieval augmented generation” can be explained through 
three steps:
1	 It retrieves relevant information from a data source external to the LLMs (Wikipe-
dia, in our example).
2	 It augments the input to the LLM with that external information.
3	 Finally, the LLM generates a more accurate result. 
A simple definition for RAG, illustrated in figure 1.8, can therefore be as follows:
Retrieval Augmented Generation is the technique of retrieving relevant information 
from an external source, augmenting the input to the LLM with that external infor-
mation, thereby enabling the LLM to generate a response that is contextual, reliable, 
and factually accurate.
Retrieve
Query
Search query
Fetch information
External source of informati
11
The novelty of RAG
1.2	
The novelty of RAG
The main idea is to provide additional context or knowledge to the LLMs. Essentially, 
it meant creating a ChatGPT-like system with three main objectives:
¡ Make LLMs respond with up-to-date information.
¡ Make LLMs respond with factually accurate information.
¡ Make LLMs aware of proprietary information.
These objectives can be achieved using diverse techniques. A new LLM can be trained 
from scratch that includes the new data. An existing model can also be fine-tuned with 
additional data. However, both approaches require a significant amount of data and 
computational resources. Furthermore, updating the model with new information at 
regular intervals is prohibitively costly. 
RAG is a cheaper, more effective, and more dynamic technique used to attain the 
three objectives. LLMs respond with information that is up-to-date and factually accu-
rate, and they are aware of proprietary information, so they have no knowledge gaps.
1.2.1	
The 
12
Chapter 1  LLMs and the need for RAG 
diverse, and factual language. We will discuss vector databases and retrievers in chap-
ters 3 and 4.
In 2025, RAG became one of the most used techniques in the LLM domain. With 
the addition of a non-parametric memory, the LLM responses are more grounded and 
factual. Let’s discuss the advantages of RAG.
1.2.2	
How does RAG help?
With the introduction of non-parametric memory, the LLM does not remain limited to 
its internal knowledge. We can conclude, at least theoretically, that this non-parametric 
memory can be extended as much as we want. It can store any volume of proprietary 
documents or data and access all sorts of sources, such as the intranet and the open 
internet. In a way, through RAG, we open up the possibility of embellishing the LLM 
with unlimited knowledge. There will always be some effort required to create this 
non-parametric memory or the knowledge base, and we will look at it in detail later. 
Chapter 3 is dedicated to t
13
Popular RAG use cases
Prompt
Search query
Fetch information
Non-parametric memory
User
{Prompt + Information}
LLM
Response
Parametric
memory
Contextual
Reliable
Factual
Retriever
Figure 1.9  RAG enhances the parametric memory of an LLM by creating access to non-parametric 
memory.
1.3	
Popular RAG use cases
RAG is not just a theoretical concept but a technique that is as popular as the LLM 
technology itself. Software developers started using language models as soon as Google 
released BERT in 2018. Today, there are thousands of applications that use LLMs to 
solve language-intensive tasks. Whenever you come across an application using LLMs, 
it will often have an internal RAG system in some shape or form. Common applications 
are described in the following sections.
1.3.1	
Search Engine Experience
Conventional search results are shown as a list of page links ordered by relevance. 
Modern search engines integrate RAG to combine live information retrieval with 
generative answers. Go
14
Chapter 1  LLMs and the need for RAG 
relevant and factual. By pulling in the right information (e.g., a brand’s style guide or 
latest stats) at generation time, these platforms produce personalized, on-brand mar-
keting content that resonates with audiences.
1.3.3	
Real-time event commentary
Imagine an event such as a sport or a news event. A retriever can connect to real-
time updates/data via APIs and pass this information to the LLM to create a virtual 
commentator. These can further be augmented with text-to-speech models. A prime 
example is IBM’s Watson AI at the US Open—it generates audio and text tennis com-
mentary by pulling in live match data and even thousands of news articles for context. 
This RAG approach allowed Watson to mention player stats, head-to-head records, and 
match highlights as it narrated, creating fact-driven commentary on the fly. In finan-
cial markets, vendors are doing something similar—Bloomberg’s AI-driven tools use 
RAG to ground their insights
15
Popular RAG use cases
transformed by RAG—instead of relying on an LLM’s limited training data, companies 
can equip AI assistants to search internal documents, wikis, or manuals on the fly.
1.3.6	
Virtual assistants
Virtual personal assistants such as Siri, Alexa, and others are beginning to use LLMs 
to enhance the user’s experience. Coupled with more context on user behavior using 
RAG, these assistants are set to become more personalized. Amazon’s next-generation 
Alexa, for instance, incorporates retrieval techniques, so it can answer with informa-
tion beyond its core training. By augmenting voice assistant answers with retrieved 
facts, RAG helps virtual assistants such as Alexa and Google Assistant give far more 
accurate and current answers to user queries.
1.3.7	
AI-powered research
AI agents have been gaining traction in research-intensive fields such as law and 
finance. RAG has been extensively used to retrieve and analyze case law to assist law-
yers. A lot of portfolio
16
Chapter 1  LLMs and the need for RAG 
This introductory chapter dealt with the RAG concept. Overcoming the limitations 
of LLMs, RAG addresses these challenges by providing access to a non-parametric 
knowledge base to the system. With this foundational understanding of RAG, in the 
next chapter, we take the first step toward understanding how RAG systems are built by 
looking at the different components of their design.
Summary
¡ RAG enhances the memory of LLMs by providing access to external information.
¡ LLMs are next-word (or token) prediction models trained on massive amounts 
of text data to generate human-like text.
¡ LLMs face challenges of having a knowledge cut-off date and being trained only 
on public data. They are also prone to generating factually incorrect information 
(i.e., hallucinating).
¡ RAG overcomes the LLM limitations by incorporating non-parametric memory 
and increases context awareness and reliability of responses.
¡ Popular use cases of RAG include sear
17
2
RAG systems 
and their design
This chapter covers
¡ The concept and design of RAG systems
¡ An overview of the indexing pipeline
¡ An overview of the generation pipeline
¡ An initial look at RAG evaluation
¡ A high-level look at the RAG operations stack
The first chapter explored the core principles behind retrieval-augmented gener-
ation (RAG) and the large language model (LLM) challenges addressed by it. To 
construct a RAG system, several components need to be assembled. This process 
includes the creation and maintenance of the non-parametric memory, or a knowl-
edge base, for the system. Another pipeline facilitates real-time interaction by send-
ing the prompts to and accepting the response from the LLM, with retrieval and 
augmentation steps in the middle. Evaluation is yet another critical component, 
ensuring the effectiveness and accuracy of the system. All these components are 
supported by layers of the operations stack.
18
Chapter 2  RAG systems and their design
Chapter 2 discusses the design of a RAG system, examining the steps involved and 
the need for two different pipelines. We will call the pipeline that creates the knowledge 
base the “indexing pipeline.” The other pipeline that allows real-time interaction with 
the LLM will be referred to as the “generation pipeline.” We will discuss their individ-
ual components, such as data loading, embeddings, vector stores, retrievers, and more. 
Additionally, we will get an understanding of how the evaluation of RAG systems is con-
ducted and introduce the RAG operations (RAGOps) stack that powers such systems.
This chapter will introduce you to various components discussed in detail in the 
coming chapters. By the end of chapter 2, you will have acquired a deep understanding 
of the components of a RAG system and will be ready to dive deep into the different 
components. By the end of the chapter, you should
¡ Be able to understand the several componen
19
What does a RAG system look like?
Prompt
Search query
Fetch information
Non-parametric memory
User
{Prompt + Information}
LLM
Response
Parametric memory
External source of information
Retriever
User asks a
question.
Step 1
The system searches for
relevant information.
The information relevant to the
input question is fetched, or
retrieved.
The prompt with the user
question is augmented with
the retrieved information.
The LLM responds with
a contextual answer.
Step 2
Step 3
Step 4
Step 5
Figure 2.1  Generation pipeline covering the five RAG steps. The journey from query to the response 
involves search and retrieval, augmentation, and generation.
This pipeline enables real-time contextual interaction with the LLM. There are, of 
course, several intricacies in each of the five steps needed to create the generation 
pipeline. Some decisions need to be made about the design of the retriever and the 
LLM choice. The construction of prompts will also affect the quality of the response. 
W
20
Chapter 2  RAG systems and their design
We will also need to know the format and nature of data storage to be able to extract 
the information from the source files.
When data is stored across multiple sources, such as the internet and an internal 
data lake, the system must connect to each source, search for relevant information in 
various formats, and organize it according to the original query. Every time a question 
is asked, this process of connecting, extracting, and parsing will have to be repeated. 
Information from different sources may lead to factual inconsistencies that will have to 
be resolved in real time. Searching through all the information might be prohibitively 
time-consuming. This will, therefore, prove to be a highly suboptimal, unscalable pro-
cess that may not yield the desired results. A RAG system will work best if the informa-
tion from different sources is
¡ Collected in a single location.
¡ Stored in a single format.
¡ Broken down into small pieces of 
21
What does a RAG system look like?
Connect to
external sources.
Extract documents and
parse text from
documents.
Break down long pieces
of text into smaller
manageable pieces.
Convert these small
pieces into a suitable
format.
Store the information.
Storage
Source
Connector
Extracter &
parser
Splitter
Converter
Step 1
Step 2
Step 3
Step 4
Step 5
Knowledge
base for RAG
t
Figure 2.2  Indexing pipeline covering the steps to create the knowledge base for RAG. This involves 
connecting to the source, parsing, splitting, converting, and storing information.
User asks a
question.
The system searches
for relevant
information.
The information relevant
to the input question is
fetched, or retrieved.
The prompt with the user
question is augmented
with the retrieved information.
The LLM responds
with a contextual answer.
Connects to
external sources.
Extract documents and
parse text from
documents.
Breaks down long 
pieces of text into
smaller manageable pieces.
Converts these small
pieces into 
22
Chapter 2  RAG systems and their design
2.2	
Design of RAG systems
We saw how RAG systems are created by the indexing and generation pipelines. 
These two pipelines include several parts themselves. Like all software applications, 
production-ready RAG systems require more than just the basic components. We need 
to think about accuracy, observability, scalability, and other important factors. This 
book discusses some of these components at length. Figure 2.4 presents a rough layout 
of a RAG system. Apart from the indexing and generation component, we’ll add layers 
for infrastructure, security, evaluation, etc.
Orchestrator
Input/Output
 
Retrievers
LLMs setup
Prompt
management
Storage
component:
Vector DBs
Data-loading
component
Data-splitting
component:
Chunking
 
Conversion
component:
Embeddings
Caching
Guardrails
LLM security
Evaluation
Monitoring
Privacy
Service Infrastructure
“Offline” data
indexing components
for knowledge base
creation
“Real-time”
interaction
components f
23
Indexing pipeline
These next three components complete the generation pipeline:
¡ Retrievers—Responsible for searching and fetching information from the storage
¡ LLM setup—Responsible for generating the response to the input
¡ Prompt management—Enables the augmentation of the retrieved information to 
the original input
The evaluation component measures the accuracy and reliability of the system before 
and after deployment. The monitoring component tracks the performance of the 
RAG system and helps detect failures. Other components include caching, which helps 
store previously generated responses to expedite retrieval for similar queries; guard-
rails, to ensure compliance with policy, regulation, and social responsibility; and secu-
rity, to protect LLMs against breaches such as prompt injection, data poisoning, and 
similar. All the layers are supported by a service infrastructure.
All these components are managed and controlled by a central orchestration layer, 
which is resp
24
Chapter 2  RAG systems and their design
extracting and parsing the text from these files. These external sources can be 
filesystems, data lakes, content management systems, and so forth. The files 
received from the sources can be in various formats such as PDF, docs, JSON, 
HTML, and more. 
This component, therefore, comprises several connectors (for different exter-
nal sources), extractors, and parsers (for different file types). In chapter 3, we 
will look at several examples of such loaders. The data-loading component also 
involves efficient preprocessing of data for knowledge consistency, removal of 
irrelevant information and masking of confidential data. Metadata information 
is another aspect the data-loading component manages. Chapters 3 and 6 discuss 
how the data loading component is built and enhanced. 
¡ Data splitting (text splitting)—Breaking down text into smaller segments enhances 
the system’s ability to process and analyze information efficiently. These smaller
25
Generation pipeline
This application can also be thought of as a RAG system. But there is a difference. 
This system has outsourced the search and retrieval operation to the third-party API. 
It is the third party that maintains the data. For such systems, the indexing pipeline 
is not required to be built since the search and retrieval happens outside the system. 
Another example is applications that ask the user to input external information, like 
document summarizers. The search operation here is outsourced to the user. 
Therefore, systems that use augment external information to the prompts but do not 
necessarily search and retrieve information themselves, do not warrant the creation 
of a knowledge base, and therefore, do not have an indexing pipeline. Some will argue 
that such systems are not RAG systems in the first place.
2.4	
Generation pipeline
Building on the foundation established by the indexing pipeline, the generation pipe-
line facilitates real-time interactions i
26
Chapter 2  RAG systems and their design
Therefore, the retriever also contributes heavily to the overall latency of the sys-
tem. We will discuss different retrievers and retrieval strategies in chapters 4 and 6.
¡ Prompt management—Once the relevant information is retrieved by the retriever, 
it needs to be combined, or augmented, with the original user query. Now, this 
may seem like a simple task at first glance. However, the construction of the 
prompt makes significant difference to the quality of the generated response. 
This component also falls in the gambit of prompt engineering. We will explore 
different prompting and prompt management strategies in chapter 4.
¡ LLM setup—At the end, LLMs are responsible for generating the final response. 
A RAG system may rely on more than one LLM. The LLMs can be the foundation 
(base) models that have been pretrained and generally available either open 
source, like those by Meta or Mistral, or through a managed service, like OpenAI 
o
27
The RAGOps Stack
Retrieved information
or context
Response or
answer
User query
or prompt
Context relevance
Is the retrieved
information or context
relevant to the user
query or prompt?
Groundedness
Is the response or
answer faithful to the
retrieved context?
Answer relevance
Is the answer
relevant to the user
query?
Figure 2.7  The triad of RAG evaluation proposed by TruEra. The three pivotal dimensions of RAG 
evaluation are the query, context, and response.
though RAG is aimed at reducing hallucinations, the system might still ignore the 
retrieved information. There are several reasons for it, which will be discussed in 
subsequent chapters.
¡ Between the final response (answer) and the user query (prompt)—Is the final response 
in line with the question the user had originally asked? To assess the overall effec-
tiveness of the system, the relevance of the final response to the original question 
is necessary.
There are several metrics that help assess each of these three dimen
28
Chapter 2  RAG systems and their design
1	 Data layer—Tools and platforms used to process and store data in the form of 
embeddings
2	 Model layer—Providers of proprietary or open source LLMs
3	 Prompt layer—Tools offering maintenance and evaluation of prompts
4	 Evaluation layer—Tools and frameworks providing evaluation metrics for RAG
5	 App orchestration—Frameworks that facilitate invocation of different components 
of the system
6	 Deployment layer—Cloud providers and platforms for deploying RAG apps
7	 Application layer—Hosting services for RAG apps
8	 Monitoring layer—Platforms offering continuous monitoring of RAG apps
Chapter 7 explores the various layers of infrastructure that support RAG systems.
2.7	
Caching, guardrails, security, and other layers
Finally, there are certain other components frequently used in RAG systems. These 
components address the problems of system latency, regulatory and ethical compli-
ances among other aspects.
¡ Caching—Caching is the process in 
29
Summary
Summary
¡ A RAG-enabled system consists of two main pipelines: the indexing and the gen-
eration pipeline.
¡ The indexing pipeline is responsible for creating and maintaining the knowl-
edge base, which involves data loading, text splitting, data conversion (embed-
dings), and data storage in a vector database.
¡ The generation pipeline manages real-time interactions by retrieving informa-
tion, augmenting queries, and generating responses using an LLM.
¡ Evaluation and monitoring are crucial components for the assessment of system 
performance, covering the relevance between the retrieved information and 
query, the final response and retrieved information, and the final response and 
the original query.
¡ The service infrastructure for RAG systems includes layers for data, models, 
prompts, evaluation, app orchestration, deployment, application hosting, and 
monitoring.
¡ Additional components such as caching, guardrails, and security measures are 
often employed to improv

Part 2
Creating RAG systems
Now that you are familiar with the fundamental idea of RAG and the 
components of a RAG system, the second part of the book will guide you through 
building a basic RAG system with the core pipelines and their evaluation. This 
part of the book not only offers theoretical details, but also simple code snippets 
that will provide you with hands-on experience in building a RAG pipeline.
In chapter 3, you’ll learn the details of the indexing pipeline and its four com-
ponents: loading, chunking, embeddings, and vector storage. Each of these com-
ponents has a variety of techniques to choose from. This chapter also discusses 
the suitability of these options for different use cases. Step by step, you’ll build an 
indexing pipeline and create the knowledge base for your RAG system. 
Chapter 4 talks about retrievers, prompting techniques, and using LLMs for 
output generation. These elements form the three components of the generation 
pipeline: retrieval, augment

33
3
Indexing pipeline: 
Creating a knowledge 
base for RAG
This chapter covers
¡ Data loading
¡ Text splitting or chunking
¡ Converting text to embeddings
¡ Storing embeddings in vector databases
¡ Examples in Python using LangChain
In chapter 2, we discussed the main components of retrieval-augmented generation 
(RAG) systems. You may recall that the indexing pipeline creates the knowledge 
base or the non-parametric memory of RAG applications. An indexing pipeline 
needs to be set up before the real-time user interaction with the large language 
model (LLM) can begin.
This chapter elaborates on the four components of the indexing pipeline. We begin 
by discussing data loading, which involves connecting to the source, extracting files, 
and parsing text. At this stage, we introduce a framework called LangChain, which 
has become increasingly popular in the LLM app developer community. Next, we 
elaborate on the need for data splitting or chunking and discuss chunking strategies.
34
Chapter 3  Indexing pipeline: Creating a knowledge base for RAG
Embeddings is an important design pattern in the world of AI and ML. We explore 
embeddings in detail and how they are relevant in the RAG context. Finally, we look at a 
new storage technique called vector storage and the databases that facilitate it. 
By the end of this chapter, you should have a solid understanding of how a knowl-
edge base, or the non-parametric memory of a RAG application, is created. We also 
embellish this chapter with snippets of Python code, so those of you who are so inclined 
can try a hands-on development of the indexing pipeline.
By the end of this chapter, you should
¡ Know how to extract data from sources.
¡ Get a deeper understanding of text-chunking strategies.
¡ Learn what embeddings are and how they are used.
¡ Gain knowledge of vector storage and vector databases.
¡ Have an end-to-end knowledge of setting up the indexing pipeline.
3.1	
Data loading
This section focuses on the first s
35
Data loading
for news, weather, and similar. Although LangChain has received some criti-
cism, it is still a good starting point for developers.
Installing LangChain 
To install LangChain (we’ll use the version 0.3.19 in this chapter) using pip, run
%pip install langchain==0.3.19
The langchain-community package contains third-party integrations. It is automati-
cally installed by LangChain, but in case it does not work, you can also install it sepa-
rately using pip:
%pip install langchain-community
Now that you have installed LangChain, we will use it to connect to Wikipedia and 
extract data from the page about the 2023 Cricket World Cup. For this task, we will use 
the AsyncHtmlLoader function from the document_loaders library in the langchain 
-community package. To run AsyncHtmlLoader, we will have to install another Python 
package called bs4:
#Installing bs4 package
%pip install bs4==0.0.2 --quiet
#Importing the AsyncHtmlLoader
from langchain_community.document_loaders import
36
Chapter 3  Indexing pipeline: Creating a knowledge base for RAG
Cup title………………………………………………… "datePublished":"2013-06-29T19:20:08Z","dateMo
dified":"2024-05-01T05:16:34Z","image":"https:\\/\\/upload.wikimedia.org\\/
wikipedia\\/en\\/e\\/eb\\/2023_CWC_Logo.svg","headline":"13th edition of the 
premier international cricket competition"}</script>\n</body>\n</html>', 
metadata={'source': 'https://en.wikipedia.org/wiki/2023_Cricket_World_Cup', 
'title': '2023 Cricket World Cup - Wikipedia', 'language': 'en'})]
The variable data is a list of documents that includes two elements: page_content and 
metadata. page_content contains the text sourced from the URL. You will notice that 
the text along with the relevant information also has newline characters (\n) and other 
HTML tags; however, metadata contains another important data aspect.
Metadata is information about the data (e.g., type, origin, and purpose). This can 
include a data summary; the way the data was created; who created it an
37
Data loading
the Creative Commons Attribution-ShareAlike License 4.0; additional terms 
may apply. By using this site, you agree to the Terms of Use and Privacy 
Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, 
Inc., a non-profit organization.    * Privacy policy   * About Wikipedia   * 
Disclaimers   * Contact Wikipedia   * Code of Conduct * Developers   * 
Statistics   * Cookie statement   * Mobile view    *
The text is more coherent now since we have removed the HTML part of the data. There 
can be further cleanup, such as removing special characters and other unnecessary 
information. Data cleaning also removes duplication. Yet another step to include in the 
data-loading stage can be masking of sensitive information such as PII (Personally Identi-
fiable Information) or company secrets. In some cases, a fact check may also be required.
The source for our data was Wikipedia (more precisely, a web address pointing to a 
Wikipedia page), and the format wa
38
Chapter 3  Indexing pipeline: Creating a knowledge base for RAG
We have now obtained data from the source and cleaned it to an extent. This Wikipe-
dia page that we have loaded has more than 8,000 words, alone. Imagine the number 
of words if we had multiple documents. For efficient management of information, we 
employ something called data splitting, which will be discussed in the next section.
3.2	
Data splitting (chunking)
Breaking down long pieces of text to manageable segments is called data splitting or 
chunking. This section discusses why chunking is necessary and the different chunking 
strategies. We also use functions from LangChain to illustrate a few examples. 
3.2.1	
Advantages of chunking
In the previous section, we loaded the data from a URL (a Wikipedia page) and 
extracted the text. It was a long piece of text of approximately 8,000 words. When it 
comes to overcoming the major limitations of using long pieces of text in LLM applica-
tions, chunking offers the fol
39
Data splitting (chunking)
3.2.2	
Chunking process
The chunking process can be divided into three steps, as illustrated in figure 3.2: 
1	 Divide the longer text into compact, meaningful units (e.g., sentences or 
paragraphs).
2	 Merge the smaller units into larger chunks until a specific size is achieved. After 
that, this chunk is treated as an independent segment of text.
3	 When creating a new chunk, include a part of the previous chunk at the start of 
the new chunk. This overlap is necessary to maintain contextual continuity. 
This process is also known as “small to big” chunking. 
-------
-------
-------
-------
-------
-------
-------
-------
-------
-------
-------
-------
-------
-------
-------
Dividing into
compact units
Loaded large document
Merging units into
larger chunks
Maintain overlap for
contextual continuity
Small to Big Chunking
-------
-------
-------
-------
-------
-------
-------
-------
-------
-------
1
2
3
Figure 3.2  Data-chunking process
3.2.3	
Chunking
40
Chapter 3  Indexing pipeline: Creating a knowledge base for RAG
¡ Split by character—Here, we specify a certain character, such as a newline char-
acter \n or a special character *, to determine how the text should be split. The 
text is split into a unit whenever this character is encountered. The chunk size 
is measured in the number of characters. We must choose the chunk size or the 
number of characters we need in each chunk. We can also choose the number of 
characters we need to overlap between two chunks. We will look at an example 
and demonstrate this method using CharacterTextSplitter from langchain 
.text_splitters. For this, we will take the same document that we loaded and 
transformed in the previous section from Wikipedia and store it in the variable 
html_data_transformed.
#import libraries
from langchain.text_splitters import CharacterTextSplitter
#Set the CharacterTextSplitter parameters
text_splitter = CharacterTextSplitter(
    separator="\n",    #The character 
41
Data splitting (chunking)
Splitting by character is a simple and effective way to create chunks. It is the 
first chunking method that anyone should try. However, sometimes, it may not be 
feasible to create chunks within the specified length. This is because the sequen-
tial occurrence of the character on which the text needs to be split is far apart. To 
address this problem, a recursive approach is employed.
¡ Recursively split by character—This method is quite like the split by character but 
instead of specifying a single character for splitting, we specify a list of characters. 
The approach initially tries creating chunks based on the first character. In case it 
is not able to create a chunk of the specified size using the first character, it then 
uses the next character to further break down chunks to the required size. This 
method ensures that chunks are largely created within the specified size. This 
method is recommended for generic texts. You can use RecursiveCharact
42
Chapter 3  Indexing pipeline: Creating a knowledge base for RAG
where data is inherently uniform, such as genetic sequences and service manuals, or 
uniformly structured reports such as survey responses.
Specialized chunking
Chunking aims to keep meaningful data together. If we are dealing with data in the 
form of HTML, Markdown, JSON, or even computer code, it makes more sense to 
split the data based on the structure rather than a fixed size. Another approach to 
chunking is to consider the format of the extracted and loaded data. A markdown 
file, for example, is organized by headers, a code written in a programming language 
such as Python or Java is organized by classes and functions, and likewise, HTML is 
organized in headers and sections. For such formats, a specialized chunking approach 
can be employed. LangChain offers classes such as MarkdownHeaderTextSplitter, 
HTMLHeader­TextSplitter, and RecursiveJsonSplitter, among others, for these 
formats. 
Here is a simple examp
43
Data splitting (chunking)
The advantage of specialized chunking is that chunk sizes are no longer limited by 
a fixed width. This feature helps in preserving the inherent structure of the data. 
Because the size of the chunks changes depending on the structure, this method is 
also sometimes called adaptive chunking. Specialized chunking works well in structured 
scenarios such as customer reviews or patient records where data can be of different 
lengths but should ideally be in the same chunk.
In the previous example, let’s see how many chunks have been created:
len(split_content)
>> 231
This method has given us 231 chunks from the URL. Chunking methods do not have 
to be exclusive. We can further chunk these 231 chunks using a fixed-size chunking 
method such as RecursiveCharacterTextSplitter.
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
separators=["\n\n","\n","."]
chunk_size=1000, chunk_overlap=100, 
)
final_
44
Chapter 3  Indexing pipeline: Creating a knowledge base for RAG
Semantic chunking
This idea, proposed by Greg Kamradt, questions two aspects of the previous chunking 
methods.
¡ Why should we have a predefined fixed size of chunks?
¡ Why don’t chunking methods take into consideration the actual meaning of 
content?
To address these problems, a method that looks at semantic similarity (or similarity in 
the meaning) between sentences is called semantic chunking. It first creates groups 
of three sentences and then merges groups that are similar in meaning. To find out 
the similarity in meaning, this method uses embeddings. (We will discuss embeddings 
in the next section.) This is still an experimental chunking technique. In LangChain, 
you can use the class SemanticChunker from the langchain_experimental.text_
splitter library. See figure 3.3 for examples of different chunking methods. 
Fixed-size chunking
Specialized chunking
Semantic chunking
Te length of chunks is uniform and
pr
45
Data splitting (chunking)
that comes up during the creation of the indexing pipeline. There are no guidelines or 
rules to answer this question. However, certain features of the application that you’re 
developing can guide you toward an effective strategy.
Nature of the content 
The type of data that you’re dealing with can be a guide for the chunking strategy. If 
your application uses data in a specific format such as code or HTML, a specialized 
chunking method is recommended. Not only that, whether you’re working with long 
documents such as whitepapers and reports or short-form content such as social media 
posts, tweets, and so on, can also guide the chunk size and overlap limits. If you’re 
using a diverse set of information sources, then you might have to use different meth-
ods for different sources.
Expected length and complexity of user query
The nature of the query that your RAG system is likely to receive also determines 
the chunking strategy. If your system expects a
46
Chapter 3  Indexing pipeline: Creating a knowledge base for RAG
3.3	
Data conversion (embeddings)
Computers, at their very core, do mathematical calculations. Mathematical calcula-
tions are done on numbers. Therefore, for a computer to process any kind of nonnu-
meric data such as text or image, it must be first converted into a numerical form. 
3.3.1	
What are embeddings?
Embeddings is a design pattern that is extremely helpful in the fields of data science, 
machine learning, and AI. Embeddings are vector representations of data. As a gen-
eral definition, embeddings are data that has been transformed into n-dimensional 
matrixes. The word embedding is a vector representation of words. I explain embed-
dings by using three words as an example: dog, bark, and fly.
NOTE  In physics and mathematics, the vector is an object that has a magni-
tude and a direction, like an arrow in space. The length of the arrow is the 
magnitude of the quantity and the direction that the arrow points 
47
Data conversion (embeddings)
2D vector space
Dog [3,3]
Bark [4,2]
Fly [6,1]
Red [-3, 5]
Love [-2,-2]
Increasing the number of
dimensions in the vector space
provides greater flexibility to
accurately map words.
Figure 3.5  Words in a 2D vector space
The goal of an embedding model is to convert words (or sentences/paragraphs) into 
n-dimensional vectors so that the words (or sentences/paragraphs) that are like each 
other in meaning lie close to each other in the vector space. See figure 3.6.
Dog
Bark
Fly
Embeddings
algorithm
[5,7,1,....]
[6,7,2,....]
[1,1,8,....]
Vector representation for “Dog”
Vector representation for “Bark”
Vector representation for “Fly”
n-dimension
embedding space
The goal of an embedding model is to
convert words (or sentences/paragraphs)
into n-dimensional vectors.
Figure 3.6  The process of embedding transforms data (such as text) into vectors and compresses the 
input information, which results in an embedding space specific to the training data.
An embeddi
48
Chapter 3  Indexing pipeline: Creating a knowledge base for RAG
¡ GloVe—Global Vectors for Word Representations is an unsupervised learning 
technique developed by researchers at Stanford University.
¡ FastText—FastText is an extension of Word2Vec developed by Facebook AI 
Research. It is particularly useful for handling misspellings and rare words.
¡ ELMo—Embeddings from Language Models was developed by researchers at 
Allen Institute for AI. ELMo embeddings have been shown to improve perfor-
mance on question answering and sentiment analysis tasks.
¡ BERT—Bidirectional Encoder Representations from Transformers, developed 
by researchers at Google, is a transformers-architecture-based model. It provides 
contextualized word embeddings by considering bidirectional context, achiev-
ing state-of-the-art performance on various NLP tasks.
Training a custom embeddings model can prove to be beneficial in some use cases 
where the scope is limited. Training an embeddings model that general
49
Data conversion (embeddings)
–	 voyage-large-2-instruct is a 1024-dimensional embeddings model that has 
become a leader in embeddings models.
–	 voyage-law-2 is a 1024-dimension model optimized for legal documents.
–	 voyage-code-2 is a 1536-dimension model optimized for code retrieval.
–	 voyage-large-2 is a 1536-dimension general-purpose model optimized for 
retrieval.
Voyage AI offers several free tokens before charging for using the embeddings 
models.
¡ Mistral AI embeddings—Mistral is the company behind LLMs such as Mistral and 
Mixtral. They offer a 1024-dimensional embeddings model known as mistral-
embed. This is an open source embeddings model.
¡ Cohere embeddings—Cohere, the developers of Command, Command R, and Com-
mand R + LLMs also offer a variety of embeddings models, which can be accessed 
via the Cohere API. Some of these are
–	 embed-english-v3.0 is a 1024-dimension model that works on embeddings for 
English only.
–	 embed-english-light-v3.0 is a lighter version
50
Chapter 3  Indexing pipeline: Creating a knowledge base for RAG
angle between the two vectors. Recall from trigonometry that the cosine of parallel 
lines (i.e., angle = 0o) is 1, and the cosine of a right angle (i.e., 90o) is 0. The cosine of 
the opposite lines (i.e., angle = 180o) is −1. Therefore, the cosine similarity lies between 
−1 and 1, where unrelated terms have a value close to 0, and related terms have a value 
close to 1. Terms that are opposite in meaning have a value of −1. See figure 3.7. 
10o
80o
175
o
Cos 10 = 0.985
Cos 80 = 0.173
Cos 175 = -0.996
Close to 1
Very similar
Close to 0
Unrelated
Close to -1
Opposite
80
Figure 3.7  Cosine similarity of vectors in 2D vector space
Yet another measure of similarity is the Euclidean distance between two vectors. Close 
vectors have a small Euclidean distance. It can be calculated using the following 
formula:
Distance (A, B) = sqrt((Ai-Bi)2), 
where i is the i-th dimension of the n-dimensional vectors
Different use cases o
51
Data conversion (embeddings)
Since we are focusing on RAG systems, here we examine using embeddings for text 
search— to find the document chunks that are closest to the user’s query. Let’s con-
tinue with our example of the Wikipedia page on the 2023 Cricket World Cup. In the 
last section, we created 67 chunks using a combination of specialized and fixed-width 
chunking. Now we will see how to create embeddings for each chunk. We will see how 
to use an open source as well as a proprietary embeddings model.
Here is the code example for creating embeddings using an open source embed-
dings model all-MPnet-base-v2 via Hugging Face: 
# Import HuggingFaceEmbeddings from embeddings library
from langchain_huggingface import HuggingFaceEmbeddings
# Instantiate the embeddings model. The embeddings model_name can be changed 
as desired
embeddings = 
HuggingFaceEmbeddings(
model_name="sentence-transformers/all-mpnet-base-v2"
)
# Create embeddings for all chunks
hf_embeddings = 
embeddings.e
52
Chapter 3  Indexing pipeline: Creating a knowledge base for RAG
Similarly, we can use a proprietary model such as the text-embedding-3-small model, 
hosted by OpenAI. The only prerequisite is obtaining an API key and setting up a bill-
ing account with OpenAI. 
# Install the langchain openai library
%pip install langchain-openai==0.3.7 --quiet
# Import OpenAIEmbeddings from the library
from langchain_openai import OpenAIEmbeddings
# Set the OPENAI_API_KEY as the environment variable
import os
os.environ["OPENAI_API_KEY"] = <YOUR_API_KEY>
# Instantiate the embeddings object
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
# Create embeddings for all chunks
openai_embeddings = 
embeddings.embed_documents(
[chunk.page_content for chunk in chunks]
)
#Check the length(dimension) of the embedding
len(openai_embedding[0])
>> 1536
This text-embedding-3-small model creates embeddings for the same chunks of dimen-
sion 1536.
There are several embeddings models available, and new 
53
Storage (vector databases)
data relevant to the user query. Once the embeddings have been created, they need to 
be stored in persistent memory for real-time access. To store embeddings, a new kind of 
database called a vector database have become increasingly popular.
3.4	
Storage (vector databases)
Now we are at the last step of the indexing pipeline. The data has been loaded, split, 
and converted to embeddings. To use this information repeatedly, we need to store it 
in memory so that it can be accessed on demand.
3.4.1	
What are vector databases? 
The evolution of databases can be traced back to the early days of computing. Data-
bases are organized collections of data, designed to be easily accessed, managed, and 
updated. Relational databases such as MySQL organize structured data into rows and 
columns. NoSQL databases such as MongoDB specialize in handling unstructured and 
semi-structured data. Graph databases such as Neo4j are optimized for querying graph 
data. In the sa
54
Chapter 3  Indexing pipeline: Creating a knowledge base for RAG
¡ Search platforms—Solr, Elastic Search, Open Search, and Apache Lucene are tradi-
tional text search platforms and engines built for full text search. They have now 
added vector similarity search capabilities to their existing search capabilities.
¡ Vector capabilities for SQL databases—Azure SQL, Postgres SQL(pgvector), Single-
Store, and CloudSQL are traditional SQL databases that have now added vector 
data-handling capabilities.
¡ Vector capabilities for NoSQL databases—Like SQL DBs, NoSQL DBs such as 
MongoDB have also added vector search capabilities.
¡ Graph databases with vector capabilities—Graph DBs such as Neo4j, have also 
opened new possibilities by adding vector capabilities, .
Using a vector index such as FAISS is supported by LangChain. To use FAISS, we first 
must install the faiss-cpu library. We will use the chunks already created in section 3.2 
and the OpenAI embeddings that we used in section 3.3
55
Storage (vector databases)
With this code, the 285 chunks of data have been converted to vector embeddings, and 
these embeddings are stored in a FAISS vector index. The FAISS vector index can also 
be saved to memory using the vector_store.save_local(folder_path,index_name) 
and FAISS.load_local(folder_path,index_name) functions. Let’s now take a cursory 
look at how a vector store can be used. We will take the original question that we have 
been asking from the beginning of this book: “Who won the 2023 Cricket World Cup?”
# Original Question
query = "Who won the 2023 Cricket World Cup?"
# Ranking the chunks in descending order of similarity
docs = vector_store.similarity_search(query)
# Printing one of the top-ranked chunk
print(docs[0].page_content)
Similarity search orders the chunks in descending order of similarity, meaning that 
the most similar chunks to the query are ranked on top. In the previous example, we 
can observe that the chunk that speaks about the world cup fina
56
Chapter 3  Indexing pipeline: Creating a knowledge base for RAG
¡ Simplicity vs. advanced features—Compare advanced algorithm optimizations, 
query features, and indexing versus how much complexity your use case necessi-
tates versus needs for simplicity.
¡ Cost—While you may incur regular costs in a fully managed solution, a self-hosted 
one might prove costlier if not managed well.
We have now completed an end-to-end indexing of a document. We continued with 
the same question (“Who won the 2023 Cricket World Cup?”) and the same external 
source—the Wikipedia page of the 2023 Cricket World Cup (https://mng.bz/yN4J). In 
this chapter, we started with the programmatic loading of this Wikipedia page extract-
ing the HTML document and then parsing the HTML document to extract. There-
after, we divided the text into small-sized chunks using a specialized and fixed-width 
chunking method. We converted these chunks into embeddings using OpenAI’s text-
embedding-003-large model. Finally, 
57
Summary
¡ Chunking can be fixed size, specialized (or adaptive), or semantic. Newer chunk-
ing methods are constantly being introduced.
¡ Your choice of the chunking strategy should be based on the nature of the con-
tent, expected length and complexity of user query, application use case, and the 
embeddings model being used. 
¡ A chunking strategy can include multiple methods.
Data conversion
¡ For processing, text needs to be converted into a numerical format.
¡ Embeddings are vector representations of data (words, sentences, documents, 
etc.).
¡ The goal of an embedding algorithm is to position similar data points close to 
each other in a vector space.
¡ Several pre-trained, open source and proprietary, embedding models are avail-
able for use.
¡ Embeddings models enable similarity search. Embeddings can be used for text 
search, clustering, ML models, and recommendation engines.
¡ The choice of embeddings is largely based on the use case and the cost 
implications.
¡ Vector da
58
4
Generation pipeline: 
Generating contextual 
LLM responses
This chapter covers
¡ Retrievers and retrieval methodologies
¡ Augmentation using prompt engineering 	
	
	 techniques
¡ Generation using LLMs
¡ Basic implementation of the RAG pipeline in 	
	 Python
In chapter 3, we discussed the creation of the knowledge base, or the non-
parametric memory of retrieval augmented generation (RAG)-based applications, 
via the indexing pipeline. To use this knowledge base for accurate and contextual 
responses, we need to create a generation pipeline that includes the steps of 
retrieval, augmentation, and generation.
This chapter elaborates on the three components of the generation pipeline. We 
begin by discussing the retrieval process, which primarily involves searching through 
the embeddings stored in vector databases of the knowledge base and returning a 
list of documents that closely match the input query of the user. You will also learn
59
Retrieval
about the concept of retrievers and a few retrieval algorithms. Next, we move to the aug-
mentation step. At this point, it is also beneficial to understand different prompt engi-
neering frameworks used with RAG. Finally, as part of the generation step, we discuss 
a few stages of the LLM life cycle, such as using foundation models versus supervised 
fine-tuning, models of different sizes, and open source versus proprietary models in 
the RAG context. In each of these steps, we also highlight the benefits and drawbacks of 
different methods.
By the end of this chapter, you will be equipped with an understanding of the two 
foundational pipelines of a RAG system. You should also be ready to build a basic RAG 
system.
By the end of this chapter, you should
¡ Know several retrievers used in RAG.
¡ Get an understanding of augmentation using prompt engineering.
¡ Learn some details about how LLMs are used in the context of RAG.
¡ Have an end-to-end knowledge of setting up a ba
60
Chapter 4  Generation pipeline: Generating contextual LLM responses
4.2. You can imagine that retrieval is a crucial step since the quality of the retrieved 
information directly affects the quality of the output that will be generated.  
LLM
Response
Retriever
User
Question
{Question +
Information}
prompts
Knowledge base
Non-parametric memory
created via the indexing
pipeline
Retrieval
Augmentation
Generation
Retrieval: Search and fetch relevant information
from the knowledge base.
Augmentation: Add the retrieved information to
the original user question.
Generate: Generate the response using an LLM
based on the augmented user question and
retrieved information.
Search
Fetch
information
1
2
3
Figure 4.1  Generation pipeline overview with the three components (i.e., retrieval, augmentation, and 
generation)
User query
Retriever
Knowledge base
Top n documents ranked by
relevance to the user query
Retriever output
All documents
Retriever receives the input
query, searches through the

61
Retrieval
We have already discussed embeddings in chapter 3 while building the indexing pipe-
line. Using embeddings, we can find documents that match the user query. Embed-
dings is one method in which retrieval can happen. There are other methods, too, and 
it is worth spending some time understanding different types of retrieval methods and 
the way they calculate the results. 
This section on retrievers first discusses different retrieval algorithms and their sig-
nificance in the context of RAG. In RAG systems, one or more retrieval methods can 
be used to build the retriever component. Next, we look at a few examples of prebuilt 
retrievers that can be used directly through a framework (e.g., LangChain). These 
retrievers are integrated with services such as databases, cloud providers, or third-party 
information sources. Finally, we will close this section by building a very simple retriever 
in LangChain using Python. We will continue to demonstrate with this example the 
au
62
Chapter 4  Generation pipeline: Generating contextual LLM responses
Term Frequency-Inverse Document Frequency
Term Frequency–Inverse Document Frequency (TF-IDF) is a statistical measure used 
to evaluate the importance of a word in a document relative to a collection of docu-
ments (corpus). It assigns higher weights to words that appear frequently in a docu-
ment but infrequently across the corpus. Figure 4.3 illustrates how TF-IDF is calculated 
for a unigram search term.
Components of TF-IDF
Term frequency (TF)
Measures how frequently term t appears in document d
Inverse document frequency (IDF)
Measures how important term t is within the entire corpus D
TF (t,d) = 
Number of times term t appears in document d
Total number of terms in document d
IDF (t,D) = 
Total number of documents D
Number of documents containing term t
(
(
log
TF-IDF(t,d,D)=TF(t,d)×IDF(t,D)
TF-IDF
Product of TF & IDF
Documents (D)
d1 = Australia won the Cricket World Cup 2023
d2 = India and Australia played i
63
Retrieval
TF-IDF not only can be used for unigrams, but also for phrases (n-grams). However, 
even TF-IDF improves on simpler search methods by emphasizing unique words, it still 
lacks context and word-order consideration, making it less suitable for complex tasks 
like RAG.
Best Match 25
Best Match 25 (BM25) is an advanced probabilistic model used to rank documents 
based on the query terms appearing in each document. It is part of the family of proba-
bilistic information retrieval models and is considered an advancement over the classic 
TF-IDF model. The improvement that BM25 brings is that it adjusts for the length of 
the documents so that longer documents do not unfairly get higher scores. Figure 4.4 
illustrates the BM25 calculation.
Calculating BM25
BM25(t,d,D) = IDF(t,D) x 
TF(t,d) + (k) x (1-b + b x         )
|d|
avgdl
TF(t,d) x (k+1)
• TF(t,d) is the term frequency of ‘t’ in document ‘d’.
• IDF(t,D) is the inverse document frequency of term in the
  corpus.
• |d| is the
64
Chapter 4  Generation pipeline: Generating contextual LLM responses
For long queries instead of single keywords, the BM25 value is calculated for each word 
in the query, and the final BM25 value for the query is a summation of the values for all 
the words. BM25 is a powerful tool in traditional IR, but it still doesn’t capture the full 
semantic meaning of queries and documents required for RAG applications. BM25 is 
generally used in RAG for quick initial retrieval, and then a more powerful retriever is 
used to re-rank the results. We will learn about re-ranking later in chapter 6, when we 
discuss advanced strategies for RAG.
Static word embeddings
Static embeddings such as Word2Vec and GloVe represent words as dense vectors in 
a continuous vector space, capturing semantic relationships based on context. For 
instance, “king” − “man” + “woman” approximates “queen.” These embeddings can 
capture nuances such as similarity and analogy, which BoW, TF-IDF, and BM25 miss. 
However,
65
Retrieval
Woman
Apple
Man
Queen
Technology
King
Fruit
Woman
Apple
Man
Queen
Technology
King
Fruit
Q : What are the health benefits of Apple?
Q : What is the share price of Apple?
Woman
Apple
Man
Queen
Technology
King
Fruit
Static embeddings
Contextual embeddings
Woman
Apple
Man
Queen
Technology
King
Fruit
• Vectors do not change with the input
    query
• Computationally cheaper but do not work
    well for words that have multiple meanings
• Vectors calculated dynamically based
    on the input query
• Capture the context very well but are
    computationally intensive
l
a
Figure 4.5  Static vs. contextual embeddings
¡ Hybrid retrieval—Combines sparse and dense methods for balanced efficiency 
and effectiveness (examples: ColBERT, COIL)
¡ Cross-encoder retrieval—Directly compares query-document pairs using trans-
former models (example: BERT-based re-rankers)
¡ Graph-based retrieval—Uses graph structures to model relationships between doc-
uments (examples: TextGraphs, graph neural
66
Chapter 4  Generation pipeline: Generating contextual LLM responses
Doc 2
Doc 1
D
Doc 2
Doc 1
Doc 7
a
c
1
Doc 6
------
Doc 7
Doc 5
Doc 4
Doc 4
Doc 5
Doc 6
Doc 3
Q
b
x
Similarity (Doc 6, Q) = Cos(a)
Similarity (Doc 5, Q) = Cos(b)
Similarity (Doc 7, Q) = Cos(c)
----------------------------------
----------------------------------
Similarity (Doc n, Q) = Cos(x)
Similarity calculation
Ranking
Cos(a) > Cos(b) > Cos(c) >......>
Cos(x)
Doc n
Doc n
k = 3
Instruct the retriever to
fetch the top k results
only.
Query vector : 2D embeddings representation of the
user query
Doc 4
Doc 5
Doc 7
Doc 6
Doc 3
2D embeddings representation of the knowledge base
Source data
User query
Q
Embeddings-based retrieval
2
3
4
c
a
Embeddings
model
Indexing
pipeline
Figure 4.6  Similarity calculation and results ranking in embeddings-based retrieval technique
Table 4.1 notes the weaknesses and strengths of different retrievers. While contextual 
embeddings are the only ones you need to know to get started with R
67
Retrieval
Technique
Key feature
Strengths
Weaknesses
Suitability for RAG
TF-IDF
Term weighting 
based on docu-
ment and corpus 
frequency
Improved rele-
vance ranking 
over BoW
Still ignores seman-
tics and word 
relationships
Low–medium: 
Better than BoW 
but limited; used 
in hybrid retrieval
BM25
Advanced rank-
ing function 
with length 
normalization
Robust perfor-
mance; industry 
standard
Limited semantic 
understanding
Medium: Good 
baseline for sim-
ple RAG; used in 
hybrid retrieval.
Static 
embeddings
Fixed dense 
vector 
representations
Captures some 
semantic 
relationships
Context-indepen-
dent; limited in 
polysemy  
handling
Medium: Intro-
duces basic 
semantics
Contextual 
embeddings
Context-aware 
dense 
representations
Rich semantic 
understanding; 
handles polysemy
Computationally 
intensive
High: Excellent 
semantic capture
Learned sparse 
retrievers
Neural-net-
work-gener-
ated sparse 
representations
Efficient, inter-
pretable, and has 
some semantic 
understan
68
Chapter 4  Generation pipeline: Generating contextual LLM responses
For RAG, LangChain provides many integrations where the algorithms such as 
TF-IDF, embeddings and similarity search, and BM25 have been abstracted as retrievers 
for developers to use. We have already seen the ones for TF-IDF and BM25. Some of the 
other popular retrievers are described in the following sections.
Vector stores and databases as retrievers
Vector stores can act as the retrievers, taking away the responsibility from the devel-
oper to convert the query vector into embeddings by calculating similarity and ranking 
the results. FAISS is typically used in tandem with a contextual embedding model for 
retrieval. Other vector DBs such as PineCone, Milvus, and Weaviate provide hybrid 
search functionality by combining dense retrieval methods such as embeddings and 
sparse methods such as BM25 and SPLADE.
Cloud providers
Cloud providers Azure, AWS, and Google also offer their retrievers. Integration with 
Am
69
Augmentation
os.environ["OPENAI_API_KEY"] = <YOUR_API_KEY>
# Instantiate the embeddings object
embeddings=OpenAIEmbeddings(model="text-embedding-3-small")
# Load the database stored in the local directory
vector_store=FAISS.load_local(
folder_path="../../Assets/Data", 
index_name="CWC_index",
embeddings=embeddings, 
allow_dangerous_deserialization=True
)
# Original Question
query = "Who won the 2023 Cricket World Cup?"
# Ranking the chunks in descending order of similarity
retrieved_docs = vector_store.similarity_search(query, k=2)
This similarity_search () function returns a list of matching documents ordered by 
a score. This score is a quantification of the similarity between the query and the docu-
ment and is hence called the similarity score. In this example, the vector index’s inbuilt 
similarity search feature was used for retrieval. As one of the retrievers we discussed 
in section 4.2.2, the vector store itself acted as the retriever. K=2 tells the function to 
retrieve th
70
Chapter 4  Generation pipeline: Generating contextual LLM responses
the query. However, some nuanced augmentation techniques help improve the qual-
ity of the generated results. See figure 4.7 for an example of simple augmentation.
Retriever output
User query
Augmentation
Augmented prompt for LLM
Figure 4.7  Simple augmentation combines the user query with retrieved documents to send to the LLM.
4.3.1	
RAG prompt engineering techniques
Prompt engineering as a discipline has, sometimes, been dismissed as being too simple 
to be called engineering. You may have heard the phrase, “English is the new program-
ming language.” Interaction with LLMs is indeed in natural language. However, what 
is also true is that the principles of programming are not the language in which code is 
written but the logic in which the machine is instructed. With that in mind, let’s exam-
ine different logical approaches that can be taken to augment the user query with the 
retrieved information. 
Contextual
71
Augmentation
External
context
provided
Figure 4.8  Information is augmented to the original question with an added instruction.
relevant to the user query. The retriever might still fetch some documents that are the 
closest to the user query. In these cases, the chances of hallucination increase because 
the LLM will still try to follow the instructions for answering the question. To avoid 
this scenario, an additional instruction is added, which tells the LLM not to answer if 
the retrieved document does not have proper information to answer the user ques-
tion (something like, “If the question cannot be answered based on the provided con-
text, say I don’t know.”). In the context of RAG, this technique is particularly valuable 
because it ensures that the model’s responses are grounded in the retrieved informa-
tion. If the relevant information hasn’t been retrieved or isn’t present in the knowl-
edge base, the model is instructed to acknowledge this lack of information rather th
72
Chapter 4  Generation pipeline: Generating contextual LLM responses
This technique is called few-shot prompting. Here “shot” refers to the examples given 
in the prompt. Figure 4.9 illustrates a prompt that includes two examples with the 
question.
Answer the given question using only the provided context. Follow the
format in these examples:
Question: Who won the first Cricket World Cup?
Context: The inaugural Cricket World Cup was held in England in 1975.
The West Indies emerged victorious, defeating Australia in the final at
Lord's Cricket Ground.
Answer: The West Indies won the first Cricket World Cup in 1975.
Question: Who has scored the most runs in Test cricket?
Context: As of 2022, Sachin Tendulkar of India holds the record for the
most runs in Test cricket, accumulating 15,921 runs over his 24-year
career from 1989 to 2013.
Answer: Sachin Tendulkar has scored the most runs in Test cricket, with
15,921 runs.
Now, answer the following question using the given context:
Questio
73
Augmentation
Question
Retrieved
context
Reasoning
steps
Figure 4.10  Chain-of-thought (CoT) prompting for reasoning tasks
74
Chapter 4  Generation pipeline: Generating contextual LLM responses
The CoT prompting approach can also be combined with the few-shot prompting 
technique, where a few examples of reasoning are provided before the final question. 
Creating these examples is a manually intensive task. In auto-CoT, the examples are 
also created using an LLM.
Other advanced prompting techniques
Prompt engineering is becoming an increasingly intricate discipline. Ongoing research 
constantly presents new improvements in prompting techniques. To dive deeper into 
prompt engineering, let’s check out some of the following techniques: 
¡ Self-consistency—While CoT uses a single reasoning chain in CoT prompting, 
self-consistency aims to sample multiple diverse reasoning paths and use their 
respective generations to arrive at the most consistent answer.
¡ Generated knowledge prompting—This technique explores the idea of prompt-
based knowledge generation by dynamically constructing relevant knowledge 
chai
75
Augmentation
Table 4.2  Comparison of prompting techniques for augmentation 
Technique
Description
Key advantage
Best use case
Complexity
Contextual 
prompting
Adds retrieved 
information to the 
prompt with instruc-
tions to focus 
on the provided 
context
Ensures focus 
on relevant 
information
General RAG 
queries
Low
Controlled gener-
ation prompting
Instructs the 
model to say “I 
don’t know” when 
information is not 
available
Reduces hallucina-
tion risk
When accuracy is 
critical
Low
Few-shot 
prompting
Provides examples 
in the prompt to 
guide response 
format and style
Improves output 
consistency and for-
mat adherence
When a specific 
output format is 
required
Medium
Chain-of-thought 
(CoT) prompting
Introduces inter-
mediate reasoning 
steps
Improves perfor-
mance on complex 
reasoning tasks
Complex queries 
requiring step-by-
step analysis
Medium
Self-consistency
Samples multiple 
diverse reasoning 
paths
Improves answer 
consistency and 
accuracy
Tasks with multiple
76
Chapter 4  Generation pipeline: Generating contextual LLM responses
4.3.2	
A simple augmentation prompt creation
In section 4.2.3, we were able to implement a FAISS-based retriever using OpenAI 
embeddings. We will now make use of this retriever and create the augmentation 
prompt:
# Import FAISS class from vectorstore library
from langchain_community.vectorstores import FAISS
# Import OpenAIEmbeddings from the library
from langchain_openai import OpenAIEmbeddings
# Set the OPENAI_API_KEY as the environment variable
import os
os.environ["OPENAI_API_KEY"] = <YOUR_API_KEY>
# Instantiate the embeddings object
embeddings=OpenAIEmbeddings(model="text-embedding-3-small")
# Load the database stored in the local directory
vector_store=FAISS.load_local(
folder_path="../../Assets/Data", 
index_name="CWC_index",
embeddings=embeddings, 
allow_dangerous_deserialization=True
)
# Original Question
query = "Who won the 2023 Cricket World Cup?"
# Ranking the chunks in descending order of similarity

77
Generation
With the augmentation step complete, we are now ready to send the prompt to the 
LLM for the generation of the desired outcome. You will now learn how LLMs gener-
ate text and the nuances of generation. 
4.4	
Generation
Generation is the final step of this pipeline. While LLMs may be used in any of the pre-
vious steps, the generation step relies completely on the LLM. The most popular LLMs 
are the ones being developed by OpenAI, Anthropic, Meta, Google, Microsoft, and 
Mistral, among other developers. While text generation is the core capability of LLMs, 
we are now seeing multimodal models that can handle images and audio along with 
text. Simultaneously, researchers are developing faster and smaller models. 
In this section, we will discuss the factors that can help choose a language model 
for your RAG system. We will then continue with our example of the retriever and aug-
mented prompt we have built so far and complete it by adding the generation step.
4.4.1	
Categ
78
Chapter 4  Generation pipeline: Generating contextual LLM responses
Pre-trained
LLM
PROMPT [....], COMPLETION[....]
Fine-tuned model
Summarize the following text:
[Example text]
[Example completion]
Translate this sentence to ....
[Example text]
[Example completion]
PROMPT [....], COMPLETION[....]
PROMPT [....], COMPLETION[....]
PROMPT [....], COMPLETION[....]
PROMPT [....], COMPLETION[....]
PROMPT [....], COMPLETION[....]
PROMPT [....], COMPLETION[....]
PROMPT [....], COMPLETION[....]
......
PROMPT [....], COMPLETION[....]
Training dataset
PROMPT [....], COMPLETION[....]
......
PROMPT [....], COMPLETION[....]
PROMPT [....], COMPLETION[....]
......
PROMPT [....], COMPLETION[....]
Validation
Test
PROMPT [....], COMPLETION[....]
LLM completion
Actual Label
Loss : Cross entropy
Validation
accuracy
Test
accuracy
PROMPT [....], COMPLETION[....]
PROMPT [....], COMPLETION[....]
-----
PROMPT [....], COMPLETION[....]
Base model
Task-specific examples
Pre-trained
LLM
Fine-tuned
LLM
trained on
79
Generation
¡ Ethical alignment—A fine-tuned model allows for better control over the responses 
in adherence to ethical guidelines and even certain privacy aspects.
A summary of the criteria is presented in table 4.3.
Table 4.3  Criteria for choosing between foundation and fine-tuned models
Criteria
Better suitability
Explanation
Domain 
specificity
Fine-tuned models
Better performance for specialized applications (e.g., patient 
records and instruction manuals)
Retrieval 
integration
Fine-tuned models
Can be trained to better utilize retrieved information
Deployment 
speed
Foundation models
Quicker deployment with no additional training required
Customization 
of responses
Fine-tuned models
Better adherence to specific format, style, tone, or vocabu-
lary requirements
Resource 
efficiency
Foundation models
Requires less storage and computational resources
Ethical 
alignment
Fine-tuned models
Allows better control over responses to ethical guidelines 
and privacy
Fine-tuned models g
80
Chapter 4  Generation pipeline: Generating contextual LLM responses
control over fine-tuning is also something that open source LLMs allow for. Cus-
tomization of proprietary models is limited to API capabilities.
¡ Ease of use—Proprietary models, however, are much easier to use. Some of the 
models such as OpenAI, Cohere, and similar offer optimized, prebuilt RAG 
solutions.
¡ Deployment flexibility—Open source models can be deployed according to your 
preference (private cloud, on-premises), while proprietary models are man-
aged by the providers. This also has a bearing on data security and privacy. 
Most proprietary model providers are now offering multiple deployment 
options.
¡ Cost—Open source LLMs may come with upfront infrastructure costs, while pro-
prietary models are priced based on usage. Long-term costs and query volumes 
are considerations to choose between open source and proprietary models. 
Large-scale deployments may favor the use of open source models.
The choice
81
Generation
¡ Gemini series by Google (https://mng.bz/eBnJ)
¡ Command R series by Cohere (https://cohere.com/command)
Some of open source models are 
¡ Llama series by Meta (https://llama.meta.com/)
¡ Mistral (https://docs.mistral.ai/getting-started/models/)
Model sizes
LLMs come in various sizes, typically measured by the number of parameters they 
contain. The size of the model greatly affects the capabilities along with the resource 
requirements. 
Larger models have several billion, even trillions, of parameters. These models 
exhibit superior performance in reasoning abilities, and language understanding, and 
have broader knowledge. They can generate more coherent text, and their responses 
are contextually more accurate. However, these larger models have significantly high 
computation, storage, and energy requirements.
Smaller models with parameter sizes in millions or a few billion offer benefits such 
as faster inference times, lower resource usage, and easier deployment on
82
Chapter 4  Generation pipeline: Generating contextual LLM responses
Table 4.5  Criteria for choosing between small and large models
Criteria
Better suitability
Explanation
Resource 
constraints
Small models
Lower resource usage; suitable for lightweight RAG applications
Reasoning 
capability
Large models
Better for complex reasoning tasks and handling ambiguity in 
retrieved information
Deployment 
options
Small models
More flexible; can be deployed on edge devices and 
resource-constrained environments
Context 
handling
Large models
Better at integrating multiple pieces of retrieved information; 
longer context windows
Query diversity
Large models
Handle diverse and unpredictable query types better
Inference 
speed
Small models
Faster inference times; suitable for applications requiring quick 
responses
Examples of popular small language models are:
¡ Phi-3 by Microsoft (https://azure.microsoft.com/en-us/products/phi-3)
¡ Gemma by Google (https://ai.google.dev/gemma)
The choice of 
83
Generation
embeddings=OpenAIEmbeddings(model="text-embedding-3-small")
# Load the database stored in the local directory
vector_store=FAISS.load_local(
    folder_path="../../Assets/Data", 
    index_name="CWC_index",
    embeddings=embeddings, 
    allow_dangerous_deserialization=True
    )
# Original Question
query = "Who won the 2023 Cricket World Cup?"
# Ranking the chunks in descending order of similarity
retrieved_docs = vector_store.similarity_search(query, k=2)
# Selecting the first chunk as the retrieved information
retrieved_context= retrieved_docs[0].page_content
# Creating the prompt
augmented_prompt=f"""
Given the context below, answer the question.
Question: {query} 
Context : {retrieved_context}
Remember to answer only based on the context provided and not from any other 
source. 
If the question cannot be answered based on the provided context, say I don't 
know.
"""
# Importing the OpenAI library from langchain
from langchain_openai import ChatOpenAI
# Instantiate t
84
Chapter 4  Generation pipeline: Generating contextual LLM responses
And there it is. We have built a generation pipeline, albeit a very simple one. It can now 
fetch information from the knowledge base and generate an answer pertinent to the 
question asked and rooted in the knowledge base. Try asking a different question to 
see how well the pipeline generalizes.
We have now covered all three steps—retrieval, augmentation, and generation—
of the generation pipeline. With the knowledge of the indexing pipeline (covered in 
chapter 3) and the generation pipeline, you are now all set to create a basic RAG system. 
What we have discussed so far can be termed a naïve RAG implementation. Naïve RAG can 
be marred by inaccuracies. It can be inefficient in retrieving and ranking information 
correctly. The LLM can ignore the retrieved information and still hallucinate. To dis-
cuss and address these challenges, in chapter 6, we examine advanced strategies that 
allow for more complex and be
85
Summary
¡ Vector stores and databases (e.g., FAISS, PineCone, Milvus, Weaviate), cloud 
provider solutions (e.g., Amazon Kendra, Azure AI Search, Google Vertex AI 
Search), and web information resources (e.g., Wikipedia, Arxiv, AskNews) are 
some of the popular retriever integrations provided by LangChain.
¡ The choice of retriever depends on factors such as accuracy, speed, and compati-
bility with the indexing method.
Augmentation
¡ Augmentation combines the user query with retrieved information to create a 
prompt for the LLM.
¡ Prompt engineering is crucial for effective augmentation, aiming for accuracy 
and relevance in LLM responses.
¡ Key prompt engineering techniques for RAG include
–	 Contextual prompting—Adding retrieved information with instructions to focus 
on the provided context.
–	 Controlled generation prompting—Instructing the LLM to admit lack of knowl-
edge when information is insufficient.
–	 Few-shot prompting—Providing examples to guide the LLM’s response for
86
Chapter 4  Generation pipeline: Generating contextual LLM responses
Smaller models allow faster inference, lower resource usage, and are easier to 
deploy on edge devices or resource-constrained environments but do not have 
the same language understanding abilities as large models.
¡ Popular LLMs include offerings from OpenAI, Anthropic, Google, and similar, 
and open source models are available through platforms such as Hugging Face.
¡ The choice of LLM depends on factors such as performance requirements, 
resource constraints, deployment environment, and data sensitivity.
¡ The choice of LLM for RAG systems requires careful consideration, experimen-
tation, and potential adaptation based on performance.
87
5
RAG evaluation: 
Accuracy, relevance, 
and faithfulness
This chapter covers
¡ The need and requirements for evaluating RAG 	
	 pipelines
¡ Metrics, frameworks, and benchmarks for RAG 	
	 evaluation
¡ Current limitations and future course of RAG 	
	 evaluation
Chapters 3 and 4 discussed the development of retrieval-augmented generation 
(RAG) systems using the indexing and generation pipelines. RAG promises to 
reduce hallucinations and ground the large language model (LLM) responses in 
the provided context, which is done by creating a non-parametric memory or knowl-
edge base for the system and then retrieving information from it. 
This chapter covers the methods used to evaluate how well the RAG system is func-
tioning. We need to make sure that the components of the two RAG pipelines are 
performing per the expectations. At a high level, we need to ensure that the infor-
mation being retrieved is relevant to the input query and that the LLM is generating
88
Chapter 5  RAG evaluation: Accuracy, relevance, and faithfulness
responses grounded in the retrieved context. To this end, there have been several 
frameworks developed over time. Here we discuss some popular frameworks and the 
metrics they calculate. 
There is also a second aspect to evaluation. While the frameworks allow for the cal-
culation of metrics, how do you make sure that your RAG pipelines are working better 
than those developed by other developers? The evaluations cannot be done in isola-
tion. For this purpose, several benchmarks have been established. These benchmarks 
evaluate the RAG systems on preset data, such as question–answer sets, for accurate 
comparison of different RAG pipelines. These benchmarks help developers evaluate 
the performance of their systems vis-à-vis those developed by other developers. 
Finally, like RAG techniques, the research on RAG evaluations is still in progress. 
There are still some limitations in the current set of evaluation parame
89
Key aspects of RAG evaluation
produce measurable outcomes). Here are several questions we need to ask ourselves 
about these two processes: 
¡ How good is the retrieval of the context from the knowledge base? 
¡ Is it relevant to the query?
¡ How much noise (irrelevant information) is present?
¡ How good is the generated response? 
¡ Is the response grounded in the provided context? 
¡ Is the response relevant to the query?
You can ask many more questions such as these to assess the performance of your RAG 
system. Contemporary research has discovered certain scores to assess the quality and 
abilities of a RAG system. The following sections discuss three predominant quality 
scores and four main abilities.
5.1.1	
Quality scores
There are three quality score dimensions prevalent in the discourse on RAG evalua-
tion. They measure the quality of retrieval and generation:
¡ Context relevance—This dimension evaluates how relevant the retrieved informa-
tion or context is to the user que
90
Chapter 5  RAG evaluation: Accuracy, relevance, and faithfulness
¡ Information integration—To obtain a comprehensive answer to a user query, it is 
also very likely the information must be retrieved from multiple documents. This 
ability of the system to assimilate information from multiple documents is called 
information integration.
¡ Counterfactual robustness—Sometimes the information in the knowledge base 
might itself be inaccurate. A high-quality RAG system should be able to address 
this problem and reject known inaccuracies in the retrieved information. This 
ability is known as counterfactual robustness.
Noise robustness is an ability that the retrieval component should possess, and other 
abilities are largely related to the generation component. 
Apart from these, latency is another often-mentioned capability. Although it is a 
non-functional requirement, it is quite critical in generative AI applications. Latency 
is the delay that happens between the user query and the
91
Evaluation metrics
Benchmarks are standardized datasets and their evaluation metrics used to measure 
the performance of RAG systems. Benchmarks provide a common ground for com-
paring different RAG approaches. They ensure consistency across the evaluations by 
considering a fixed set of tasks and their evaluation criteria. For example, HotpotQA 
focuses on multi-hop reasoning and retrieval capabilities using metrics such as Exact 
Match and F1 scores.
Benchmarks are used to establish a baseline for performance and identify strengths/
weaknesses in specific tasks or domains. We will discuss a few benchmarks and their 
characteristics in section 5.4
Developers can use frameworks to integrate evaluation in their development process 
and use benchmarks to compare their development with established standards. The 
frameworks and benchmarks both calculate metrics that focus on retrieval and the RAG 
quality scores. We will begin our discussion about the metrics in the next section before
92
Chapter 5  RAG evaluation: Accuracy, relevance, and faithfulness
query, which can lead to misleadingly high accuracy scores. It does not consider the 
ranking of the retrieved results.
Precision
Precision focuses on the quality of the retrieved results. It measures the proportion 
of retrieved documents relevant to the user query. It answers the question, “Of all the 
documents that were retrieved, how many were relevant?”
A higher precision means that the retriever is performing well and retrieving mostly 
relevant documents.
Precision@k
Precision@k is a variation of precision that measures the proportion of relevant docu-
ments among the top ‘k’ retrieved results. It is particularly important because it focuses 
on the top results rather than all the retrieved documents. For RAG, it is important 
because only the top results are most likely to be used for augmentation. For example, 
if you restrict your RAG system to use only the top five retrieved documents for context 
augmentat
93
Evaluation metrics
Like precision, recall also doesn’t consider the ranking of the retrieved documents. 
It can also be misleading as retrieving all documents in the knowledge base will result in 
a perfect recall value. Figure 5.1 visualizes various precision and recall scenarios.
REL
tot_ret
Total number of documents retrieved = tot_ret
REL
REL
E
RE
REL
REL
REL
High recall
Almost all the relevant
documents have been
retrieved.
High recall
Almost all the relevant
documents have been
retrieved.
High precision
Almost all the retrieved
documents are relevant.
Total number of relevant documents in the knowledge
base = REL
rel_ret
Total number of relevant documents retrieved =
rel_ret
Precision = rel_ret / tot_ret
Recall = rel_ret / REL
Knowledge base
High-precision low recall
Low-precision high recall
High-precision high recall
Low-precision low recall
High precision
Almost all the retrieved
documents are relevant.
Low recall
A very low proportion of all
relevant documents have
been re
94
Chapter 5  RAG evaluation: Accuracy, relevance, and faithfulness
The equation is such that the F1-score penalizes either variable having a low score; 
a high F1 score is only possible when both recall and precision values are high. This 
means that the score cannot be positively skewed by a single variable. Figure 5.2 illus-
trates how the F1-score balances precision and recall.
0
High precision = 0.9
Low recall = 0.1
High recall = 0.9
Low precision = 0.1
Med recall = 0.5
Med precision = 0.5
Precision
low
high
Recall
low
high
med
med
Low F1 score = 0.2
Low F1 score = 0.2
Med F1 score = 0.5
High recall = 0.9
High precision = 0.9
High F1 score = 0.9
F1 score is low if either
recall or precision is low.
1
Figure 5.2  F1-score balances precision and recall. A medium value of both precision and recall gets a 
higher F1-score than if one value is very high and the other is very low.
F1-score provides a single, balanced measure that can be used to easily compare differ-
ent systems. Howeve
95
Evaluation metrics
where N is the total number of queries, and ranki
 is the rank of the first relevant docu-
ment of the i-th query.
MRR is particularly useful when you’re interested in how quickly the system can find 
a relevant document and consider the ranking of the results. However, since it doesn’t 
look at anything beyond the first relevant result, it may not be useful when multiple rele-
vant results are important. Figure 5.3 shows how the mean reciprocal rank is calculated.
Query 1:
1
2
3
4
5
Query 2:
Rank of 1st relevant
3
1
Reciprocal
1/3
1
Query 3:
2
1/2
MRR =
13/24 = 0.54
  
Query 4:
3
1/3
A relevant result on rank 1 shows perfect reciprocal rank
Considers only the first relevant result
Doesn’t account for number of relevant results
1/3
1
1/2
1/3
4
+
+
+
=
Figure 5.3  MRR considers the ranking but doesn’t consider all the documents.
Mean average precision
Mean average precision, or MAP, is a metric that combines precision and recall at dif-
ferent cut-off levels of ‘k’
96
Chapter 5  RAG evaluation: Accuracy, relevance, and faithfulness
MAP provides a single measure of quality across recall levels. It is quite suitable when 
result ranking is important but complex to calculate. Let’s look at an example MAP 
calculation in figure 5.4.
1
2
3
5
6
7
8
=
Query 1
0.7042
0.6500
0.8333
0.7042
0.7125
MAP = 0.7208
Mean
1/2
2/4
3/5
3/7
4/8
1
1
1
2
2
3
2
4
3
5
3
6
3
7
4
8
1/1
2/3
3/6
1
1
1
1
0
0
0
4
Total number of relevant documents = 4
Precision
AP
(Average precision)
= 0.7042
AP
Relevance
0
Relevant docs
Total docs
1
(
0.67
0.6
0.5
0
0
0
0
)
+
+
+
+
+
+
+
4
(Total number of relevant
documents)
Query 1
Query 2
Query 3
Query 4
Query 5
Figure 5.4  MAP considers all the retrieved documents and gives a higher score for better ranking
Normalized discounted cumulative gain
Normalized discounted cumulative gain (nDCG) evaluates the ranking quality by con-
sidering the position of relevant documents in the result list and assigning higher 
scores to relevant documents 
97
Evaluation metrics
where reli is the graded relevance of the document at position I, and IDCG is the ideal 
DCG, which is the DCG for perfect ranking. 
nDCG is calculated as the ratio between actual DCG and the IDCG:
Figure 5.5 shows an example of nDCG calculation.
A
B
C
D
E
1
2
3
3
1
2
0
3
Document
Rank
Relevance
DCG
2
3
-
1
log (1+1)
2
= 7
2
1
-
1
log
(2+1)
2
= 0.63
2
2
-
1
log (3+1)
2
= 1.50
2
-
1
log (3+1)
2
= 0
0
2
3
-
1
log (5+1)
= 2.71
Ideal rank
A
E
C
B
D
3
3
2
1
0
Relevance
IDCG
= 7
= 4.41
= 1.50
= 0.43
= 0
11.84
13.35
11.84
13.35
nDCG =
= 0.887
Graded relevance and
not just binary
Penalizes relevant documents
appearing lower in rank
Single score between 0 and 1
4
5
Figure 5.5  nDCG addresses degrees of relevance in documents and penalizes incorrect ranking.
nDCG is a complex metric to calculate. It requires documents to have a relevance 
score, which may lead to subjectivity, and the choice of the discount factor affects the 
values significantly, but it accounts for varyi
98
Chapter 5  RAG evaluation: Accuracy, relevance, and faithfulness
Table 5.1  Retrieval metrics
Metric
What it measures
Strengths
Use cases
Considerations
Accuracy
Overall correct-
ness of retrieval
Simple to under-
stand; includes 
true negatives
General perfor-
mance in bal-
anced datasets
Can be mislead-
ing in imbalanced 
datasets; doesn’t 
consider ranking
Precision
Quality of 
retrieved results
Easy to under-
stand and 
calculate
General 
retrieval quality 
assessment
Doesn’t con-
sider ranking or 
completeness of 
retrieval
Precision@k
Quality of top k 
retrieved results
Focuses on most 
relevant results 
for RAG
When only top k 
results are used 
for augmentation
Choose k based 
on your RAG sys-
tem’s usage
Recall
Coverage of rele-
vant documents
Measures com-
pleteness of 
retrieval
Assessing if 
important infor-
mation is missed
Requires know-
ing all relevant 
documents in the 
corpus
F1-score
Balance between 
precision and 
recall
Single metric 
combining quality 
and cove
99
Evaluation metrics
¡ Is the information retrieval relevant to the user query?
¡ Is the generated answer rooted in the retrieved information?
¡ Is the generated answer relevant to the user query?
Let’s now take a look at each of these scores. 
Context relevance
Context relevance evaluates how well the retrieved documents relate to the original 
query. The key aspects are topical alignment, information usefulness, and redundancy. 
There are human evaluation methods, as well as semantic similarity measures to calcu-
late context relevance.
One such measure is employed by the Retrieval-Augmented Generation Assessment 
(RAGAs) framework (further discussed in section 5.3). The retrieved context should 
contain information only relevant to the query or the prompt. For context relevance, a 
metric S is estimated, where S is the number of sentences in the retrieved context rele-
vant for responding to the query or the prompt:
Figure 5.6 is an illustrative example of high and low context rele
100
Chapter 5  RAG evaluation: Accuracy, relevance, and faithfulness
Answer faithfulness
Answer faithfulness is the measure of the extent to which the response is factually 
grounded in the retrieved context. Faithfulness ensures that the facts in the response 
do not contradict the context and can be traced back to the source. It also ensures that 
the LLM is not hallucinating. In the RAGAs framework, faithfulness first identifies the 
number of claims made in the response and calculates the proportion of those claims 
present in the context:
Let’s look at an example in figure 5.7
Response 1 : High faithfulness
[Australia] won on [November 19, 2023]
Number of claims generated = 2
Number of claims in context = 2
Answer faithfulness = 1 or 100%
Response 2 : Low faithfulness
[Australia] won on [October 15, 2023] by
[defeating India]
Number of claims generated = 3
Number of claims in context = 1
Answer faithfulness = 0.33 or 33%
Query : Who won the 2023 ODI Cricket World Cup and when?
Con
101
Evaluation metrics
Answer relevance
Like context relevance measures the relevance of the retrieved context to the query, 
answer relevance is the measure of the extent to which the response is relevant to the 
query. This metric focuses on key aspects such as the system’s ability to comprehend 
the query, the response being pertinent to the query, and the completeness of the 
response. 
In RAGAs, for this metric, a response is generated for the initial query or prompt. To 
compute the score, the LLM is then prompted to generate questions for the generated 
response several times. The mean cosine similarity between these questions and the 
original one is then calculated. The concept is that if the answer addresses the initial 
question correctly, the LLM should generate questions from it that match the original 
question:
where N is the number of queries generated by the LLM. 
Note that answer relevance is not a measure of truthfulness but only of relevance. 
The response may or ma
102
Chapter 5  RAG evaluation: Accuracy, relevance, and faithfulness
Original query : Who
won the 2023 ODI Cricket
World Cup?
Answer : Australia won the 2023
ODI Cricket World Cup, which
concluded on 1November 19, 2023
Synthetic queries
1.Which team emerged victorious in the 2023 ODI Cricket World Cup?
2.Who won the 2023 Cricket World Cup, and when did the final match take place?
3.Which country claimed the title in the 2023 ODI Cricket World Cup?
4.What was the outcome of the 2023 ODI Cricket World Cup, and on what date was the champion decided?
5.Who were the champions of the 2023 Cricket World Cup, and when did the tournament come to an end?
Synthetic query 1 embeddings
Synthetic query 2 embeddings
Synthetic query 3 embeddings
Synthetic query 4 embeddings
Synthetic query 5 embeddings
Original query
embeddings
Cosine similarity
0.83
0.63
0.76
0.53
0.57
Knowledge base
Context
The 2023 Cricket World Cup, concluded on
November 19, 2023, with Australia winning the
tournament. The tournam
103
Frameworks
The relative importance of each metric will depend on your use case and user 
requirements. You may need to include other metrics specific to your downstream use 
case, such as summarization to measure conciseness, and chatbots to emphasize con-
versation coherence.
Developers can code these metrics from scratch and integrate them in the devel-
opment and deployment process of their RAG system. However, you’ll find evaluation 
frameworks that are readily available quite handy. We discuss three popular frameworks 
in the next section.
Human evaluations and ground truth data
Most of the metrics we discussed talk about a concept of relevant documents. For 
example, precision is calculated as the number of relevant documents retrieved, 
divided by the total number of retrieved documents. The question that arises is, how 
does one establish that a document is relevant?
The simple answer is a human evaluation approach. A subject matter expert looks at 
the documents and determ
104
Chapter 5  RAG evaluation: Accuracy, relevance, and faithfulness
¡ RAGAs (Retrieval-Augmented Generation Assessment)
¡ ARES (Automated RAG Evaluation System)
5.3.1	
RAGAs
Retrieval-Augmented Generation Assessment, or RAGAs, is a framework developed by 
Exploding Gradients that assesses the retrieval and generation components of RAG 
systems without relying on extensive human annotations. RAGAs
¡ Synthetically generate a test dataset that can be used to evaluate a RAG pipeline.
¡ Use metrics to measure the performance of the pipeline.
¡ Monitor the quality of the application in production.
We will continue with our example of the Wikipedia page of the 2023 Cricket World 
Cup, but we first create a synthetic test dataset using RAGAs and then use the RAGAs 
metrics to evaluate the performance of the RAG pipeline we created in chapters 3 and 4.
Synthetic test dataset generation (ground truths)
Section 5.2 pointed out that ground truths data is necessary to calculate evaluation 
metrics
105
Frameworks
To evaluate our RAG pipeline, let’s recreate the documents from the Wikipedia page 
like we did in chapter 3. Note that we will have to install the packages used in the previ-
ous chapters to continue with the following code:
#Importing the AsyncHtmlLoader
from langchain_community.document_loaders import AsyncHtmlLoader
#This is the URL of the Wikipedia page on the 2023 Cricket World Cup
url="https://en.wikipedia.org/wiki/2023_Cricket_World_Cup"
#Instantiating the AsyncHtmlLoader
loader = AsyncHtmlLoader (url)
#Loading the extracted information
html_data = loader.load()
from langchain_community.document_transformers import Html2TextTransformer
#Instantiate the Html2TextTransformer function
html2text = Html2TextTransformer()
#Call transform_documents
html_data_transformed = html2text.transform_documents(html_data)
The html_data_transformed contains the necessary document format of the Wikipe-
dia page. We will use RAGAs library to generate the dataset from these documents
106
Chapter 5  RAG evaluation: Accuracy, relevance, and faithfulness
)
# Call the generator
testset = 
generator.generate_with_langchain_docs
(
          	
html_data_transformed, 
test_size=20, 
)
The testset that we created contains 20 questions based on our document, along 
with the chunk of the document that the question was based on, and the ground truth 
answer. A screenshot of the dataset is shown in figure 5.10.
Figure 5.10  Synthetic test data generated using RAGAs
We will use this dataset to evaluate our RAG pipeline.
Recreating the RAG pipeline
From the created test dataset, we use the question and the ground_truth information. 
We pass the questions to our RAG pipeline and generate answers. We compare these 
answers with the ground_truth to calculate the evaluation metrics. First, we recreate 
our RAG pipeline. Again, it is important to note that we will have to install the pack-
ages we used in the previous chapters to continue with the code:
# Import FAISS class from vecto
107
Frameworks
embeddings=OpenAIEmbeddings(model="text-embedding-3-small")
# Load the database stored in the local directory
db=FAISS.load_local(
folder_path=db_path, 
index_name=index_name, 
embeddings=embeddings,
allow_dangerous_deserialization=True
)
# Ranking the chunks in descending order of similarity and selecting the top 
2 queries
retrieved_docs = db.similarity_search(query, k=2)
# Keeping text of top 2 retrieved chunks
retrieved_context=[ retrieved_docs[0].page_content 
+retrieved_docs[1].page_content]
# Creating the prompt
augmented_prompt=f"""
Given the context below, answer the question.
Question: {query} 
Context : {retrieved_context}
Remember to answer only based on the context 
provided and not from any other source. 
If the question cannot be answered based 
on the provided context, say I don't know.
"""
# Instantiate the LLM
llm = ChatOpenAI(
model="gpt-4o-mini",
temperature=0,
max_tokens=None,
timeout=None,
max_retries=2
)
# Create message to send to the LLM
messages
108
Chapter 5  RAG evaluation: Accuracy, relevance, and faithfulness
response = llm.invoke(messages)
# Extract the answer from the response object
   
answer=response.content
return retrieved_context, answer
We can try this pipeline to generate answers.
# Location of the stored vector index created by the indexing pipeline
db_path='../../Assets/Data'
# User Question
query="Who won the 2023 cricket world cup?"
# Index Name
index_name="CWC_index"
# Calling the RAG function
rag_function(query, db_path, index_name)
Now that we have the RAG pipeline function, we can evaluate this pipeline using the 
questions that have been synthetically generated.
Evaluations
We first generate answers to the questions in the synthetic test data using our RAG 
pipeline. We then compare the answers to the ground truth answers. We first generate 
the answers: 
# Create Lists for Questions and Ground Truths from testset
sample_queries = 
dataset.to_pandas()['user_input'].to_list()
 
expected_responses=
dataset
109
Frameworks
                    "reference":reference
            }
                ) 
For RAGAs, the evaluation set needs to be in the Dataset format:
# Import the EvaluationDataset library
from ragas import EvaluationDataset
evaluation_dataset = EvaluationDataset.from_list(dataset_to_eval)
Now that we have the complete evaluation dataset, we can invoke the metrics:
#Import all the libraries
from ragas import evaluate
from ragas.metrics import (
        LLMContextRecall, 
Faithfulness, 
FactualCorrectness, 
AnswerCorrectness, 
ResponseRelevancy)
#Set the judge LLM for evaluation
evaluator_llm = 
LangchainLLMWrapper(
ChatOpenAI(model="gpt-4o-mini")
)
# Calculate the metrics for the dataset 
result = evaluate(
dataset=evaluation_dataset,
metrics=[
LLMContextRecall(), 
Faithfulness(), 
AnswerCorrectness(), 
ResponseRelevancy(),
FactualCorrectness()],
llm=evaluator_llm)
You can also check the official documentation of RAGAs for more information 
(https://docs.ragas.io/en/stable/). RAGA
110
Chapter 5  RAG evaluation: Accuracy, relevance, and faithfulness
statements judged by the LLM as faithful (i.e., 1). Several other metrics are calculated 
using this LLM-based approach. This approach, where an LLM is used in evaluating 
a task, is also popularly called LLM as a judge approach. An important point to note 
here is that the accuracy of this evaluation is also dependent on the quality of the LLM 
being used as the judge.
5.3.2	
Automated RAG evaluation system
Automated RAG evaluation system, or ARES, is a framework developed by research-
ers at Stanford University and Databricks. Like RAGAs, ARES uses an LLM as a judge 
approach for evaluations. Both request a language model to classify answer rele-
vance, context relevance, and faithfulness for a given query. However, there are some 
differences:
¡ RAGAs relies on heuristically written prompts sent to the LLM for evaluation. 
ARES, in contrast, trains a classifier using a language model.
¡ RAGAs aggregates the respons
111
Benchmarks
While RAGAs and ARES have gained popularity, there are other frameworks, such as 
TruLens, DeepEval, and RAGChecker, that have also gotten acceptance amongst RAG 
developers. 
Frameworks provide a standardized method of automating the evaluation of your 
RAG pipelines. Your choice of the evaluation framework should depend on your use 
case requirements. For quick and easy evaluations that are widely understood, RAGAs 
may be your choice. For robustness across diverse domains and question types, ARES 
might suit better. Most of the proprietary service providers (vector DBs, LLMs, etc.) 
have their evaluation features you may use. You can also develop your metrics.
Next, we look at benchmarks. Benchmarks are used to compare competing RAG 
systems with one another. 
5.4	
Benchmarks
Benchmarks provide a standard point of reference to evaluate the quality and perfor-
mance of a system. RAG benchmarks are a set of standardized tasks, and a dataset used 
to compare the efficien
112
Chapter 5  RAG evaluation: Accuracy, relevance, and faithfulness
Noise robustness
Negative rejection
RAG
RAG
Information integration
Counterfactual robustness
RAG
RAG
Figure 5.11  Four abilities required of RAG systems. Source: Benchmarking Large Language Models in Retrieval-
Augmented Generation by Chen et al., https://arxiv.org/pdf/2309.0143.
evaluated using ChatGPT to determine whether the responses contain rejection 
information.
¡ Error detection rate—Used for counterfactual robustness. It is measured by exact 
matching of the model’s output with a specific error-detection phrase and is also 
evaluated using ChatGPT.
113
Benchmarks
¡ Error correction rate—Used for counterfactual robustness. It measures whether the 
model can provide the correct answer after identifying errors.
You can use the GitHub repository to implement RGB (https://github.com/
chen700564/RGB).
Multi-hop RAG
Curated by researchers at HKUST, multi-hop RAG contains 2556 queries, with evidence 
for each query distributed across two to four documents. The queries also involve doc-
ument metadata, reflecting complex scenarios commonly found in real-world RAG 
applications. It contains four types of queries:
¡ Inference—Synthesizing information across multiple sources (e.g., Which report 
discusses the supply chain risk of Apple—the 2019 annual report or the 2020 
annual report?)
¡ Comparison—Comparing facts from different sources (e.g., Did Netflix or Goo-
gle report higher revenue for the year 2023?)
¡ Temporal—Analyzing the temporal ordering of events (e.g., e.g. Did Apple intro-
duce the AirTag tracking device before or after the 
114
Chapter 5  RAG evaluation: Accuracy, relevance, and faithfulness
Question type
Definition
Simple
Simple w. condition
Set
Comparison
Aggregation
Multi-hop
Post-processing
heavy
False premise
Questions asking for simple facts that are unlikely to change overtime, such as the
birth date of a person or the authors of a book
Questions asking for simple facts with some given conditions, such as stock prices
on a certain date or a director’s recent movies in a certain genre
Questions that expect a set of entities or objects as the answer (e.g., “What are the
continents in the southern hemisphere?”)
Questions that compare two entities (e.g., “Who started performing earlier, Adele or
Ed Sheeran?”)
Questions that require aggregation of retrieval results to answer (e.g., “How many
Oscar awards did Meryl Streep win?”)
Questions that require chaining multiple pieces of information to compose the answer
(e.g., “Who acted in Ang Lee‘s latest movie?”)
Questions that need reasoning or processing of
115
Limitations and best practices
Benchmark
Dataset
Task
Metrics
Applicability
HotpotQA
Wikipedia-based 
QA
Multi-hop QA
EM, F1-score
QA involving mul-
tiple documents, 
complex reasoning 
tasks
BEIR
Multiple datasets
Information 
retrieval
nDCG@10
Comprehensive IR 
model evaluation 
across multiple 
domains
RGB
News articles, 
ChatGPT-
generated QA
Robust QA
Accuracy, rejec-
tion rate, error 
detection rate, 
error correction 
rate
Robustness and 
reliability of RAG 
systems
Multi-hop RAG
HKUST-curated 
queries
Complex QA
Various
RAG applications 
requiring multi-
source synthesis
CRAG
Multiple sources 
(finance, sports, 
music, etc.)
Factual QA
Four-class eval-
uation (perfect, 
acceptable, miss-
ing, and incorrect)
Evaluating factual 
QA with diverse 
question types
We have looked frameworks that help in automating the calculation of evaluation met-
rics and benchmarks that enable comparisons across different implementations and 
approaches. Frameworks will assist you in improving 
116
Chapter 5  RAG evaluation: Accuracy, relevance, and faithfulness
faithfulness for evaluation brings in the necessary nuances required for RAG evalua-
tion. However, even for these metrics, there’s no standard way of calculation and each 
framework brings in its methodology.
Best practice: Compare the results on RAG specific metrics from different frameworks. 
Sometimes, it may be warranted to change the calculation method with respect to the 
use case. 
Overreliance on LLM as a judge
The evaluation of RAG-specific metrics (in RAGAs, ARES, etc.) relies on using an LLM 
as a judge. An LLM is prompted or fine-tuned to classify a response as relevant or not. 
This adds to the complexity of the LLMs’ ability to do this task. It may be possible that 
the LLM may not be very accurate in judging for your specific documents and knowl-
edge bases. Another problem that arises is that of self-reference. It is possible that if 
the judge LLM is the same as the generation LLM in your system, you
117
Summary
Apart from these, you should also carefully consider the aspects of bias and toxicity, 
focusing on information integration and negative rejection, which the frameworks do 
not evaluate well. It is also important to keep an eye on how these evaluation frame-
works and benchmarks evolve.
In this chapter, we comprehensively examined the evaluation metrics, frameworks, 
and benchmarks that will help you evaluate your RAG pipelines. We used RAGAs to 
evaluate the pipeline that we have been building. 
Until now, we have looked at building and evaluating a simple RAG system. This 
also marks the second part 2 of this book. You are now familiar with the creation of 
the RAG knowledge brain using the indexing pipeline, enabling real-time interaction 
using the generation pipeline and evaluating your RAG system using frameworks and 
benchmarks.
In the next part, we will move toward discussing the production aspects of RAG sys-
tems. In chapter 6, we will look at strategies and advan
118
Chapter 5  RAG evaluation: Accuracy, relevance, and faithfulness
Evaluation frameworks
¡ Frameworks such as RAGAs and ARES automate the evaluation process and assist 
in synthetic data generation.
¡ RAGAs is an easy-to-implement framework that can be used for quick evaluation 
of RAG pipelines.
¡ ARES uses a more complex approach, including classifier training and confi-
dence interval calculations.
Benchmarks
¡ Benchmarks provide standardized datasets and metrics for comparing different 
RAG implementations on specific tasks.
¡ Popular benchmarks such as SQuAD, natural questions, HotpotQA, and BEIR 
focus on retrieval quality.
¡ Recent benchmarks such as RGB, multi-hop RAG, and CRAG are more holistic 
from a RAG perspective.
¡ Benchmarks focus on different aspects of RAG performance, such as multi-hop 
reasoning or specific domains.
Limitations and best practices
¡ Challenges in RAG evaluation include lack of standardized metrics, overreliance 
on LLMs as judges, and static nature
Part 3
RAG in production
You must be confident by now in building and evaluating a core RAG pipeline. 
Applications such as “chat with your PDF” or question-answering systems based 
on web pages should no longer be a mystery. This part of the book will guide 
you in improving your RAG pipeline and also lay out a blueprint for the layers 
required to build a production-ready RAG system.
In chapter 6, you’ll be able to try out different techniques for improving the 
basic RAG pipeline into a more advanced one. You’ll get to know the techniques 
that improve RAG in three different stages—before, during, and after retrieval. 
You’ll also learn about modularity and how modern RAG systems are made up of 
replaceable components. 
Chapter 7 discusses the operations stack for RAG. You will learn about the crit-
ical layers without which any RAG system will fail, the essential layers that improve 
system performance, and the enhancement layers that focus on usability, scalabil-
ity, and efficien

121
6
Progression of RAG 
systems: Naïve, advanced, 
and modular RAG
This chapter covers
¡ Limitations of the naïve RAG approach
¡ Advanced RAG strategies and techniques
¡ Modular patterns in RAG
In the first two parts of this book, you learned about the utility of retrieval-
augmented generation (RAG), along with the development and evaluation of a 
basic RAG system. The basic, or the naïve RAG approach that we have discussed is, 
generally, inadequate when it comes to production-grade systems. 
This chapter focuses on more advanced concepts in RAG. We begin by revisiting 
the limitations and the points of failure of the naïve RAG approach. Next, we discuss 
the failures at the retrieval, augmentation, and generation stages. Advanced strate-
gies and techniques to address these points of failure will be elaborated on in distinct 
phases of the RAG pipeline.
Better indexing of the knowledge base leads to better RAG outcomes. We will look 
at a few data indexing strategies that build on
122
Chapter 6  Progression of RAG systems: Naïve, advanced, and modular RAG
the user query to the documents in the knowledge base. Finally, in the post-retrieval 
stage, the focus is on aligning the retrieved context with the desired result and making 
it suitable for generation. 
The last part of the chapter discusses a modular approach to RAG that has been 
emerging to find applicability in RAG systems. The modular approach is an architec-
tural enhancement to the basic RAG system.
Note that the strategies and techniques for RAG improvement are expansive, and 
this chapter highlights a few popular ones. The chapter is interspersed with code exam-
ples, but for a more exhaustive supporting code, check out the source code repository 
of this book.
By the end of this chapter, you should
¡ Understand why the naïve approach to RAG is not suitable for production.
¡ Be aware of indexing strategies that make the RAG knowledge base more efficient.
¡ Know some of the popular pre-retrieval, ret
123
Advanced RAG techniques
The naïve RAG approach is marred with drawbacks at each of the three stages: 
¡ Retrieval—Naïve retrieval is often observed to have low precision that leads to 
irrelevant information being retrieved. It also has a low recall, which means that 
relevant information is missed, which leads to incomplete results. 
¡ Augmentation—There is a real possibility of redundancy and repetition when 
multiple retrieved documents have similar information. Also, when information 
is sourced from different documents, the context becomes disjointed. There’s 
also the problem of context length of the LLMs that has an effect on the volume 
of retrieved context that can be passed on to the LLM for generation. 
¡ Generation—With the inadequacies of the upstream processes, the generation suf-
fers from hallucination and lack of groundedness of the generated content. The 
LLM faces challenge in reconciling information. The challenges of toxicity and 
bias also persist. It is also 
124
Chapter 6  Progression of RAG systems: Naïve, advanced, and modular RAG
–	 Query optimization—Optimizing the user query so it aligns better with the 
retrieval and generation tasks
¡ Retrieval stage—Certain strategies can improve the recall and precision of the 
retrieval process. This goes beyond the capability of the underlying retrieval algo-
rithms discussed in chapter 4.
¡ Post-retrieval stage—Once the information has been retrieved, the context can be fur-
ther optimized to better align with the generation task and the downstream LLM.
With techniques employed at these three stages, the advanced RAG process follows 
a “rewrite then retrieve then re-rank then read” frameworks. Two additional compo-
nents of rewrite and re-rank are added, and the retrieve component is enhanced in 
comparison with naïve RAG. This structure is presented in figure 6.3.
Indexing
Retriever
Prompt
LLM
Documents
User query
Pre-
retrieval
Retrieval
strategies
Response
Rewrite
Retrieve
Re-rank
Read
Index
125
Pre-retrieval techniques
6.3	
Pre-retrieval techniques
The primary objective of employing pre-retrieval techniques is to facilitate better 
retrieval. We have noted that the retrieval stage of naïve RAG suffers from low recall 
and low precision—irrelevant information is retrieved, and not all relevant informa-
tion is retrieved. This can happen mainly because of two reasons: 
¡ Knowledge base is not suited for retrieval. If the information in the knowledge base is 
not stored in a manner that is easy to search through, then the quality of retrieval 
will remain suboptimal. To address this problem, index optimization is done in the 
indexing pipeline for more efficient storage of the knowledge base.
¡ Retriever doesn’t completely understand the input query. In generative AI applications, 
the control over the user query is generally limited. The level of detail a user 
provides is subjective. The retriever sometimes may misunderstand or not com-
pletely understand the context of th
126
Chapter 6  Progression of RAG systems: Naïve, advanced, and modular RAG
query, and the desired response time. There is no one-size-fits-all approach to 
optimizing chunk sizes. Experimentation and evaluation of different chunk sizes 
on metrics such as faithfulness, relevance, and response time (as discussed in 
chapter 5) can help in identifying the optimal chunk size for the RAG system. 
Chunk size optimization may require periodic reassessment as data or require-
ments change.
¡ Context-enriched chunking—This method adds the summary of the larger docu-
ment to each chunk to enrich the context of the smaller chunk. This makes more 
context available to the LLM without adding too much noise. It also improves the 
retrieval accuracy and maintains semantic coherence across chunks. This feature 
is particularly useful in scenarios where a more holistic view of the information 
is crucial. While this approach enhances the understanding of the broader con-
text, it adds a level of comp
127
Pre-retrieval techniques
chunks=text_splitter.split_text(	
	
	
	
	
data_transformed[0].page_content	
)  
context_enriched_chunks = 	
	
	
	
	
[answer + "\n" + chunk for chunk in chunks]  
embedding = OpenAIEmbeddings(openai_api_key=api_key)  
vector_store = FAISS.from_texts(	
	
	
context_enriched_chunks, 
embedding	
)  
¡ Fetch surrounding chunks—In this technique, chunks are created at a granu-
lar level, say, at a sentence level, and when a relevant chunk of text is found in 
response to a query, the system retrieves not only that chunk but also the sur-
rounding chunks. This makes the search granular but also performs contextual 
expansion by retrieving adjacent chunks. It is useful in long-form content such 
as books and reports where information flows across paragraphs and sections. 
This technique also adds a layer of processing cost and latency to the system. 
Apart from that, there is a possibility of diluting the relevance as the neighboring 
chunks may contain noise. 
Chun
128
Chapter 6  Progression of RAG systems: Naïve, advanced, and modular RAG
technique is reverse hypothetical document embeddings. It involves using a lan-
guage model to generate potential queries that could be answered by each doc-
ument or chunk. These synthetic queries are then added to metadata. During 
retrieval, the system compares the user’s query with these synthetic queries to 
find the most relevant chunks. 
Metadata is a great tool for improving the accuracy of the retrieval system. However, a 
degree of caution must be exercised when adding metadata to the chunks. Designing 
the metadata schema is important to avoid redundancies and managing processing 
and storage costs. Providing improved relevance and accuracy, metadata enhancement 
has become extremely popular in contemporary RAG systems. 
Index structures
Another important aspect of the knowledge base is how well the information is struc-
tured. In the naïve RAG approach, there is no structural order to documents/chun
129
Pre-retrieval techniques
spoken language. When dealing with domain-specific or specialized content, 
these models may not yield good results. Fine-tuning embedding models let 
you optimize vector representations for your specific domain or task, leading 
to more accurate retrieval of relevant context. Fine-tuning is a slightly com-
plex process since it requires curation of the training dataset and resources 
for recalculating the embeddings model. In case you’re dealing with highly 
specialized domains where the vocabulary is different from commonly spoken 
languages, you should consider fine-tuning the embedding model for your 
domain.
Like the indexing pipeline, index optimization is a periodic process and does not hap-
pen in real-time. The objective of index optimization is to set up the knowledge base 
for better retrieval. One must also be mindful of the added complexity that leads to an 
increase in computational, memory, and storage requirements. Figure 6.4 is an illustra-
130
Chapter 6  Progression of RAG systems: Naïve, advanced, and modular RAG
6.3.2	
Query optimization
The second stage of pre-retrieval techniques is a part of the generation pipeline. The 
objective of this stage is to optimize the input user query in a manner that makes it bet-
ter suited for the retrieval tasks. Some of the popular query optimization strategies are 
listed in the following sections. 
Query expansion
In query expansion, the original user query is enriched to retrieve more relevant infor-
mation. This helps in increasing the recall of the system and overcomes the challenge 
of incomplete or very brief user queries. Some of the techniques that expand user 
queries are
¡ Multi-query expansion—In this approach, multiple variations of the original query 
are generated using an LLM, and each variant query is used to search and retrieve 
chunks from the knowledge base. For a query “How does climate change affect 
polar bears?” a multi-query expansion might generate “Impact 
131
Pre-retrieval techniques
bears?” The approach to sub-query is similar to that for multi-query, except for 
the changes to the prompt:
sub_query_expansion_prompt=f" \
Break down the following \
query into {num} sub-queries targeting \
different aspects of the query: {original_query}. \
Respond in JSON format. "
¡ Step-back expansion—The term comes from the step-back prompting approach 
where the original query is abstracted to a higher-level conceptual query. During 
retrieval, both the original query and the abstracted query are used to fetch 
chunks. Similar to above example, an abstracted step-back query may be “What 
are the ecological impacts of climate change on arctic ecosystems?” Here is an 
example of the prompt that can be used:
step_back_expansion_prompt = f"	\
Given the query: {original_query}, \
generate a more abstract, \
higher-level conceptual query. "
While multi-query expansion generates various rephrasing or synonyms of the original 
query to cast a wider net duri
132
Chapter 6  Progression of RAG systems: Naïve, advanced, and modular RAG
¡ HyDE—Hypothetical document embedding, or HyDE, is a technique where the 
language model first generates a hypothetical answer to the user’s query without 
accessing the knowledge base. This generated answer is then used to perform a 
similarity search against the document embeddings in the knowledge base, effec-
tively retrieving documents that are similar to the hypothetical answer rather 
than the query itself. Here is an example that generates hypothetical document 
embeddings: 
# Original Query
original_query=	
"How does climate change \ 
affect polar bears?"  
# Prompts for generating HyDE
system_prompt="You are an expert in \	
climate change and arctic life."  
hyde_prompt=f"Generate an answer to the \	
question: {original_query}"  
# Using OpenAI to generate a hypothetical answer
from openai import OpenAI  
client = OpenAI()  
response = client.chat.completions.create(  
  model="gpt-4o-mini",  
  mess
133
Retrieval strategies
¡ Intent classification—A pre-trained classification model is used to classify the 
intent of the user query to select the appropriate retrieval method. A modifi-
cation to this technique is prompt-based classification, where instead of a pre-
trained classifier, an LLM is prompted to categorize the query into an intent.
¡ Metadata routing—In this approach, keywords and tags are extracted from the 
user query and then filtering is done on the chunk metadata to narrow down the 
scope of the search.
¡ Semantic routing—In this approach, the user query is matched with a pre-defined 
set of queries for each retrieval method. Wherever the similarity between the user 
query and pre-defined queries is the highest, that retrieval method is invoked.
In customer support chatbots, query routing ensures that technical queries are 
directed to databases with troubleshooting guides, while billing questions are routed 
to account information, enhancing user satisfaction.
Imple
134
Chapter 6  Progression of RAG systems: Naïve, advanced, and modular RAG
User
Hybrid retriever
Pre-retrieval
Graph search
Sparse embedding
retrieval
Dense embedding
retrieval
Knowledge
graph
Vector storage
Combiner
Post-retrieval
Hybrid retrieval
Figure 6.5  Hybrid retriever employs multiple querying techniques and combines the results.
6.4.2	
Iterative retrieval
Instead of using a retrieve–generate linear process, the iterative retrieval strategy 
searches the knowledge base repeatedly based on the original query and the generated 
text, which allows the system to gather more information by refining the search based 
on initial results. It is useful when solving multi-hop or complex queries. While effec-
tive, iterative retrieval can lead to longer processing times and may introduce chal-
lenges in managing larger amounts of retrieved information. There are examples of 
iterative retrieval that have demonstrated remarkably improved performance such as 
Iter-RetGen, which is an iter
135
Retrieval strategies
predicts future content needs based on the current generation and retrieves rele-
vant information proactively. Adaptive retrieval is a part of a broader trend of agen-
tic AI. Agentic AI refers to AI systems that can make autonomous decisions during 
tasks, adapting their actions based on the context. In the context of RAG, agentic RAG 
involves AI agents that dynamically decide when and how to retrieve information, 
thus enhancing the flexibility and efficiency of the retrieval process. Agentic AI is an 
important emerging RAG pattern. We will discuss Agentic RAG in detail in chapter 8.
Figure 6.6 compares the three retrieval strategies that focus on repeated retrieval 
cycles. While recursive and iterative approaches need a threshold to break out of the 
iterations, in the adaptive approach, a judge model decides on-demand retrieval and 
generation steps.
Query
Retriever
Generator LLM
Judge model
Response
Query
Retriever
Generator LLM
Judge model
Response
Qu
136
Chapter 6  Progression of RAG systems: Naïve, advanced, and modular RAG
All the advanced retrieval strategies introduce overheads in terms of computational 
complexity, and therefore the accuracy must be balanced against the cost and latency 
of the system. 
By employing advanced pre-retrieval techniques and a suitable retrieval strategy, we 
can expect that richer, deeper, and more relevant context is being retrieved from the 
knowledge base. Even when the relevant context is retrieved, the LLM may struggle to 
assimilate all the information. To address this problem, in the next section, we discuss 
a couple of post-retrieval strategies that help curate the context before augmenting the 
prompt with the necessary information.
6.5	
Post-retrieval techniques
Even if the retrieval of the chunks happens in an expected manner, a point of failure 
still remains. The LLM might not be able to process all the information. This may be 
due to redundancies or disjointed nature of the context
137
Post-retrieval techniques
improves the overall response quality. There are commonly available re-rankers such 
as multi-vector, Learning to Rank (LTR), BERT-based, and even hybrid re-rankers that 
can be employed. Specialized APIs such as Cohere Rerank offer pre-trained models for 
efficient reranking integration. 
In this section, we discuss some of the popular advanced RAG strategies and tech-
niques employed at different stages of the RAG pipeline. It is important to also con-
sider the tradeoffs that come with these techniques. Almost any advanced technique 
will introduce overheads to the system. These can be in the form of computational load, 
latency in the system, and increased storage and memory requirements. Therefore, 
these techniques warrant a performance versus overhead assessment catered to specific 
use cases. Table 6.1 provides a summary of the 12 strategies discussed so far.
Table 6.1  Advanced RAG strategies with their benefits and limitations
Strategy
Descriptio
138
Chapter 6  Progression of RAG systems: Naïve, advanced, and modular RAG
Strategy
Description
Benefits
Challenges
Compression
Reducing context 
length by removing 
irrelevant information
Fits within LLM context 
window; reduces noise 
and costs
Potential loss of import-
ant information; needs 
balance
Reranking
Reordering retrieved 
documents to prioritize 
relevance
Enhances response 
quality; ensures most 
relevant info is used
Requires additional 
models; may introduce 
overhead
Figure 6.7 is an illustrative example of what a generation pipeline looks like after incor-
porating advanced techniques.
Re-write Multi-query
Re-rank Compress
Query
router
User
query
Knowledge
base
Retriever
Augmentation
LLM
Response
Retrieval
Pre-retrieval
Post-retrieval
Query routing enhances
retrieval and generation
by matching method to
query type.
At the pre-retrieval
stage, interventions
optimize the input user
query in a manner that
makes it more suitable
for retrieval tasks.
Post-retrieval techni
139
Modular RAG
queries evolve. A modular RAG approach discussed in the next section aims to provide 
greater architectural flexibility over the traditional RAG system.
6.6	
Modular RAG
AI systems are becoming increasingly complex, demanding more customizable, flexi-
ble, and scalable RAG architectures. The emergence of modular RAG is a leap forward 
in the evolution of RAG systems. Modular RAG breaks down the traditional monolithic 
RAG structure into interchangeable components. This allows for tailoring of the sys-
tem to specific use cases. The modular approach brings modularity to RAG compo-
nents, such as retrievers, indexing, and generation, while also adding more modules 
such as search, memory, and fusion. We can think of the modular RAG approach in 
two parts:
¡ Core components of RAG developed as flexible, interchangeable modules
¡ Specialized modules to enhance the core features of retrieval, augmentation, 
and generation
6.6.1	
Core modules
The core components of the RAG sy
140
Chapter 6  Progression of RAG systems: Naïve, advanced, and modular RAG
¡ Pre-retrieval module—Allows flexibility of pre-retrieval techniques to improve the 
quality of indexed content and user query.
¡ Post-retrieval module—Like the pre-retrieval module, this module allows for flex-
ible implementation of post-retrieval techniques to refine and optimize the 
retrieved context.
You may note that the first three modules complete the naïve RAG approach, and the 
addition of the pre-retrieval and post-retrieval modules enhances the naïve RAG into 
an advanced RAG implementation. It can also be said that naïve RAG is a special (and 
limited) case of advanced RAG.
6.6.2	
New modules
The modular RAG framework has introduced several new components to enhance the 
retrieval and generation capabilities of naïve and advanced RAG approaches. Some of 
these components/modules are
¡ Search—The search module is aimed at performing searches on different data 
sources. It is customized to differen
141
Modular RAG
summarization, translation, or sentiment analysis. By incorporating a small num-
ber of task-specific examples or prompts, the module adjusts the retrieval and 
generation components to produce outputs tailored to the desired task, enhanc-
ing versatility without extensive retraining.
You may observe that advanced RAG is a special case within the modular RAG frame-
work. You also saw earlier that naïve RAG is a special case of advanced RAG. This means 
that the RAG approaches (i.e., naïve, advanced, and modular) are not competing but 
progressive. You may start by trying out a naïve implementation of RAG and move to a 
more modular approach. Figure 6.8 shows the progression of RAG systems.
Retrieval
Generation
Pre-retrieval
Post-retrieval
Naive RAG is composed of three core modules.
Indexing
Advanced RAG adds pre-retrieval and post-retrieval
modules to the naive RAG modules.
Routing
Fusion
Task adapter
Memory
Search
Predict
Enables direct
search across
different sources
142
Chapter 6  Progression of RAG systems: Naïve, advanced, and modular RAG
While building a modular RAG system, remember that each module should be 
designed to work independently. This requires defining clear inputs and outputs. 
Along with the independent modules, the orchestration layer should be flexible to 
allow mixing and matching of modules. One should also bear in mind that a modular 
approach introduces complexity in the process. Managing interfaces, dependencies, 
configurations, and versions of modules can be complex. Ensuring compatibility and 
consistency between modules can be challenging. Testing each module independently 
and collectively requires a robust evaluation strategy. Extra modules may also add 
latency and inference costs to the system.
Despite the added complexities, the modular approach toward RAG is state-of-the-
art in large-scale RAG systems. It enables rapid experimentation, efficient optimiza-
tion, and seamless integration of new technologies as they
143
Summary
Advanced RAG techniques
¡ The advanced RAG process follows a “rewrite then retrieve then re-rank then 
read” framework, where the query is optimized through rewriting, retrieval is 
enhanced for better precision, results are re-ranked to prioritize relevance, and 
the most relevant information is used for generating the final response.
¡ Pre-retrieval techniques include
–	 Index optimization—Improves document storage for better searchability
–	 Chunk optimization—Balances chunk sizes to avoid losing context or introduc-
ing noise
–	 Context-enriched chunking—Adds summaries to each chunk to improve retrieval
–	 Metadata enhancements—Adds tags and metadata like timestamps or categories 
for better filtering
–	 Query optimization—Expands or rewrites user queries for improved retrieval 
accuracy
¡ Retrieval techniques include
–	 Hybrid retrieval—Combines keyword-based and semantic searches
–	 Iterative retrieval—Refines searches by repeatedly querying based on initial 
results

144
Chapter 6  Progression of RAG systems: Naïve, advanced, and modular RAG
–	 Routing module—Dynamically selects the best path for handling different types 
of queries
–	 Task adapter module—Adapts the system for different downstream tasks like 
summarization or translation
Tradeoffs and best practices
¡ Advanced techniques improve RAG accuracy but add complexity.
¡ Techniques such as hybrid retrieval or re-ranking can increase computational 
costs and latency.
¡ Modular RAG offers flexibility but requires careful management of interfaces 
and module compatibility.
¡ Testing each module independently and as a whole is important to ensure system 
stability and performance.
¡ Tradeoffs between performance, cost, and system complexity should be carefully 
assessed.
145
7
Evolving RAGOps stack
This chapter covers
¡ The design of RAG systems
¡ Available tools and technologies that enable  
	 a RAG system
¡ Production best practices for RAG systems
So far, we have discussed the indexing pipeline, generation pipeline, and evaluation 
of a retrieval-augmented generation (RAG) system. Chapter 6 also covered some 
advanced strategies and techniques that are useful when building production-grade 
RAG systems. These strategies help improve the accuracy of retrieval and genera-
tion and, in some cases, reduce the system latency. With all this information, you 
should be able to stitch together a RAG system for your use cases. Chapter 2 briefly 
laid out the design of a RAG system. This chapter elaborates on that design.
A RAG system is composed of standard application layers, as well as layers spe-
cific to generative AI applications. Stacked together, these layers create a robust RAG 
system.
146
Chapter 7  Evolving RAGOps stack
These layers are supported by a technology infrastructure. We delve into these layers 
and the available technologies and tools offered by popular service providers that can 
be used in crafting a RAG system. Some providers have started offering managed end-
to-end RAG solutions, which we touch upon in this chapter. 
We wrap up the chapter with some learnings and best practices for putting RAG sys-
tems in production. Chapter 7 also marks the end of part 3 of the book. 
By the end of this chapter, you should 
¡ Understand the details of the layers in a RAG (RAGOps) stack. 
¡ Be familiar with a host of service providers and the tools and technologies they 
offer for RAG systems.
¡ Know some of the pitfalls and best practices of putting RAG systems in 
production.
A RAG system includes a lot of additional components compared to traditional soft-
ware applications. Vector stores and embeddings models are essential components of 
the indexing pipeline. 
147
The evolving RAGOps stack
7.1.1	
Critical layers
The indexing pipeline and the generation pipeline (discussed in detail in chapters 
3 and 4) form the core of a RAG system. Figure 7.1 illustrates the indexing pipeline 
that facilitates the creation of the knowledge base for RAG systems and the generation 
pipeline that uses the knowledge base to generate context-aware responses.  
User asks a
question.
The system searches
for relevant
information.
The information relevant
to the input question is
fetched, or retrieved.
The prompt with the user
question is augmented
with the retrieved
information.
The LLM responds
with a contextual
answer.
Connects to
external sources.
Extracts documents
and parses text from
documents.
Breaks down long 
pieces of text into
smaller manageable
pieces.
Converts these small
pieces into a suitable
format.
Generation pipeline:
Uses the knowledge base
to generate context-aware
responses
LLM
Response
Search
Retriever
User
Question
{Question + Information}
P
148
Chapter 7  Evolving RAGOps stack
usable format, and storing it for efficient retrieval. Here are some components of the 
data layer: 
¡ Data ingestion component—It collects data from source systems such as databases, 
content management systems, file systems, APIs, devices, and even the inter-
net. The data can be ingested in batches or as a stream, depending on the use 
case. For ingesting data, your choice of tool can depend on factors such as data 
volume, types of data source, ingestion frequency, cost, and ease of setup. Data 
ingestion is not specific to RAG but is a mainstream component in modern soft-
ware applications. AWS Glue, Azure Data Factory, Google Cloud Dataflow, Five-
tran, Apache NiFi, Apache Kafka, and Airbyte are among tools available for use. 
For rapid prototyping and proof of concepts (PoCs), frameworks such as Lang­
Chain and LlamaIndex have inbuilt functions that can assist in connecting to 
some sources and extracting information.
¡ Data transformation co
149
The evolving RAGOps stack
The flow from source systems to data storage via the ingestion and transformation 
components that lead to the creation of the knowledge base is shown in figure 7.2.
Vector Stores
Internet
CMS
Object
Store
Data
Lake
File System
NoSQL
Devices
EFSS
Structured
Third-party
API
Curated
data
Graph Storage
Extraction
and parsing
Source Systems
Data Ingestion
Cleaning
Tagging
Processing
Chunking
ER Mapping
Vectorization/
Embeddings
Data Transformation
Data Storage
Connect to source systems to extract data.
Transform extracted data into the desired format.
Load transformed data in databases.
Figure 7.2  Data layer: Creating the knowledge base by extracting, transforming, and loading (ETL) data from 
source systems
A strong data layer is the foundation of an efficient RAG system. The data layer also 
comes in handy when there is a need for fine-tuning of models. We discuss this feature 
briefly later in the chapter. Next, we look at the model layer, which includes t
150
Chapter 7  Evolving RAGOps stack
using embeddings models. Apart from this, there are other models used in RAG 
systems:
–	 Embeddings models are used to transform data into vector format. We have 
discussed embeddings models in detail in chapter 3. Recall that the choice 
of embeddings model depends on the domain, use case, and cost consider-
ations. Providers such as OpenAI, Gemini by Google, Voyage AI, and Cohere 
provide a variety of embeddings model choices, and a host of open source 
embeddings models can also be used via Hugging Face transformers. Mul-
timodal embeddings map data of different modalities into a shared embed-
dings space.
–	 Foundation models or the pre-trained LLMs are used for the generation of 
outputs, as well as for evaluation and adaptive tasks where LLMs are used to 
judge. We have discussed LLMs as part of the generation pipeline in chapter 
4. Recall that the GPT series by OpenAI, Gemini Series by Google, Claude 
Series by Anthropic, and Command R seri
151
The evolving RAGOps stack
Figure 7.3 illustrates different components of the model layer. It shows how the model 
layer helps in deciding which models to use in the RAG system, facilitates training and 
fine-tuning of the model, and optimizes the models for efficient serving. 
All open 
source,
fine-tuned
and custom
models are
optimized for
inferencing.
Proprietary
models are
generally
optimized by
the service
provider.
Query
classification
Moderation
Hallucination
detection
Entity
recognitions
Custom models
and fine-tuned
models are
trained and
stored in model
library.
LLMs
Embeddings
Task-specific models
Proprietary
LLMs
Fine-tuned
LLMs
Small language
models
Model library
Model training/ Fine-tuning
Fine-tuning pre-trained models
Training new models
Model lifecycle management
Training examples from the data layer
KV caching
Batching
Quantization
Other optimizations
Inference optimization
Training data is
sourced from
the data layer.
Open source
LLMs
Proprietary
embeddings
Fine-tu
152
Chapter 7  Evolving RAGOps stack
¡ Fully managed deployment—It can be provided by proprietary model providers 
such as OpenAI, Google, Anthropic, and Cohere, where all infrastructure for 
model deployment, serving, and scaling is managed and optimized by these 
providers. Services such as AWS SageMaker, Google Vertex AI, Azure Machine 
Learning, and Hugging Face offer platforms to deploy, serve, and monitor both 
open source and custom-developed models. Amazon Bedrock is another fully 
managed service that provides access to a variety of foundation models, both pro-
prietary and open source, simplifying model access and deployment.
¡ Self-hosted deployment—This type of deployment is enabled by cloud VM providers 
such as AWS, GCP, Azure, and hardware providers such as Nvidia. In this scenario, 
models are deployed in private clouds or on-premises, and the infrastructure is 
managed by the application developer. Tools such as Kubernetes and Docker are 
widely used for containerizati
153
The evolving RAGOps stack
With the data and the model layers, the most essential components of the RAG system 
are in place. Now we need a layer that manages the co-ordination between the data and 
the models. This is the responsibility of the application orchestration layer.
Application orchestration layer
When we hear the term orchestration, a musical conductor leading a group of musicians 
in an orchestra comes to mind. An application orchestration layer is somewhat similar. 
It is responsible for managing the interactions among the other layers in the system. It 
is a central coordinator that enables communication between data, retrieval systems, 
generation models, and other services. The major components of the orchestration 
layer are 
¡ Query orchestration component—Responsible for receiving and orchestrating user 
queries. All pre-retrieval query optimization steps such as query classification, 
expansion, and rewriting are orchestrated by this component. The query orches-
154
Chapter 7  Evolving RAGOps stack
Figure 7.5 illustrates the orchestration layer components interacting with the applica-
tion layer, which is supported by the model deployment and data layer.
Generation
coordination
Optimized
query is sent
for retrieval.
Query
parsing
Query
optimization
Retrieval
strategies
Post
Augmentation
Model calling
Query
orchestration
Retrieval
coordination
Agent orchestration
User query
is received
from the
application
layer.
Response
is sent back
to the
application
layer.
Applic
ation
layer
M
o
d
el
l
a
y
e
r
Applic
ation
layer
M
o
d
el
l
a
y
e
r
Retrieved
context and 
query are sent
for generation.
Coordinates among all
assigned agents
Manages the orchestration
process
q
fo
nt
l.
t
.
q
f
Data layer
Data l
ayer
Figure 7.5  The app orchestration layer accepts the user query from the application layer and sends the response 
back to the application layer.
LangChain and LlamaIndex are the most common orchestration frameworks used to 
develop RAG systems. They
155
The evolving RAGOps stack
File
system
Query
orchestration
Retrieval
coordination
The orchestration layer
interacts with all other
layers to orchestrate the
RAG pipelines.
The orchestration layer
receives input from
application and
returns the response.
Internet
CMS
Object
store
Data
lake
NoSQL
Devices
EFSS
Structured
Third-party
API
Curated
data
Source systems
Managed service
providers
Managed language
model
Embedding services
Data services
Managed storage
Other managed services
Data
ingestion
Data
transformation
Data
storage
Data layer
Deployment layer
Managed hosting
Self hosting
Local/Edge deployment
Model layer
Model library
Inference optimization
Model training/ Fine-tuning
LLMs
Embeddings
Task-specific models
Various service providers
offer managed solutions
across the layers of
the RAGOps stack.
Application layer
Orchestration
Generation
coordination
Figure 7.6  Core RAGOps stack where data, model, model deployment, and app orchestration layers interact with 
source systems 
156
Chapter 7  Evolving RAGOps stack
hallucinations and subpar results. Proper engineering and evaluation of the prompts 
are vital to guiding the model toward generating relevant, grounded, and accurate 
responses. This process often involves experimentation. Developers create prompts, 
observe the results, and then iterate on the prompts to improve the effectiveness of 
the app. This also requires tracking and collaboration. Azure Prompt Flow, Lang­
Chain Expression Language (LCEL), Weights & Biases prompts, and PromptLayer are 
among the several applications that can be used to create and manage prompts.
Evaluation layer
Chapter 5 discussed RAG evaluations at length. Regular evaluation of retrieval accu-
racy, context relevance, faithfulness, and answer relevance of the system is necessary to 
ensure the quality of responses. TruLens by TruEra, Ragas, and Weights & Biases are 
commonly used platforms and frameworks for evaluation.
Monitoring layer
Continuous monitoring ensures the l
157
The evolving RAGOps stack
Source systems
Service providers
Data
ingestion
Data
transformation
Data
storage
Data layer
Deployment layer
Managed
hosting
Self
hosting
Local/Edge
deployment
Model layer
Model library
Inference
optimization
Model training/
Fine-tuning
LLMs
Embeddings
Specific
models
Various
service
providers
offer
managed
solutions
across the
layers of
the RAGOps
stack.
Application layer
Orchestration layer
Query
orchestration
Retrieval
coordination
Generation
coordination
The orchestration
layer interacts
with all other
layers to
orchestrate
the RAG
pipelines.
The orchestration
layer receives
input from
application
and returns
the response.
Caching layer
The orchestration layer manages
cache to store and retrieve responses.
The orchestration layer
manages the prompt
library to optimize
prompts to the LLMs.
Prompt layer
• Manages
  prompts for
  all language
  model calls
• Interacts with
  the app
  orchestration
  layer
• Evaluates the RAG pipeline
  during the develop
158
Chapter 7  Evolving RAGOps stack
Table 7.1 is a recap of the critical and essential layers of the RAGOps stack. 
Table 7.1  Critical and essential layers of the RAGOps stack
Layer
Category
Description
Example tools
Data layer
Critical
Responsible for creating and storing the 
knowledge base via ingestion from various 
sources, transformation into embeddings or 
graph structures, and storing for retrieval
AWS Glue, Apache 
Kafka, FAISS, Pine-
cone, Neo4j, Weavi-
ate, Milvus
Model layer
Critical
Contains the models required for generation 
and retrieval in RAG; includes embeddings 
models for vector generation, LLMs for text 
generation, and models for query classifica-
tion, hallucination detection, or re-ranking
OpenAI, Hugging 
Face Transformers, 
Google Gemini, Llama, 
Anthropic
Model 
deployment
Critical
Ensures the models are accessible, perfor-
mant, and scalable; responsible for serving 
models and optimizing inference for fast 
response times
SageMaker, Vertex AI, 
NVIDIA Tr
159
The evolving RAGOps stack
We will now briefly look at a few enhancement layers, which are not mandatory but 
may be employed to further improve the RAG systems. Note that there can be several 
enhancement layers and that they should be tailored to the use case requirements.
7.1.3	
Enhancement layers
Enhancement layers are the parts of the RAGOps stack that are optional but can lead 
to significant gains, depending on the use case environment. They focus on the effi-
ciency, usability, and scalability of the system. Some possible layers are described in the 
following. 
Human-in-the-loop layer
This layer provides critical oversight where human judgment is necessary, especially for 
use cases requiring higher accuracy or ethical considerations. It helps reduce model 
hallucinations and bias. 
Cost optimization layer
RAG systems can become very costly, especially with multiple calls to the LLMs for 
advanced techniques, evaluations, guardrails, and monitoring. This layer helps 
manage
160
Chapter 7  Evolving RAGOps stack
(continued)
Several other providers offer RAG as a service and can handle video and audio tran-
scription, image content extraction, and document parsing. For quick and easy deploy-
ment of a RAG solution, managed service providers can be considered.
We have also discussed several service providers, tools, and technologies that you can 
use in the development of RAG systems. The choice of these tools and technologies 
may depend on factors such as 
¡ Scalability and performance required—RAG systems need to handle large volumes 
of data efficiently, while maintaining low latency. As data scales or traffic spikes, 
the system must remain performant to ensure fast response times. Choose cloud 
platforms that allow for auto-scaling and variable loads. For high-performance 
and scalable retrieval, choose the vector databases that can handle millions of 
embeddings with low-latency search capabilities. Use inference optimization 
tools to help reduce late
161
Production best practices
LangChain, and similar are more likely to offer frequent updates, plugins, and 
third-party integrations.
With the knowledge of the critical, essential, and enhancement layers, you should be 
ready to put together a technology stack to build your RAG system. Let’s now look at 
some common pitfalls and best practices to consider when building and deploying 
production-grade RAG system.
7.2	
Production best practices
Despite earnest efforts in designing and planning the RAG system, some problems will 
inevitably creep up during development and deployment. Although RAG is still in its 
nascent form, some early trends of common mishaps and best practices have emerged. 
There have been many experiments and learnings derived from them to make RAG 
systems work. This section discusses five such practices:
¡ Latency of the system—RAG systems can introduce latency due to the need for mul-
tiple steps: retrieval, reranking, and generation. High latency can significa
162
Chapter 7  Evolving RAGOps stack
¡ Inadequate handling of data privacy and PII—Pre-trained models may generate 
content that includes sensitive information (e.g., personal data and confidential 
details) due to biases in training data. RAG systems may inadvertently leak sensi-
tive information or personally identifiable information (PII) in their responses, 
leading to privacy breaches. Data exfiltration, also known as data theft, extru-
sion, or exportation, is a major threat in the digital world. The solution is to use 
PII masking and data redaction during both the pre- and post-processing stages. 
Ensure compliance with privacy regulations such as GDPR or HIPAA and deploy 
models with privacy filters.
The list of best practices continues to evolve. Latency and scalability are critical for 
managing user experience and access. The promise of hallucination-free gener-
ation and data safety needs to be maintained for the reliability of the system. Table 
7.2 summarizes the challen
163
Summary
Summary
¡ RAGOps stack is a layered approach to designing a RAG system.
¡ These layers are categorized into critical, essential, and enhancement layers.
¡ Critical layers are fundamental for operation; essential layers ensure perfor-
mance and reliability; and enhancement layers improve efficiency, scalability, 
and usability.
Critical layers
¡ Data layer—Responsible for collecting, transforming, and storing the knowl-
edge base. Ingestion tools such as AWS Glue, Azure Data Factory, and Apache 
Kafka enable data collection. Data transformation includes chunking, metadata 
enrichment, and converting data into vector formats. Tools such as FAISS, Pine-
cone, and Neo4j are used for storing embeddings and graph data.
¡ Model layer—Includes embeddings models and LLMs for generation. Embed-
dings models transform the text into vectors, with options from OpenAI, Google, 
Cohere, and Hugging Face. Foundation models (LLMs) such as GPT, Claude, 
and Llama generate outputs and evaluat
164
Chapter 7  Evolving RAGOps stack
¡ Caching layer—Caches frequently generated responses to reduce costs and 
latency in RAG systems.
Enhancement layers
¡ Human-in-the-loop layer—Adds human oversight to ensure higher accuracy and 
ethical decision-making.
¡ Cost optimization layer—Reduces infrastructure costs, especially in large-scale RAG 
systems.
¡ Explainability and interpretability layer—Provides transparency into system deci-
sions, critical for domains such as healthcare and legal.
¡ Collaboration and experimentation layer—Useful for team-based development and 
continuous improvement.
Production best practices
¡ Latency—RAG systems often introduce latency due to multiple steps. Using tech-
niques such as filtering in hybrid retrieval can help reduce response times.
¡ Hallucination—LLMs may still generate incorrect responses. Post-processing vali-
dation and human-in-the-loop systems help mitigate this.
¡ Scalability—Early prototypes may struggle to scale. Managed vector databa
Part 4
Additional considerations
R AG is an evolving technique, and significant research activity has been 
ongoing in this field. In this concluding part of the book, you will learn about the 
popular state-of-the-art variants of RAG and a RAG development framework that 
will assist you in planning and building RAG systems. 
Chapter 8 will teach you about the most important variants of RAG—
multimodal RAG, knowledge graph-enhanced RAG, and agentic RAG—along 
with some other popular ones. Learning about these variants will let you customize 
your RAG systems to the use case you are building. 
Chapter 9 revisits all the concepts discussed in this book, organized within a 
RAG development framework. This framework will help you strategically plan the 
development of your RAG system. You’ll also get to know a few areas of research 
that remain open at the time of writing this book. 
This concluding part of the book wraps up your introduction to RAG. By the 
end of this book, you should no

167
8
Graph, multimodal, 
agentic, and other 
RAG variants
This chapter covers
¡ Introducing RAG variants
¡ Knowledge graph RAG
¡ Multimodal RAG
¡ Agentic RAG
¡ Other RAG variants
The first part of the book introduced retrieval-augmented generation (RAG) and 
the core idea behind it. The second part dealt with building and evaluating basic 
RAG systems. Part 3 took RAG beyond the naïve approach and discussed advanced 
techniques and the technology stack that supports a RAG system. The last part of 
the book looks at more RAG patterns, and we conclude our discussion with a few 
best practices and some areas for further exploration.
Chapter 8 looks at some popular RAG variants. These variants adapt different 
stages of RAG (i.e., indexing, retrieval, augmentation, and generation) to specific 
use case requirements. The chapter begins by discussing the emergence of these 
variants and the purpose they serve. We then continue talking about three important
168
Chapter 8  Graph, multimodal, agentic, and other RAG variants
variants that have gained prominence in applied RAG. These are knowledge-graph-
enhanced, multimodal, and agentic RAG. We also briefly examine other RAG variants 
that significantly contribute to the evolution of RAG in practical applications. We 
discuss the purpose and motivation behind each variant. This chapter also breaks down 
the workflow, features, and technical details of the variants along with their strengths 
and weaknesses. For simplicity, the code for these variants is not included in this chapter 
but can be found in the book’s code repository.
By the end of this chapter, you should 
¡ Be familiar with the idea and motivation behind RAG variants.
¡ Have an in-depth understanding of graph, multimodal, and agentic RAG.
¡ Be aware of several popular RAG variants and the use cases they solve.
There are several limitations of a naïve approach to RAG that affect the overall usability 
of a standard RAG system. T
169
Multimodal RAG
enables the system to fetch information from nontextual documents and provide 
additional context.
¡ Knowledge graph RAG—Integrates knowledge graphs into the retrieval process. 
This idea was introduced in chapter 6 as part of improving the indexing struc-
ture. Knowledge graphs help establish relationships between entities, providing 
better context, especially in multi-hop queries.
¡ Agentic RAG—Incorporates LLM agents into the RAG framework. These agents 
enable autonomous decision making across the RAG value chain from index-
ing to generation. Simultaneously, all components become adaptive to the user 
query.
In addition to these three, we also touch upon additional variants, such as corrective 
RAG, self-RAG, and more, but first, we begin by discussing multimodality.
8.2	
Multimodal RAG
Until now, we have seen that standard RAG systems are effective in managing and 
retrieving textual data to generate context-aware and grounded responses. However, 
the scope of
170
Chapter 8  Graph, multimodal, agentic, and other RAG variants
Multimodal RAG is, therefore, the extended variant of standard RAG with the capabil-
ity to process multiple data modalities. Before diving into the requirements and archi-
tectural details of multimodal RAG, let’s ponder over the use cases where multimodal 
RAG is necessary. 
8.2.2	
Multimodal RAG use cases
There are several industries and functions where a multimodal variant of RAG is 
required, such as 
¡ Medical diagnosis—A diagnostic assistant can work with patient records that may 
include medical history (in text form), lab results (in tabular form), and diagnos-
tic images (like X-rays, MRIs, etc.), along with studies and research papers that 
include graphs, charts, or microscopic images. When the patient comes in for a 
consultation, this assistant can provide a holistic analysis to the doctor.
¡ Investment analysis—Working with financial reports and other filings that have 
charts showing trends, earnings, and
171
Multimodal RAG
files of different modalities, creating embeddings for multimodal data requires special 
attention. Let’s look at each of the components one by one. 
The data-loading step is quite like the standard text-only RAG but now includes 
connectors and data loaders for nontext modalities. There are several options 
available. Pillow, also known as PIL, is a popular Python library for loading images. 
Unstructured is an open source library that includes components for ingesting a 
variety of data formats. Pydub is another Python library that allows the loading of audio 
files such as WAV and MP3. LangChain provides an integration with the unstructured 
library. UnstructuredImageLoader is a class available in LangChain document 
loaders for loading images. For audio and video transcription, libraries such as 
OpenAIWhisperParser, AssemblyAIAudioTranscriptLoader, and YoutubeLoader can 
be used. Likewise, for tabular data CSVLoader and DataFrameLoader come in handy. 
For simpli
172
Chapter 8  Graph, multimodal, agentic, and other RAG variants
Dog
Bark
Fly
Queen
King
Similar data across
modalities lies close to
each other in a shared
embeddings space.
Figure 8.2   
Images, text, 
video, and audio 
are plotted on the 
same embeddings 
space. Dog, bark, 
and dog’s image 
are close to each 
other.
Pretraining, or CLIP) and an audio-text embeddings model (e.g., Contrastive 
Language–Audio Pretraining, or CLAP). The knowledge base has text, image, and 
audio embeddings in different embeddings spaces and stored separately. Figure 8.3 
is an example of CLIP image–text embeddings where image and text embeddings are 
projected onto a shared embeddings space.
...
...
T
T
T
I1
I2
In
1
2
n
T1I1 T1I2
T1In
T2I1 T2I2
T2In
TnI1 TnI2
TnIn
...
...
...
...
...
...
...
Text
encoder
Image
encoder
Image and text
embeddings are
projected in a
shared embeddings
space.
Figure 8.3  CLIP uses multimodal pre-training to convert classification into a retrieval task, which 
enables pre-tra
173
Multimodal RAG
Conversion of all non-text data into text is employed to first convert all nontext 
(image) data into text using a multimodal LLM and then follow the standard text-
only RAG approach. (A multimodal LLM is a large language model that processes 
all modalities of data. You will read more about multimodal LLMs later in this sec-
tion.) In this strategy, you may notice that we may not be entirely using multimodal 
data as information loss is bound to occur when converting nontext to text data. In 
a variation of this strategy, instead of converting all multimodal data into text and 
using it as text, a two-pronged approach is employed. Here all multimodal data is 
summarized in text using a multimodal LLM. Embeddings of this text are used to 
search for during the retrieval process. However, for generation, not only the sum-
mary but the actual multimodal file (e.g., a .jpeg) is retrieved and passed to the mul-
timodal LLM for generation. This reduces the loss of informa
174
Chapter 8  Graph, multimodal, agentic, and other RAG variants
Table 8.1  Indexing pipelines of text-only vs. multimodal RAG
Indexing 
component
Text-only RAG
Multimodal RAG
Loading
Standard text data 
loaders are used to 
load documents, such 
as plain text files, 
PDFs, and other text-
based formats.
Requires connectors for additional data types. For images, 
libraries such as Pillow (PIL) and Unstructured­
ImageLoader in LangChain are used; for audio, we use 
libraries such as Pydub or OpenAIWhisperParser, 
whereas CSVLoader and DataFrameLoader are used 
for tabular data. Audio and video transcription tools such 
as AssemblyAI and YoutubeLoader are also incorporated to 
preprocess audio/video content.
Chunking
Text data is divided 
into segments 
(chunks) based on 
context or structure 
(e.g., sentences, para-
graphs) and optionally 
enriched semantically.
Follows text chunking when data is transcribed to text 
(audio/video). For raw audio, voice activity detection 
(VAD) can be 
175
Multimodal RAG
documents are retrieved from each modality-specific embeddings space based 
on similarity. These documents may later be re-ranked before augmentation 
and generation. 
–	 When nontext data is converted into text, the retrieval process is the same as 
the standard text-only RAG. In the variation where both text summaries and 
raw files are used, the retriever first retrieves the relevant summaries from the 
text embeddings space, and then the files from the document stores mapped 
to those summaries are also retrieved.
¡ Augmentation—The augmentation step remains the same as text-only RAG, 
except that the augmented prompt now includes the raw multimodal file accom-
panying the text prompt.
¡ Generation—Like multimodal embeddings, for processing and generating mul-
timodal data, multimodal LLMs are used. LLMs are limited by their ability to 
process text data only. Multimodal LLMs are transformers-based models, too, 
but have been trained on data of all modalities, in
176
Chapter 8  Graph, multimodal, agentic, and other RAG variants
Input modality
embeddings
Text embeddings
Multimodal LLM
Augmentation
While the search
takes place on the
text embeddings, raw
documents are also
retrieved to pass to
the LLM.
User query
Multimodal
embeddings
Multi-vector
retriever
Text
retriever
To retrieve all
relevant documents,
search is conducted
in the collection
of each modality.
Multimodal
retriever
Option A: Multimodal
Option B: Specific modality
Option C: Single modality
M
Figure 8.5  For each of the three approaches, the generation pipeline also adapts.
8.2.4	
Challenges and best practices
Multimodal RAG systems are gaining prominence owing to the diversity present in 
enterprise data. However, one must note that with multimodality, the complexity of 
the system increases along with higher latency and more expenditure on multimodal 
embeddings and generation. Some of the common challenges associated with multi-
modal RAG are 
¡ Ensuring coherent alignment betw
177
Knowledge graph RAG
¡ Not all models are capable of effectively processing and integrating multimodal 
data of all modalities. Incorporate only those that add significant value to the task 
to optimize performance and resource utilization. 
We have looked at a RAG variant that extends the capability of RAG to different data 
modalities. However, standard RAG is still deficient when the information is dispersed 
across different documents. Let’s now look at a pattern in which knowledge graphs are 
used to establish higher-order relationships.
8.3	
Knowledge graph RAG
Imagine summarizing a large report or answering complex questions that draw infor-
mation from diverse sources. For example, a question such as, “What are the main 
themes in this report?” or “Which products in the catalogue are endorsed by the same 
celebrities?” are questions that are difficult for standard RAG systems to answer. 
In a summarization task such as the “main themes” in a report, there is no chunk of 
the
178
Chapter 8  Graph, multimodal, agentic, and other RAG variants
Customer
A
Product X
Product Y
Customer
B
Review 1
purchased
wrote
feedback for
purchased
purchased
Id: C001
Name: John Doe
Customer since: 11/1/2020
Id: C002
Name: Jane Doe
Customer since: 12/2/2010
SKU: PX001
Brand: Brandex
Name: ProductX
SKU: PX002
Brand: Brandex
Name: ProductY
Timestamp: ...
NODE
EDGE
ATTRIBUTES
Figure 8.6  Knowledge graph representation of customer activity where nodes (circles) represent 
entities, edges (arrows) represent relationships, and attributes (rectangles) are the properties.
data, and query languages such as Cypher, Gremlin, and SparkQL are used for graph 
traversal. Readers are encouraged to learn more about graph databases, but some key 
concepts to keep in mind are
¡ Nodes and edges—Nodes represent entities, and edges represent relationships to 
form the graph structure and enable a visual structure to the knowledge.
¡ Attributes—Attributes are properties of entities(nodes) and relatio
179
Knowledge graph RAG
8.3.2	
Knowledge graph RAG use cases
Knowledge graphs can be useful in a variety of use cases where the ability to handle 
multi-hop relationships, entity disambiguation, and complex networks is required. 
Standard RAG systems are limited to retrieving isolated information chunks, while 
knowledge graph RAG can dynamically connect and analyze data points within a net-
work, making it ideal for applications requiring a deep understanding of interrelated 
data. Here are some examples: 
¡ Personalized treatment plans—Knowledge graph RAG can link drugs, treatments, 
and conditions in a networked format, which allows it to identify potential inter-
actions and customize treatment recommendations based on multiple factors. 
Standard RAG can retrieve information about a specific drug or treatment but 
struggles to cross-reference interactions across a network of symptoms, condi-
tions, and treatments.
¡ Personalized product recommendations—Standard RAG can retrieve ind
180
Chapter 8  Graph, multimodal, agentic, and other RAG variants
for additional context if required. This approach enhances the precision of retrieval, 
while maintaining the broader context. 
An efficient way to store documents in a hierarchical structure is in graphs. Parent 
and child documents can be stored in the nodes with a relationship “is child of.” More 
levels of hierarchies can be created. In figure 8.7, there are three levels of indexing hier-
archy, and while the search happens at the lowest level, parent documents at a higher 
hierarchy level are retrieved for deeper context.
Graph knowledge base
Document
A
Section
I
Section
II
Section
III
Chunk
1
Chunk
2
Hierarchy
level 3
Hierarchy
level 2
Hierarchy
level 1
[0.32,0.64......., 0.91]
[0.21,0.4......., 0.19]
is section of
is chunk of
User query
Retriever
Augment
Response
Retriever searches chunks
at the lowest graph
hierarchy and finds the
most similar ones.
While the search is at
the lowest level, the
retrieved text is f
181
Knowledge graph RAG
knowledge graph to retrieve related chunks. To do this, a set of entities and relation-
ships are extracted from the chunks using an LLM. 
In the retrieval stage, the first step is a usual vector search executed based on the user 
query. An initial set of chunks is identified that has a high similarity with the user query. 
In the next step, the knowledge graph is traversed to fetch-related entities around the 
entities of the chunks identified in the first step. By doing this, the retriever fetches not 
only the chunks similar to the user query but also related chunks, which leads to deeper 
context and can be quite effective in solving multi-hop queries. This is often coupled 
with hierarchical structures and a re-ranking of retrieved documents. Figure 8.8 shows 
Higher-order hierarchies
User query
Retriever
Graph knowledge base
Augment
LLM
Response
Retriever searches
chunks at the lowest
graph hierarchy and
finds the most similar
ones.
While the search
identi
182
Chapter 8  Graph, multimodal, agentic, and other RAG variants
an enhanced knowledge graph, where chuwnks also have the extracted entities and 
relationships. During retrieval, in addition to similar chunks, the parent chunks of 
related entities are also retrieved.
Graph communities and community summaries
As discussed before, knowledge graphs are about entities and their relationships. 
Depending on the process, there may be patterns in which certain entities interact 
more with each other. Graph communities are a subset of entities connected more 
densely. For example, communities of customers with similar demographics and buy-
ing patterns can be identified or clusters of product features that appear together 
can be discovered. Community detection algorithms such as the Leiden and the Lou-
vain algorithm are employed to detect communities within a knowledge graph. After 
detecting these communities, an LLM is used to generate summaries of the entities 
and the relationship info
183
Knowledge graph RAG
A
B
X
Y
Higher-order hierarchies
User query
Retriever
Graph knowledge base
Augment
LLM
Response
Similarity
search happens
at the
community
summary level.
The most similar
summary is
retrieved.
Optionally, all the
chunks belonging
to that community
can also be
retrieved.
Since the community
summary contains
information not of a few
chunks but of all the chunks
of similar themes, these
community summaries can
answer questions at a higher
thematic level.
Community II
Community I
is chunk of
is entity of
belongs to
belongs to
belongs to
[0.32,0.64......., 0.91]
[0.32,0.64......., 0.91]
Communities
are created
using a
community
identification
algorithm.
Communities
summaries are
created using
an LLM and
stored in the
vector form.
f
Figure 8.9  Communities club entities under a consistent theme and summarize the information at this 
group level. Since the summaries are created from a high number of thematically related chunks, these 
summaries can answer broad queries
184
Chapter 8  Graph, multimodal, agentic, and other RAG variants
class is available in the langchain_experimental library that abstracts the entity 
relationship extraction from documents. 
¡ Storage—Once the entities, relationships, and attributes have been extracted, 
these can be stored in a graph database such as Neo4j. LangChain has integration 
with the Neo4j graph database, and the Neo4jGraph library from the langchain_
community can be used. Since the entity relationship extraction is done at a chunk 
level, the storage is also iterative, and the graph database is updated after each 
pass. In LangChain, the add_graph_documents() function of the Neo4jGraph 
library can be used to directly update the knowledge graph. 
¡ Creating community summaries—As discussed previously, once the knowledge 
graph is created, an algorithm is used to detect communities, and an LLM is used 
to create a summary of the community. Graphrag, a library developed by Micro-
soft, provides end-to-end kno
185
Knowledge graph RAG
Generation pipeline
Since the nature of the knowledge base in graph RAG is quite unlike standard RAG, it 
requires significant changes in the generation pipeline. The retrieval process becomes 
slightly more nuanced than vector retrieval because of an additional step of graph tra-
versal. Graph databases such as Neo4j have introduced vector indexes, via the Neo4j 
vector search plugin, which represent nodes and attributes as embeddings and enable 
similarity search. For effective retrieval, the user query (in natural language) is con-
verted into a graph query that can be used to traverse the knowledge graph. Neo4j uses 
a graph query language called Cypher. For using the Cypher query language, there are 
a couple of approaches: 
¡ Template based—Several pre-defined Cypher templates are created and based on 
the user query, an LLM selects which template to use. This is an extremely rigid 
and limiting approach.
¡ LLM-generated query—An LLM generates the Cypher q
186
Chapter 8  Graph, multimodal, agentic, and other RAG variants
Step
Vector RAG
Graph RAG
Storage
Stores embeddings in a vector 
database
Entities and relationships are stored in a graph 
database (e.g., Neo4j), with the option to update 
the graph iteratively. Tools such as LangChain’s 
Neo4jGraph can automate this process.
Community 
summaries
Not applicable; primarily relies 
on similarity search on individ-
ual embeddings
Detects communities within the knowledge graph 
and uses an LLM to create summaries for each 
community. These summaries can be stored as 
vectors for a hybrid graph–vector RAG approach.
Retrieval
Performs direct similarity 
searches on embeddings
Involves graph traversal using Cypher queries, 
generated either from pre-defined templates or 
dynamically by an LLM. Neo4j’s vector indexes can 
enhance similarity-based node searches.
Augmentation
Uses retrieved embeddings to 
augment the user’s query
Retrieved nodes, relationships, or summaries aug-
ment the user’s
187
Agentic RAG
8.4	
Agentic RAG
By now, you understand that challenges exist with standard RAG systems. They may 
struggle with reasoning, answering complex questions, and multistep processes. One 
of the key aspects of comprehensive RAG systems is the ability to search through multi-
ple sources of data. This can be internal company documents, the open internet, third-
party applications, and even structured data sources like an SQL database. So far in 
this book, we have built systems that can search through a single knowledge base, and 
for any query, the entire knowledge base is searched through. 
Two challenges arise with this approach. First, all information must be indexed and 
stored in a single vector store, which leads to storage problems at scale. Second, for any 
query, the entire knowledge base needs to be searched, which is highly inefficient for 
large knowledge bases. To overcome this challenge, a module that can understand the 
user’s query and route the query to a re
188
Chapter 8  Graph, multimodal, agentic, and other RAG variants
Tools assist the agent in performing actions on resources external to it. This can be 
conducting a web search on the internet, querying an external database such as an SQL 
database, invoking a third-party API such as a weather API, and similar. The core LLM 
brain is responsible for sending the payload request to the tools in the accepted format. 
These four components and their interactions are shown in figure 8.11. 
Memory
module
Tools
Receives and parses inputs from the
user, identifies user intent, acts on
the plan from the planning module,
and responds to the user request
Creates a step-by-step sequence of
tasks that should be followed to
respond to the user’s request
Planning
module
User
Core LLM
brain
Provides access to external
resources. These can be in form of
APIs, functions, and other
interfaces. The LLM brain invokes
these tools with the appropriate
payload.
Stores short-term
conversational history
and lon
189
Agentic RAG
retrieved from memory, the core LLM brain must prompt the user to provide this infor-
mation. When the core LLM brain gets the plan from the planning module, it retrieves 
previous booking information, invokes the tool to retrieve flight information, compares 
the new information with the old information in memory, and crafts a response based 
on this analysis. This simple workflow of the agent is illustrated in figure 8.12.
The LLM brain
calls flight API
with the necessary
inputs.
“Is my flight on
schedule?”
Intent -> Check
flight status
1. Get flight details from memory
    or from the user.
2. Use flight API to fetch live status.
3. Generate a user-friendly
    response.
Flight number XX000 is scheduled
on 01JAN2025 at 4 PM.
{
"flight number”:“XX000",
"scheduled_time":"010120251600",
"estimated_time":"010120251645"
}
Core LLM brain
Tools
Memory
Planning
“Your flight XX000 is
delayed by 45 minutes
and will now depart on
01 Jan 2025 at 4:45 PM.”
User query
Response
Cor
190
Chapter 8  Graph, multimodal, agentic, and other RAG variants
agentic RAG span across industries, so it makes more sense to look at the capabilities 
of agentic RAG.
8.4.2	
Agentic RAG capabilities
In our introduction to agentic RAG, we highlighted the challenge in standard RAG 
using a single knowledge base. Agentic RAG infuses abilities in the RAG system that 
make the system more efficient and accurate.
Query understanding and routing
Based on the user query, an LLM agent can be tasked with deciding which knowledge 
base to search through. For example, assume a programming assistant that can not 
only search the codebase but also the product documentation, along with searching 
the web. Depending on the question that the developer asks, the agent can decide 
which database to query. For generic messages such as greetings, the agent can also 
decide not to invoke the retriever and send the message directly to the LLM for a 
response. 
Tools usage
In the previous example, the syst
191
Agentic RAG
¡ Data loading—Loading data and extracting information is the first and incredi-
bly crucial step of RAG system development. Accurate parsing of information is 
critical in building an accurate RAG system. Parsing complex documents such as 
PDF reports can be tough. While there are libraries and tools present for these 
tasks, LLM agents can be used for high-precision parsing. The importance of 
metadata in RAG cannot be overstated. It is useful for filtering, more contex-
tual mapping, and source citation. In most scenarios, it is difficult to source rich 
metadata. LLM agents can be used to build metadata architecture and extract 
contextual metadata. 
¡ Chunking—In agentic chunking, chunks from the text are created based on a 
goal or a task. Consider an e-commerce platform wanting to analyze customer 
reviews. The best way for the reviews to be chunked is if the reviews about a par-
ticular topic are put in the same chunk. Similarly, the critical and positive review
192
Chapter 8  Graph, multimodal, agentic, and other RAG variants
Agentic parsing is effective in extracting comprehensive
information from complex documents, and agentic
metadata extraction not only extracts the metadata but
can also decide the accurate metadata architecture.
Vector
Graph
Embedding
Graph summaries
Chunking
Data loading
Storage
Document parsing
Metadata extraction
The chunking agent processes
every sentence in a passage
and allocates it to a chunk
with similar sentences.
Agents recursively extract
entities and relationships and
create sub-graphs to generate
cohesive summaries.
Figure 8.13  Agentic embellishment to the indexing pipeline enhances the quality of the knowledge base.
Adaptive retrieval strategies also bring significant improvement in the retrieval 
stage. 
¡ Augmentation—Agents can choose the correct prompting technique for augmen-
tation, depending on the nature of the query and the retrieved context. Prompts 
can also be generated dynamically by an agent.
193
Agentic RAG
to the user query. This is particularly useful in multi-hop reasoning and fact 
verification.
Another way to think about agentic RAG is that wherever dynamic decision-making 
can improve the RAG system, an agent can be used to autonomously make those deci-
sions. From the previous discussion, you may conclude that agentic RAG is a superior 
version of standard RAG. Table 8.4 summarizes the advantages of agentic over stan-
dard RAG.
Table 8.4  Advantages of agentic RAG
Aspect
Standard RAG
Agentic RAG
Retrieval 
process
Passive retrieval based on 
initial query
Adaptive retrieval with intelligent agents routing and 
reformulating queries as needed
Handling com-
plex queries
Struggles with multistep rea-
soning and complex queries
Can be used to break down and address complex, 
multifaceted queries
Tool integration
Limited integration with exter-
nal tools and APIs
Seamless integration with various external tools and 
APIs for enhanced information gathering
Scalability
Cha
194
Chapter 8  Graph, multimodal, agentic, and other RAG variants
and variations to the standard RAG systems. The next section discusses variants that 
show significant promise.
8.5	
Other RAG variants
We have talked about the three major RAG variants in this chapter. Research in the 
field is bustling, and every week, several papers are released by researchers about their 
experiments and key findings. Out of these papers, quite a few demonstrate RAG vari-
ants that find relevance in practical applications. We close this chapter by briefly dis-
cussing four such RAG variants.
8.5.1	
Corrective RAG
The effectiveness of a RAG system depends on the quality of retrieval. Inaccuracies 
in retrieval negate all RAG benefits. To address this, the corrective RAG (CRAG) 
approach evaluates the quality of retrieved documents. It uses a lightweight evaluator 
and triggers corrective action if the retrieved information is found to be inaccurate. 
The key CRAG components are
¡ Retrieval evaluator—A
195
Other RAG variants
Decomposed
knowledge strips
User query
Knowledge
base
Retriever
Retrieval
evaluator
LLM
Response
Knowledge refinement
Web search
Correct
Ambiguous
Incorrect
Evaluator
Evaluated
knowledge strips
Recomposed
knowledge
Query
refinement
Search
API
Search
results
Selected
results
Retrieved
documents are
evaluated by a
retrieval evaluator
that classifies
them into
correct, incorrect,
and ambiguous.
Documents identified as correct are further split into
“knowledge strips,” which are generally single
statements. These strips are re-evaluated by the
evaluator. The strips classified as correct are
recomposed and passed to the LLM as the context.
Documents identified as incorrect are substituted
by results from web search. The original user
query is rewritten for web search. The search
results are filtered and combined to be sent to
the LLM as context.
Correct
Ambiguous
Incorrect
Corrective RAG
Figure 8.14  CRAG corrects the knowledge at the most granular level, hence the na
196
Chapter 8  Graph, multimodal, agentic, and other RAG variants
¡ RAG drafter—A smaller LLM produces initial answer drafts based on each cluster 
subset, generating responses and rationales in parallel for efficiency.
¡ RAG verifier—A larger LLM evaluates each draft’s accuracy and coherence, 
assigning confidence scores based on self-consistency and rationale support.
The key advantage of speculative RAG is faster response generation by reducing the 
workload on the generator LLM and performing parallel draft generation. However, 
some of the following limitations require careful consideration: 
¡ Involves managing a two-model setup and document clustering, which may 
increase initial setup complexity. 
¡ Document clustering directly affects draft diversity, and poor clustering can lead 
to redundant drafts by grouping highly similar or repetitive documents into mul-
tiple clusters. 
¡ The smaller LLM may require training for effective draft and rationale generation.
Unlike standard 
197
Other RAG variants
¡ Processing multiple passages in parallel and self-reflection may increase compu-
tational demands.
¡ The additional training and use of reflection tokens require fine-tuning of 
thresholds.
Self RAG is one of the most cited techniques in research on RAG. Its dynamic adjust-
ment of retrieval based on task needs evaluates output quality, achieving superior 
accuracy.
8.5.4	
RAPTOR
Recursive abstractive processing for tree-organized retrieval, or RAPTOR, is a RAG 
variant designed to handle hierarchical relationships in data. It creates a multilevel, 
tree-based structure of recursive summaries, capturing both granular details and over-
arching themes in long documents. Like graph RAG, RAPTOR uses a tree structure to 
achieve similar objectives. Here are the key RAPTOR components:
¡ Chunk clustering and summarization—Chunk embeddings are clustered based on 
similarity, and an LLM is used to summarize the clusters. Soft clustering with 
Gaussian mixture models all
198
Chapter 8  Graph, multimodal, agentic, and other RAG variants
With this chapter, we are almost at the end of our discussion on RAG. The last chap-
ter discusses some of the independent considerations and best practices across differ-
ent stages of RAG system lifecycle. 
Summary
Introducing RAG variants
¡ RAG variants are adaptations of the naïve RAG framework that extend its func-
tionality to specific use cases.
¡ These variants address challenges, such as processing nontextual data, improv-
ing relational understanding, enhancing accuracy, and enabling autonomous 
decision-making.
¡ Three major RAG variants were discussed in depth: multimodal, graph, and 
agentic RAG.
¡ Other promising RAG variants are corrective RAG, speculative RAG, self RAG, 
and RAPTOR.
Multimodal rag
¡ It extends RAG capabilities to handle multiple data modalities such as text, 
images, audio, and video. It can be used for
–	 Medical diagnosis—Analyzing text, images (X-rays), and tabular data (lab 
results)

199
Summary
¡ As for the pipeline enhancements, the knowledge graph RAG extracts entities, 
relationships, and attributes from chunks to create a graph in the indexing pipe-
line. As for the generation pipeline, it incorporates graph traversal using graph 
query languages such as Cypher.
¡ Building and maintaining knowledge graphs is complex and computationally 
expensive. It also requires custom adaptations for each deployment.
Agentic RAG
¡ It introduces LLM-based agents for autonomous decision-making and dynamic 
query routing. Agentic RAG can be used for
–	 Query understanding and routing to relevant data sources
–	 Adaptive retrieval and multistep generation
–	 Integration with tools such as web search APIs and external databases
¡ With regard to pipeline enhancements, agentic RAG enhances chunking, meta-
data extraction, and embeddings selection with agentic decision-making in the 
indexing pipeline. In the generation pipeline, it dynamically augments prompts 
and employs iterati
200
9
RAG development 
framework and 
further exploration
This chapter covers
¡ A recap of the concepts covered in this book 	
	 using a six-stage RAG development framework
¡ Areas for further exploration
The previous eight chapters covered a wide breadth of retrieval-augmented gener-
ation (RAG), including a conceptual foundation, critical components, evaluation 
methods, advanced techniques, the operations stack, and essential variants of RAG. 
By now, you should be equipped with the necessary information required to develop 
RAG systems. 
This concluding chapter summarizes the discussion and recaps all the previously 
discussed concepts. To accomplish this, we put all the different aspects of developing 
RAG systems together and came up with a RAG development framework. Across the 
six stages of this RAG development framework, we recap the concepts covered in this 
book along with some best practices. This framework not only covers the technical 
aspects but also looks at the develo
201
RAG development framework
RAG is a rapidly evolving technique. At the end of this chapter, we also discuss some 
of the ideas that you can explore further. Some of these approaches to incorporating 
context may compete with the RAG technique, while others may be complementary. 
By the end of this chapter, you should 
¡ Have reviewed and consolidated your understanding of key RAG concepts.
¡ Get a solid understanding of the RAG development framework.
¡ Be ready to build and deploy RAG systems.
Often, the problem statements that the developer of a RAG system is presented with 
will be open ended. For example, an e-commerce platform wants to develop a buying 
assistant, or the marketing function wants a research agent to track and summarize 
competitive information. So, how does one navigate from an open-ended problem 
statement to a fully developed RAG system? It becomes very important that this journey 
is guided by a thought process. For this purpose, let’s define and discuss a fra
202
Chapter 9  RAG development framework and further exploration
6	 Maintenance—This final stage is an ongoing one that involves system moni-
toring, incorporating user feedback, and keeping abreast of technological 
enhancements.
Bear in mind that the RAG development framework is not a linear process, but flexi-
ble, iterative, and cyclic. Figure 9.1 illustrates the cyclic nature of the six stages of the 
RAG development framework, showing the key artifacts of each stage.
DESIGN
DEVELOPMENT
DEPLOYMENT
MAINTENANCE
EVALUATION
INITIATION
Designing RAG
pipelines and finalizing
the layers of the
RAGOps stack
Developing RAG
pipelines, and creating a
prototype for evaluation
and feedback
Deployment of system to
production and serving
the desired users
Tracking and measuring
system performance and
improving the system
based on feedback
Assessing RAG metrics
and system
performance
Understanding the use
case, and gathering and
analyzing requirements
R
A
G
d
e
v
e
l
o
p
m
e
n
t
f
r
a
m
e
w
o
r
k
203
RAG development framework
9.1.1	
Initiation stage: Defining and scoping the RAG system
The journey toward a successful RAG system begins with the initial interactions with 
the stakeholders. This is an opportunity to gain an in-depth understanding of the 
problem statement and the user requirements. It is an exploratory stage and sets the 
direction of the project.
Use case identification
A lot of the choices a developer will make in the development process of a RAG sys-
tem depend heavily on the use case being addressed. Even a basic understanding of 
the industry domain/function and a simple definition of the use case is enough to 
answer crucial starting questions about the system. The requirement of a RAG system 
needs to be assessed here. Recall from chapter 1 the challenges that RAG solves: RAG 
overcomes training data limitations, knowledge cut-off date, and LLM hallucinations 
to bring factual accuracy, reliability, and trust to the system. It is important to assess 
whethe
204
Chapter 9  RAG development framework and further exploration
Use case
System
requires data
that may not
be present in
training set?
System
requires data
that is current
or updates
frequently?
System
generates
facts?
Are users
looking for
sources?
Is a RAG
system
required?
Creative writing
assistance
No: LLMs do not
need any
additional data for
creative writing.
No: LLMs do not
need any current
information for
creative writing.
Maybe:
Creative writing
may not
necessarily
need generated
facts.
No: User
expectations
from creative
writing do
not need any
source
citation.
No
Customer support
bot
Yes:
Product/Company
specific
information may
not be present in
LLM training data.
Yes: Product
information,
inventory levels,
and order
information changes
frequently.
Yes: All
generated
information is
factual.
Maybe:
Sources may
enhance
customer
experience.
Yes
Language
translation
No: LLMs do not
need any
additional data for
language
translation.
No: LLMs do not
need any current
information f
205
RAG development framework
¡ Functional requirements—These are the core functionalities of the system, such 
as the supported data types, number of documents to be retrieved and length/
tone/style of generation, and similar. Functional requirements are influenced 
by user needs and business objectives. They are also the main influencers of the 
development process.
¡ Non-functional requirements—These are requirements about the performance, 
scalability, reliability, security, and privacy of the system. There may be additional 
requirements such as legal and compliance, especially for regulated industries.
¡ Constraints—One should also focus on any constraints that the system should be 
cognizant of, such as access to the internet, availability of data, cost, and integra-
tion with existing systems. 
A customer service system, for example, may be envisioned to reduce customer query 
resolution time, requiring quick response time and a constraint of integrating with 
existing customer
206
Chapter 9  RAG development framework and further exploration
precise, and quantifiable so that they can lead to specific development steps. For exam-
ple, a non-functional need for a quick response may be too vague. Instead, a better 
requirement is that 90% of queries should be responded to within 2 seconds. Similarly, 
a constraint of limited internet connectivity can lead the developer to believe that a 
completely offline system is required. Such vagueness in the requirements needs to be 
addressed in further interactions with the stakeholders. 
At this stage, it is also important to define the success criteria on which the system will 
be evaluated. A few success metrics need to be defined and agreed on. For developers, 
these success metrics should be different from the business objectives since business 
outcomes may depend on factors beyond their control. Latency, throughput, percent-
age of queries resolved, and similar, are good criteria for success metrics. Figure 9.4 
p
207
Design stage: Layering the RAGOps stack
High-level architecture
Once the requirements are understood well, the initiation stage can be deemed com-
plete. It is good practice to close the initiation stage with a high-level architecture dia-
gram that can be used as a starting point for the design stage. This architecture can be 
used to bring alignment among stakeholders and discuss the requirements further. The 
focus of this high-level architecture is to illustrate the system inputs and outputs. Since 
data plays such a crucial role in a RAG system, this high-level architecture should also 
include the data component. As illustrated in figure 9.5, for a multichannel customer 
support system, the system must allow inputs and outputs from and to different channels.
Model layer
App
orchestration
Prompt
Guardrails
Security
Cache
Data layer
Deployment
Human in
the loop
email
Support portal
WhatsApp
User
query
email
Support portal
WhatsApp
Product portal Catalogue Order data
Escalation

208
Chapter 9  RAG development framework and further exploration
systems, file types, and nature of the data itself to determine the development steps 
for the knowledge base. Recall from chapter 3 that the knowledge base is created for 
a RAG system via the indexing pipeline. Components such as data loading, chunking, 
embeddings, and storage form the indexing pipeline. In chapter 7, we also discussed 
that the data layer of the RAGOps stack enables this by extracting, transforming, and 
loading the data. Figure 9.6 summarizes the indexing pipeline components and the 
data layer.
The data-loading
component is
responsible for
connecting to external
sources, and
extracting and parsing
information.
The data-splitting
component is
responsible breaking
down long pieces
of text into smaller,
manageable parts
called “chunks.”
The data conversion
component is
responsible converting
text chunks into
numerical vectors
called “embeddings.”
The data storage
component stores
the embeddings in
perm
209
Design stage: Layering the RAGOps stack
¡ Are the connectors readily available? If yes, which tools or services are required 
to establish these connections?
¡ Which connectors will need to be developed? Which technology will these con-
nectors be developed on?
¡ Is access to open internet required? How will the system connect to the internet?
The following group of questions is about parsing files:
¡ Which file formats will be ingested?
¡ How will the web pages be scraped, if required? 
¡ Do we have the necessary parsers for the different file types?
¡ Is some special parsing technique required to be developed?
¡ Can there be more than one modality of data in a single file? 
The answers to these questions will determine the tools you will need to use for ingest-
ing data and the parts that will need to be developed.
Data transformation
Once the data is ingested, the transformation step converts the data into a suitable for-
mat for the knowledge base. In the data transformation st
210
Chapter 9  RAG development framework and further exploration
When it comes to chunking, consider asking the following questions:
¡ Is the chunk size pre-determined? If not, what chunk sizes should be experi-
mented with?
¡ Is the data in a format that will warrant structured chunking? 
¡ What techniques and models will be employed for semantic chunking, if 
required?
¡ Is a chunking agent readily available, or will it need to be built? Which models, 
algorithms, and tools will be used by the chunking agent?
The following group of questions covers graphRAG:
¡ Is a hierarchical indexing structure required? 
¡ Do we need to extract entities and relationships for relational context? Do we 
have the necessary budget?
¡ What approaches are we going to take for entity-relationship extraction? 
¡ Are we using any frameworks for graph extraction?
¡ Which models are going to be used?
As for embeddings, ask the following:
¡ Which embeddings model will we use? Are there any domain-specific emb
211
Design stage: Layering the RAGOps stack
With the storage in place, the creation of the knowledge base can be executed. It is 
important to note that the choices at this stage should be flexible. You should also keep 
options available for tools, services and libraries that can be experimented with during 
development. You’ll also have to estimate the costs associated with different steps of 
this stage and ensure that the stakeholders are aligned with these costs.
With the data layer of the RAGOps stack, the design of the indexing pipeline is com-
plete. You may also note that the indexing pipeline also interacts with the model layer 
where embeddings models and LLMs along with other task specific algorithms sit.
9.2.2	
Generation pipeline design
We have discussed that the real-time interaction of the user with the knowledge base 
is facilitated by the generation pipeline. In chapter 4, we developed the three main 
components of the generation pipeline—the retrievers, augmentation 
212
Chapter 9  RAG development framework and further exploration
Retrieval
Retrieval is a pivotal component of RAG systems. There are many retrieval techniques 
and strategies discussed in this book. The quality of the RAG system hinges on the accu-
racy of the retrieval component. You may use a dense embeddings similarity match for 
simple RAG systems. In more complex systems, you will need to use hybrid, iterative, or 
adaptive retrieval strategies. The questions to ask at this stage are
¡ Does our retrieval component need high precision, high recall, or both?  
¡ Can the queries be resolved with a simple similarity match?
¡ Do we need graph retrieval? 
¡ Will searching through the entire data be prohibitively long? Do we need 
filtering?
¡ Will a single pass retrieve all necessary documents?
¡ Will the information from the retrieved documents lead to more questions?
¡ Which models and techniques will we use for adaptive, recursive, or iterative 
retrieval?
¡ Which retrieval algorith
213
Design stage: Layering the RAGOps stack
Augmentation
Augmentation is the process of adding the retrieved context to the original query in a 
prompt that can be sent to the LLM for generation. While it may seem a simple step, 
there can be many nuances to it. All the use case context along with the retrieved con-
text also needs to be passed. Sometimes, you may need to pass examples of desired 
responses or the thought process. In cases where you need to use the LLMs internal 
parametric knowledge, this can also be specified in the prompt. Key questions to ask at 
this stage are
¡ What is the system prompt or the overall persona that we need the LLM to take?
¡ Does the response require nuanced analysis? Can that be passed as a chain of 
thought?
¡ Do we want to restrict the responses to the context only?
¡ What kind of examples should be given?
¡ Will different query types need different prompting techniques?
Augmentation is done through prompts, and prompts can be managed by the pr
214
Chapter 9  RAG development framework and further exploration
for relevance to checking the format and appending the responses with the retrieved 
sources. Some questions that can help with the assessment at this stage are 
¡ Does the response from the LLM be presented to the user as is?
¡ Is there any kind of verification that the responses need to go through?
¡ What is the impact of a sub-optimal result?
¡ Are there any workflows that need to be triggered based on the responses?
Response optimizations are highly subjective and closely coupled to the use case, but it 
is a consideration that should not be overlooked. 
With these seven steps, the generation pipeline design is complete. The model library 
and the training/fine-tuning components of the RAGOps stack can be covered with the 
necessary tools, platforms, and algorithms. The orchestration of the generation pipeline 
can also be finalized depending on the choices made during this stage. The prompt layer 
can also be address
215
Design stage: Layering the RAGOps stack
design considerations regarding security, guardrails, caching, and other use case 
requirements.
9.2.3	
Other design considerations
While well-designed core RAG pipelines complete the critical layers of the RAG system, 
other system considerations and business requirements also need to be addressed: 
¡ What kind of guardrails are required in the system? Should the user queries be 
restricted? Is there any kind of information that should not be output?
¡ Is it possible and useful to cache certain kinds of responses?
¡ Do we need human supervision or action at any stage in the system?
¡ How will the models be protected from adverse attacks?
¡ Is there any approval workflow required in the system?
¡ Are users looking for explainability?
These questions will help address the essential and enhancement layers of the RAGOps 
stack. You should be able to have a complete view of the necessary components, tools, 
platforms, and libraries for the develo
216
Chapter 9  RAG development framework and further exploration
for domain adaptation. In rare cases, you may choose to train language models from 
scratch. In such cases, the development of RAG systems may take a back seat, and train-
ing the models will be the core of the development effort. You can follow a progressive 
approach when deciding whether to fine-tune embeddings models and LLMs.
When creating embeddings using a pre-trained model, you will need to assess if a 
similarity search yields relevant results. To do this, you can also create ground truth 
data. The ground truth data can be a set of manually curated search queries and their 
matching documents. If the embeddings model can retrieve the documents accurately, 
you may use the pre-trained model. If not, you can either look for another embeddings 
model more suited for the use case domain or fine-tune the pre-trained embeddings 
model for the use case domain.
Similarly, if a pre-trained LLM generates desired results b
217
Design stage: Layering the RAGOps stack
Retrieval
Generation
Pre-retrieval
Post-retrieval
Naïve RAG is composed of three core modules.
Indexing
Advanced RAG adds pre-retrieval and post-retrieval
modules to the naïve RAG modules.
Routing
Fusion
Task adapter
Memory
Search
Predict
Enables direct
search across
different sources
Uses parametric
LLM memory to
guide retrieval
Selects the
optimal RAG
pathway for a
query
Expands user
queries
Enables direct
context
generation from
the LLM
Tailors the RAG
pipeline for
downstream
tasks
New modules interact with the advanced/naïve RAG framework, as well as with each other.
Naive RAG
Advanced RAG
Modular RAG
Figure 9.8  Modular structure allows for flexibility and scalability of individual components.
system. This workflow should be flexible enough to adapt with feedback for different 
query types. 
You will also have access to various managed services, frameworks, libraries, and tools 
that you can integrate with any of the modules. For example
218
Chapter 9  RAG development framework and further exploration
9.2.5	
Evaluation stage: Validating and optimizing the RAG system
Evaluation of the RAG system is a key component of its development process. All 
the different strategies, tools, and frameworks must be evaluated against some set of 
benchmarks. The actual business effect can only be measured post-deployment, but 
some metrics can be evaluated at the development stage. We can look at these metrics 
in two broad categories.
RAG components
The purpose of evaluating the RAG system is to assess the performance of different 
RAG components. To this end, there can be retriever-specific, generation-specific, and 
overall RAG evaluation metrics. Here is a summary of these metrics discussed in chap-
ter 5. We begin with retriever-specific metrics:
¡ Accuracy is typically defined as the proportion of correct predictions (both true 
positives and true negatives) among the total number of cases examined.
¡ Precision focuses on the qu
219
Design stage: Layering the RAGOps stack
¡ Conciseness evaluates whether the response is succinct and to the point, avoiding 
unnecessary verbosity, while still conveying complete information. 
We conclude with a summary of overall RAG metrics:
¡ Context relevance assesses the proportion of retrieved information relevant to the 
user query.
¡ Faithfulness or groundedness assesses the proportion of the claims in the response 
that are backed by the retrieved context.
¡ Hallucination rate calculates the proportion of generated claims in the response 
that are not present in the retrieved context.
¡ Coverage measures the number of relevant claims in the context and calculates 
the proportion of relevant claims present in the generated response.
¡ Answer relevance assesses the overall effectiveness of the system by calculating the 
relevance of the final response to the original question.
Recall the triad of RAG evaluation from chapter 5. Figure 9.9 shows the pairwise inter-
action betw
220
Chapter 9  RAG development framework and further exploration
question–context–response examples, akin to labeled data in supervised machine 
learning parlance. Ground truth data created for your knowledge base can be used for 
the evaluation of your RAG system. 
You can measure these metrics for different components. For example, you can 
check if context relevance increases by replacing a hybrid retrieval strategy with an 
adaptive one. You can also check the effectiveness of query and context optimization. 
You can also compare two service providers for a particular component.
System performance
System performance metrics relate to the non-functional requirements of the system, 
which affect the usability of the system more than the accuracy of the system. Some of 
these metrics are 
¡ Latency—Measures the time taken from receiving a query to delivering a response. 
Low latency is crucial for user satisfaction, especially in real-time applications.
¡ Throughput—Indicates the numb
221
Design stage: Layering the RAGOps stack
without affecting the live traffic. Consequently, there is zero downtime and an easy 
option for a rollback if any problem is encountered. However, it is a costly option since 
the entire production environment is duplicated. Indexing pipelines can be updated 
in the green environment without affecting the live system. Changes to retrieval strate-
gies or embeddings models can be safely validated before production use.
Canary deployment
Canary deployment gradually releases the new RAG system to a small number of users. 
If it performs well with these users, it is expanded to all users. Canary deployment 
allows for real-time user feedback that enables early detection of problems. However, it 
adds feedback and monitoring complexity and multiple versions to manage. It can test 
changes in retrieval algorithms, embeddings, or generation models on limited queries 
or specific regions.
Rolling deployment
Rolling deployment is used when there are 
222
Chapter 9  RAG development framework and further exploration
Now that the system is available to the users, you will start getting real-time feedback, 
and the success and failure of the system will also depend on how you react to the feed-
back. To measure and improve the system, continuous monitoring is required.
9.2.7	
Maintenance stage: Ensuring reliability and adaptability
Deploying a RAG system into production is only the first milestone in the journey 
toward an evolved contextual AI system. Explicit user feedback, evolving technology, and 
changing user behavior present previously unexplored challenges that the system may 
encounter. It is therefore essential to be continually vigilant and monitor the system per-
formance. There are several reasons why a RAG system may fail in production. There are 
operational reasons such as compute resource constraints, sudden spikes in load, and 
malicious attacks. The reason can also be a shift in the type of data in the knowledge base
223
Ideas for further exploration
without changing the parameters, SFT changes the parameters of a foundation model 
and therefore influences the parametric memory. RAG and SFT should be considered 
as complementary, rather than competing, techniques because both address differ-
ent parts of a generative AI system. You may prefer fine-tuning over RAG if there is a 
change required in the writing style, tonality, and vocabulary of the LLM responses. In 
their paper “Retrieval-Augmented Generation for Large Language Models: A Survey” 
(https://arxiv.org/abs/2312.10997), Gao and colleagues plot the evolution of prompt 
engineering to RAG and fine-tuning. This is illustrated in figure 9.10, demonstrating 
the need for fine-tuning with the increase in the need for model adaptation.
External knowledge
required
Model adaptation
required
High
Low
Low
High
Prompt
engineering
Fine-tuning
All of the above
RAG
Standard
prompting
Few-shot
prompting
Advanced
prompting
Naive RAG
Advanced
RAG
Modular

224
Chapter 9  RAG development framework and further exploration
9.3.2	
Long-context windows in LLMs
Context windows in LLMs keep growing significantly with iteration. As of this writing, 
Claude 3.5 sonnet supports a window of up to 200,000 tokens, while GPT-4o, O1, and 
variants can process 128,000 tokens. Google Gemini 1.5 leads with a massive 1-million-
token context window. It is possible that when you read this book, there may be models 
with even longer context windows. So, in a lot of cases, we can just pass the entire context 
such as a long document to the model as part of the prompt. This would eliminate the 
need for chunking, indexing, and retrieval in cases where the knowledge base is not 
too large. In their paper, “Retrieval Augmented Generation or Long-Context LLMs? A 
Comprehensive Study and Hybrid Approach” (https://arxiv.org/abs/2407.16833), Li 
and colleagues systematically compare RAG and LLMs with long-context windows. They 
demonstrate that long-context LLMs out
225
Summary
components can be configured without the need for custom development. For exam-
ple, knowledge bases are an Amazon Bedrock capability that facilitates implementation 
of the entire RAG workflow. Azure AI Search provides indexing and query capabilities, 
with the infrastructure of the Azure cloud, and Vertex AI RAG Engine is a component 
of Google’s Vertex AI platform that facilitates RAG. There are also independent service 
providers such as CustomGPT, Needle AI, Ragie, and so forth that provide managed 
RAG pipelines. As with managed solutions across technologies, the factors to consider 
are cost, applicability to the use case, flexibility, and control over components. 
9.3.4	
Difficult queries
Some key reasons for failures in RAG systems are related to the types of queries. As 
RAG developers, it is important to keep focusing on these query types so that the tech-
nique can be improved. Some of these are 
¡ Multi-step reasoning—RAG struggles with queries needing multi-ho
226
Chapter 9  RAG development framework and further exploration
¡ The framework emphasizes both the technical and operational aspects of RAG 
system development.
RAG development framework stages
¡ Initiation stage
–	 Focuses on understanding the problem statement, aligning stakeholders, and 
gathering requirements.
–	 Emphasizes use case identification and assessing the need for RAG, using tools 
like use case evaluation cards.
–	 Involves requirements gathering across business, functional, and non-
functional needs.
–	 Concludes with drafting a high-level architecture diagram for alignment and 
strategic decision-making.
¡ Design stage
–	 Transforms high-level architecture into detailed pipeline designs for indexing 
and generation.
–	 Incorporates choices around chunking, embeddings, and retrieval strategies.
–	 Addresses additional considerations such as guardrails, caching, security, and 
deployment strategies.
¡ Development stage
–	 Implements modular RAG pipelines, enabling flex
227
Summary
Best practices in RAG development
¡ Modular design improves adaptability and ease of updates.
¡ Ground truth datasets are essential for accurate evaluation and fine-tuning.
¡ Deployment strategies should align with system criticality, scale, and risk 
tolerance.
¡ Regularly monitor for changes in user behavior, data, and performance to main-
tain reliability.
Ideas for further exploration
¡ RAG vs. fine-tuning
–	 RAG complements fine-tuning by enhancing non-parametric memory, while 
fine-tuning adapts parametric memory for style, tonality, and vocabulary.
–	 Use cases may benefit from hybrid approaches, depending on specific needs.
¡  Long-context windows in LLMs
–	 Advances in LLMs (e.g., 200k+ token contexts) can reduce reliance on chunk-
ing and retrieval for smaller knowledge bases.
–	 Hybrid models such as SELF-ROUTE combine RAG with long-context pro-
cessing to optimize cost and accuracy.
¡ Managed solutions
–	 Services such as Amazon Bedrock, Azure AI Search, and Goo

229
index
A
A/B testing  221
accuracy  91, 218
vs. speed  55
active prompt  74
adaptive chunking  43
adaptive retrieval  135, 190
advanced techniques  84, 85
agentic RAG (Retrieval-Augmented 
Generation)  187–194. See also RAG 
(Retrieval Augmented Generation)
capabilities  190
challenges and best practices  193
LLM agents  187–190
pipelines  190–193
AI-powered research  15
ANNOY (Approximate Nearest Neighbors Oh 
Yeah)  53
answer faithfulness  100
answer relevance  101, 219
Apache Lucene  54
APE (automatic prompt engineer)  74
application orchestration layer  153, 163
ARES (Automated RAG Evaluation System)  110, 
156
ARISE  156
ART (automatic reasoning and tool use)  74
AssemblyAIAudioTranscriptLoader library  171
AsyncHtmlLoader function  35
augmentation  69–77
prompt engineering techniques  70–75
simple prompt creation  76
AutoGen  154
Azure SQL  54
B
BEIR (benchmarking information retrieval)  111
benchmarks  91
BERT (Bidirectional Encoder Representations from 
Transformers)  48
bia
230
index
ChromaDB  53, 55
chunk clustering and summarization  197
chunking  56, 148, 191
chunk optimization  125, 126
CLAP (Contrastive Language–Audio 
Pretraining)  172
Claude series, Anthropic  80
CLIP (Contrastive Language–Image 
Pretraining)  172
CLM (causal language modeling)  6
CloudSQL  54
Cohere embeddings  49
coherence  218
collaboration and experimentation layer  159–161, 
164
Command R series, Cohere  81
community support  161
complex queries  225
compression  136–139
conciseness  219
constraints  205
context-enriched chunking  126
context handling  81
context relevance  99, 219
contextual embeddings  64, 84
contextual prompting  70, 85
context window of LLMs  38
continued hallucination  161
controlled generation prompting  71, 85
conversational agents  14
core LLM brain  187
cosine similarity  50
cost  56, 80
efficiency  160
optimization layer  159, 164
per query  220
CoT (chain of thought) prompting  72, 85
counterfactual robustness  90
coverage  100, 219
CRAG (comprehens
231
index
embeddings  46, 148, 171, 173, 191
choosing  52
models  150
pretrained models  48
use cases  49–52
EM (Exact Match)  90, 111
ESG (environmental, social, and governance)  15
ETL (extract–transform–load)  148
Euclidean distance  50
evaluation component  23
evaluation layer  156, 163
evaluation stage  218–220
RAG components  218–220
system performance  220
explainability and interpretability layer  159, 164
F
F1-score  90, 93, 111, 218
faiss-cpu library  54
FAISS (Facebook AI Similarity Search)  53, 55, 56
faithfulness  219
FastText  48
fetch surrounding chunks  127
few-shot prompting  72, 85
examples  110
FiD (fusion-in-decoder)  223
File Search tool  159
fine-tuned models  77–79
fixed-size chunking  39–42
FLARE (forward-looking active retrieval-augmented 
generation)  135
flexibility vs. performance  55
foundation models  77, 150
frameworks  90, 103–111
ARES  110
RAGAs  104–110
fully managed deployment  152
functional requirements  205
fusion module  140
G
Galileo  156
Gemini 
232
index
index structures  128
in-domain passage set  110
inference optimization component  150
information integration  90
insufficient scalability planning  161
integration with existing stack  160
intent classification  133
interleaving experiments  221
iterative retrieval  134
K
KG (knowledge graph)  113
knowledge base  125
knowledge graph index  128
knowledge graph RAG  177–186, 198
approaches  179–182
challenges and best practices  186
knowledge graphs  177
pipelines  182–185
use cases  179
knowledge refinement  194
L
LanceDB  53
LangChain  34, 35, 154
langchain.chains library  185
langchain-community package  35, 36
langchain_experimental library  184
langchain_experimental.text_splitter library  44
latency  90, 164, 220
LCEL (LangChain Expression Language)  156
learned sparse retrieval  64
lesser hallucination  12
LlamaIndex  154
Llama series, Meta  81
LLamaTokenizer  41
LLMGraphTransformer class  184
LLMs (large language models)  26, 33, 87, 146
agents  187–190
categorization
233
index
multimodal RAG (retrieval-augmented 
generation)  169–177, 198
challenges and best practices  176
data modality  169
pipelines  170–175
use cases  170
multi-query expansion  130
multi-step reasoning  225
N
naïve RAG (retrieval-augmented generation), 
limitations of  122
nDCG (normalized discounted cumulative 
gain)  91, 96, 218
negative rejection  89
Neo4j  54
Neo4jGraph library  184
NER (named entity recognition)  78
neural IR models  65
news generation and content curation  15
NLP (natural language processing)  24, 38
NMSLIB (Non-Metric Space Library)  53
noise robustness  89
nonfunctional requirements  205
NQ (Natural Question)  111
O
OpenAI  48
OpenAIWhisperParser library  171
Open Search  54
open-source models  79–82
orchestration  217
original models  77–79
P
parameter-efficient fine-tuning (PEFT)  223
parent-child document structure  128
PEFT (parameter-efficient fine-tuning)  223
personalized marketing content generation  14
Phi-3, Microsoft  82
PII (Personally Identi
234
index
augmentation  69–77, 85
benchmarks  111–115
categorization of LLMs and suitability for 
RAG  77–82
completing pipeline, generation using LLMs  82
components of  28
corrective RAG  194
defined  8
design of systems  22
development framework  200, 201–207, 225
best practices in  227
deployment stage  220
design stage  207–222
development stage  215–217
evaluation stage  218–220
gathering requirements  204
generation pipeline design  211–215
high-level architecture  207
indexing pipeline design  208–211
initiating stage  203–207
maintenance stage  222
other design considerations  215
RAG components  218–220
requirements analysis  206
stages of  226
system performance  220
use case identification  203
difficult queries  225
discovery of  11
drafter  196
evaluation  88–91
ARES  110
benchmarks are static  116
frameworks  103–111
lack of standardized metrics  116
lack of use case subjectivity  116
limitations and best practices  115–117
overreliance on LLM as a judge  116
quality sco
235
index
tree construction  197
reflection tokens  196
relevance token  196
requirements 
analysis  206
gathering  204
re-ranking  137–139
resource 
constraints  81
utilization  220
retrieval  60, 212–215
augmentation  213
context optimization  212
coordination component  153
evaluator  194
generation  213
metrics  91–98
module  139
popular retrievers  67–69
progression of methods  61–66
response optimization  214
stage  124
retrievers  23, 26, 67–69
retrieve token  196
rewrite  131
RGB (Retrieval-augmented Generation 
Benchmark)  112
robustness  90
rolling deployment  221
ROUGE scores  90
routing module  140
S
scalability  164
and performance required  160
ScaNN (Scalable Nearest Neighbors)  53
scikit-learn  62
search 
engines  13
module  140
platforms  54
security  28
self-consistency  74
self-critique  196
self-hosted deployment  152
self RAG  196
SemanticChunker class  44
semantic chunking  44
semantic routing  133
SFR-Embedding-Mistral model  52
SFT (supervised fine-tuning)  77
S
236
index
U
Unstructured library  171
use case identification  203
use case-specific evaluation criteria  90
user needs  204
utility token  196
V
VAD (voice activity detection)  171
Vald  53
vector capabilities for SQL and NoSQL 
databases  54
vector databases  53, 148
choosing  55
types of  53–55
vector indexes  53
vector storage  34
vendor lock-in constraints  160
Vespa  53
virtual assistants  15
Voyage AI  48
W
Weaviate  53
web search supplementation  194
Word2Vec  47
workflow automation component  153
Y
YoutubeLoader library  171
DESIGN
DEVELOPMENT
DEPLOYMENT
MAINTENANCE
EVALUATION
INITIATION
Designing RAG
pipelines and finalizing
the layers of the
RAGOps stack
Developing RAG
pipelines, and creating a
prototype for evaluation
and feedback
Deployment of system to
production and serving
the desired users
Tracking and measuring
system performance and
improving the system
based on feedback
Assessing RAG metrics
and system performance
Understanding the use
case, and gathering and
analyzing requirements
R
A
G
d
e
v
e
l
o
p
m
e
n
t
f
r
a
m
e
w
o
r
k
Requirements document
High-level architecture
RAGOps stack
Working prototype
Ready-to-deploy system
Released system
Evolved system
The six stages of the RAG development framework are iterative and cyclic. 
At each stage, specific artifacts can be created.
ISBN-13: 978-1-63343-585-8
I
f you want to use a large language model to answer ques-
tions about your specifi c business, you’re out of luck. Th e 
LLM probably knows nothing about it and may even 
make up a response. Retrieval Augmented Generation is an 
approach that solves this class of problems. Th e model fi rst 
retrieves the most relevant pieces of information from your 
knowledge stores (search index, vector database, or a set of 
documents) and then generates its answer using the user’s 
prompt and the retrieved material as context. Th is avoids 
hallucination and lets you decide what it says.
A Simple Guide to Retrieval Augmented Generation is a plain-
English guide to RAG. Th e book is easy to follow and packed 
with realistic Python code examples. It takes you concept-
by-concept from your fi rst steps with RAG to advanced 
approaches, exploring how tools like LangChain and Python 
libraries make RAG easy. And to make sure you really 
understand how RAG works, you’ll build
